I1209 13:54:13.614411  3198 upgrade_proto.cpp:990] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': ./caffenet128_lsuv_no_lrn_BatchNormBeforeReLU_EltwiseAffine.prototxt
I1209 13:54:13.614568  3198 upgrade_proto.cpp:997] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 13:54:13.614573  3198 upgrade_proto.cpp:999] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 13:54:13.614639  3198 caffe.cpp:184] Using GPUs 0
I1209 13:54:13.739256  3198 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.01
display: 20
max_iter: 320000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 10000
snapshot_prefix: "snapshots/caffenet128_no_lrn_lsuv_bn_before_ea"
solver_mode: GPU
device_id: 0
net_param {
  name: "CaffeNet"
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TRAIN
    }
    transform_param {
      scale: 0.04
      mirror: true
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 124
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: CUBIC
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/home/share/storage/datasets/imagenet/lmdb/ilsvrc12_train_lmdb"
      batch_size: 256
      backend: LMDB
    }
  }
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TEST
    }
    transform_param {
      scale: 0.04
      mirror: false
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 123
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: CUBIC
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/home/share/storage/datasets/imagenet/lmdb/ilsvrc12_val_lmdb"
      batch_size: 50
      backend: LMDB
    }
  }
  layer {
    name: "conv1"
    type: "Convolution"
    bottom: "data"
    top: "conv1"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 96
      kernel_size: 11
      stride: 4
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "conv1_BN"
    type: "BatchNorm"
    bottom: "conv1"
    top: "conv1_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "conv1_BN"
    type: "BatchNorm"
    bottom: "conv1"
    top: "conv1_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN1"
    type: "EltwiseAffine"
    bottom: "conv1_BN"
    top: "conv1_BN_ea"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu1"
    type: "ReLU"
    bottom: "conv1_BN_ea"
    top: "relu1"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "relu1"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv2"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 2
      kernel_size: 5
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "conv2_BN"
    type: "BatchNorm"
    bottom: "conv2"
    top: "conv2_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "conv2_BN"
    type: "BatchNorm"
    bottom: "conv2"
    top: "conv2_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN2"
    type: "EltwiseAffine"
    bottom: "conv2_BN"
    top: "conv2_BN_ea"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu2"
    type: "ReLU"
    bottom: "conv2_BN_ea"
    top: "relu2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "relu2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv3"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "conv3_BN"
    type: "BatchNorm"
    bottom: "conv3"
    top: "conv3_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "conv3_BN"
    type: "BatchNorm"
    bottom: "conv3"
    top: "conv3_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN3"
    type: "EltwiseAffine"
    bottom: "conv3_BN"
    top: "conv3_BN_ea"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu3"
    type: "ReLU"
    bottom: "conv3_BN_ea"
    top: "relu3"
  }
  layer {
    name: "conv4"
    type: "Convolution"
    bottom: "relu3"
    top: "conv4"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "conv4_BN"
    type: "BatchNorm"
    bottom: "conv4"
    top: "conv4_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "conv4_BN"
    type: "BatchNorm"
    bottom: "conv4"
    top: "conv4_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN4"
    type: "EltwiseAffine"
    bottom: "conv4_BN"
    top: "conv4_BN_ea"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu4"
    type: "ReLU"
    bottom: "conv4_BN_ea"
    top: "relu4"
  }
  layer {
    name: "conv5"
    type: "Convolution"
    bottom: "relu4"
    top: "conv5"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "conv5_BN"
    type: "BatchNorm"
    bottom: "conv5"
    top: "conv5_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "conv5_BN"
    type: "BatchNorm"
    bottom: "conv5"
    top: "conv5_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN5"
    type: "EltwiseAffine"
    bottom: "conv5_BN"
    top: "conv5_BN_ea"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu5"
    type: "ReLU"
    bottom: "conv5_BN_ea"
    top: "relu5"
  }
  layer {
    name: "pool5"
    type: "Pooling"
    bottom: "relu5"
    top: "pool5"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "pool5"
    top: "fc6"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "fc6_BN"
    type: "BatchNorm"
    bottom: "fc6"
    top: "fc6_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "fc6_BN"
    type: "BatchNorm"
    bottom: "fc6"
    top: "fc6_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN6"
    type: "EltwiseAffine"
    bottom: "fc6_BN"
    top: "fc6_BN_ea"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu6"
    type: "ReLU"
    bottom: "fc6_BN_ea"
    top: "relu6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "relu6"
    top: "relu6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "relu6"
    top: "fc7"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "fc7_BN"
    type: "BatchNorm"
    bottom: "fc7"
    top: "fc7_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "fc7_BN"
    type: "BatchNorm"
    bottom: "fc7"
    top: "fc7_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN7"
    type: "EltwiseAffine"
    bottom: "fc7_BN"
    top: "fc7_BN_ea"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu7"
    type: "ReLU"
    bottom: "fc7_BN_ea"
    top: "relu7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "relu7"
    top: "relu7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc8"
    type: "InnerProduct"
    bottom: "relu7"
    top: "fc8"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 1000
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "accuracy"
    type: "Accuracy"
    bottom: "fc8"
    bottom: "label"
    top: "accuracy"
    include {
      phase: TEST
    }
  }
  layer {
    name: "loss"
    type: "SoftmaxWithLoss"
    bottom: "fc8"
    bottom: "label"
    top: "loss"
  }
}
iter_size: 1
type: "SGD"
I1209 13:54:13.739472  3198 solver.cpp:86] Creating training net specified in net_param.
I1209 13:54:13.739619  3198 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1209 13:54:13.739627  3198 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer conv1_BN
I1209 13:54:13.739647  3198 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer conv2_BN
I1209 13:54:13.739652  3198 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer conv3_BN
I1209 13:54:13.739668  3198 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer conv4_BN
I1209 13:54:13.739672  3198 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer conv5_BN
I1209 13:54:13.739684  3198 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer fc6_BN
I1209 13:54:13.739706  3198 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer fc7_BN
I1209 13:54:13.739709  3198 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1209 13:54:13.739903  3198 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.04
    mirror: true
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 124
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: CUBIC
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/share/storage/datasets/imagenet/lmdb/ilsvrc12_train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_BN"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN1"
  type: "EltwiseAffine"
  bottom: "conv1_BN"
  top: "conv1_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1_BN_ea"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv2_BN"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN2"
  type: "EltwiseAffine"
  bottom: "conv2_BN"
  top: "conv2_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2_BN_ea"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv3_BN"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN3"
  type: "EltwiseAffine"
  bottom: "conv3_BN"
  top: "conv3_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3_BN_ea"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv4_BN"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN4"
  type: "EltwiseAffine"
  bottom: "conv4_BN"
  top: "conv4_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4_BN_ea"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv5_BN"
  type: "BatchNorm"
  bottom: "conv5"
  top: "conv5_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN5"
  type: "EltwiseAffine"
  bottom: "conv5_BN"
  top: "conv5_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5_BN_ea"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "fc6_BN"
  type: "BatchNorm"
  bottom: "fc6"
  top: "fc6_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN6"
  type: "EltwiseAffine"
  bottom: "fc6_BN"
  top: "fc6_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6_BN_ea"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "relu6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "relu6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "fc7_BN"
  type: "BatchNorm"
  bottom: "fc7"
  top: "fc7_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN7"
  type: "EltwiseAffine"
  bottom: "fc7_BN"
  top: "fc7_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7_BN_ea"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "relu7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "relu7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1209 13:54:13.740010  3198 layer_factory.hpp:76] Creating layer data
I1209 13:54:13.740499  3198 net.cpp:106] Creating Layer data
I1209 13:54:13.740510  3198 net.cpp:411] data -> data
I1209 13:54:13.740535  3198 net.cpp:411] data -> label
I1209 13:54:13.741202  3202 db_lmdb.cpp:38] Opened lmdb /home/share/storage/datasets/imagenet/lmdb/ilsvrc12_train_lmdb
I1209 13:54:13.750697  3198 data_layer.cpp:41] output data size: 256,3,128,128
I1209 13:54:13.822890  3198 net.cpp:150] Setting up data
I1209 13:54:13.822927  3198 net.cpp:157] Top shape: 256 3 128 128 (12582912)
I1209 13:54:13.822932  3198 net.cpp:157] Top shape: 256 (256)
I1209 13:54:13.822934  3198 net.cpp:165] Memory required for data: 50332672
I1209 13:54:13.822952  3198 layer_factory.hpp:76] Creating layer conv1
I1209 13:54:13.822968  3198 net.cpp:106] Creating Layer conv1
I1209 13:54:13.822971  3198 net.cpp:454] conv1 <- data
I1209 13:54:13.822985  3198 net.cpp:411] conv1 -> conv1
I1209 13:54:13.950083  3198 net.cpp:150] Setting up conv1
I1209 13:54:13.950114  3198 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I1209 13:54:13.950117  3198 net.cpp:165] Memory required for data: 138806272
I1209 13:54:13.950131  3198 layer_factory.hpp:76] Creating layer conv1_BN
I1209 13:54:13.950155  3198 net.cpp:106] Creating Layer conv1_BN
I1209 13:54:13.950157  3198 net.cpp:454] conv1_BN <- conv1
I1209 13:54:13.950162  3198 net.cpp:411] conv1_BN -> conv1_BN
I1209 13:54:13.950328  3198 net.cpp:150] Setting up conv1_BN
I1209 13:54:13.950335  3198 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I1209 13:54:13.950345  3198 net.cpp:165] Memory required for data: 227279872
I1209 13:54:13.950353  3198 layer_factory.hpp:76] Creating layer ea_BN1
I1209 13:54:13.950361  3198 net.cpp:106] Creating Layer ea_BN1
I1209 13:54:13.950362  3198 net.cpp:454] ea_BN1 <- conv1_BN
I1209 13:54:13.950366  3198 net.cpp:411] ea_BN1 -> conv1_BN_ea
I1209 13:54:13.950785  3198 net.cpp:150] Setting up ea_BN1
I1209 13:54:13.950793  3198 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I1209 13:54:13.950805  3198 net.cpp:165] Memory required for data: 315753472
I1209 13:54:13.950809  3198 layer_factory.hpp:76] Creating layer relu1
I1209 13:54:13.950814  3198 net.cpp:106] Creating Layer relu1
I1209 13:54:13.950844  3198 net.cpp:454] relu1 <- conv1_BN_ea
I1209 13:54:13.950846  3198 net.cpp:411] relu1 -> relu1
I1209 13:54:13.951009  3198 net.cpp:150] Setting up relu1
I1209 13:54:13.951015  3198 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I1209 13:54:13.951027  3198 net.cpp:165] Memory required for data: 404227072
I1209 13:54:13.951030  3198 layer_factory.hpp:76] Creating layer pool1
I1209 13:54:13.951035  3198 net.cpp:106] Creating Layer pool1
I1209 13:54:13.951036  3198 net.cpp:454] pool1 <- relu1
I1209 13:54:13.951040  3198 net.cpp:411] pool1 -> pool1
I1209 13:54:13.951305  3198 net.cpp:150] Setting up pool1
I1209 13:54:13.951313  3198 net.cpp:157] Top shape: 256 96 15 15 (5529600)
I1209 13:54:13.951324  3198 net.cpp:165] Memory required for data: 426345472
I1209 13:54:13.951328  3198 layer_factory.hpp:76] Creating layer conv2
I1209 13:54:13.951334  3198 net.cpp:106] Creating Layer conv2
I1209 13:54:13.951336  3198 net.cpp:454] conv2 <- pool1
I1209 13:54:13.951349  3198 net.cpp:411] conv2 -> conv2
I1209 13:54:13.959532  3198 net.cpp:150] Setting up conv2
I1209 13:54:13.959553  3198 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I1209 13:54:13.959556  3198 net.cpp:165] Memory required for data: 485327872
I1209 13:54:13.959563  3198 layer_factory.hpp:76] Creating layer conv2_BN
I1209 13:54:13.959580  3198 net.cpp:106] Creating Layer conv2_BN
I1209 13:54:13.959583  3198 net.cpp:454] conv2_BN <- conv2
I1209 13:54:13.959586  3198 net.cpp:411] conv2_BN -> conv2_BN
I1209 13:54:13.959734  3198 net.cpp:150] Setting up conv2_BN
I1209 13:54:13.959739  3198 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I1209 13:54:13.959751  3198 net.cpp:165] Memory required for data: 544310272
I1209 13:54:13.959756  3198 layer_factory.hpp:76] Creating layer ea_BN2
I1209 13:54:13.959763  3198 net.cpp:106] Creating Layer ea_BN2
I1209 13:54:13.959764  3198 net.cpp:454] ea_BN2 <- conv2_BN
I1209 13:54:13.959777  3198 net.cpp:411] ea_BN2 -> conv2_BN_ea
I1209 13:54:13.959919  3198 net.cpp:150] Setting up ea_BN2
I1209 13:54:13.959923  3198 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I1209 13:54:13.959925  3198 net.cpp:165] Memory required for data: 603292672
I1209 13:54:13.959939  3198 layer_factory.hpp:76] Creating layer relu2
I1209 13:54:13.959942  3198 net.cpp:106] Creating Layer relu2
I1209 13:54:13.959954  3198 net.cpp:454] relu2 <- conv2_BN_ea
I1209 13:54:13.959959  3198 net.cpp:411] relu2 -> relu2
I1209 13:54:13.960198  3198 net.cpp:150] Setting up relu2
I1209 13:54:13.960206  3198 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I1209 13:54:13.960217  3198 net.cpp:165] Memory required for data: 662275072
I1209 13:54:13.960221  3198 layer_factory.hpp:76] Creating layer pool2
I1209 13:54:13.960225  3198 net.cpp:106] Creating Layer pool2
I1209 13:54:13.960228  3198 net.cpp:454] pool2 <- relu2
I1209 13:54:13.960240  3198 net.cpp:411] pool2 -> pool2
I1209 13:54:13.960475  3198 net.cpp:150] Setting up pool2
I1209 13:54:13.960484  3198 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I1209 13:54:13.960494  3198 net.cpp:165] Memory required for data: 675120128
I1209 13:54:13.960497  3198 layer_factory.hpp:76] Creating layer conv3
I1209 13:54:13.960503  3198 net.cpp:106] Creating Layer conv3
I1209 13:54:13.960505  3198 net.cpp:454] conv3 <- pool2
I1209 13:54:13.960510  3198 net.cpp:411] conv3 -> conv3
I1209 13:54:13.982767  3198 net.cpp:150] Setting up conv3
I1209 13:54:13.982784  3198 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:54:13.982787  3198 net.cpp:165] Memory required for data: 694387712
I1209 13:54:13.982795  3198 layer_factory.hpp:76] Creating layer conv3_BN
I1209 13:54:13.982813  3198 net.cpp:106] Creating Layer conv3_BN
I1209 13:54:13.982816  3198 net.cpp:454] conv3_BN <- conv3
I1209 13:54:13.982822  3198 net.cpp:411] conv3_BN -> conv3_BN
I1209 13:54:13.982962  3198 net.cpp:150] Setting up conv3_BN
I1209 13:54:13.982967  3198 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:54:13.982969  3198 net.cpp:165] Memory required for data: 713655296
I1209 13:54:13.982986  3198 layer_factory.hpp:76] Creating layer ea_BN3
I1209 13:54:13.983026  3198 net.cpp:106] Creating Layer ea_BN3
I1209 13:54:13.983031  3198 net.cpp:454] ea_BN3 <- conv3_BN
I1209 13:54:13.983034  3198 net.cpp:411] ea_BN3 -> conv3_BN_ea
I1209 13:54:13.983173  3198 net.cpp:150] Setting up ea_BN3
I1209 13:54:13.983178  3198 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:54:13.983180  3198 net.cpp:165] Memory required for data: 732922880
I1209 13:54:13.983194  3198 layer_factory.hpp:76] Creating layer relu3
I1209 13:54:13.983199  3198 net.cpp:106] Creating Layer relu3
I1209 13:54:13.983201  3198 net.cpp:454] relu3 <- conv3_BN_ea
I1209 13:54:13.983204  3198 net.cpp:411] relu3 -> relu3
I1209 13:54:13.983458  3198 net.cpp:150] Setting up relu3
I1209 13:54:13.983464  3198 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:54:13.983476  3198 net.cpp:165] Memory required for data: 752190464
I1209 13:54:13.983479  3198 layer_factory.hpp:76] Creating layer conv4
I1209 13:54:13.983485  3198 net.cpp:106] Creating Layer conv4
I1209 13:54:13.983489  3198 net.cpp:454] conv4 <- relu3
I1209 13:54:13.983502  3198 net.cpp:411] conv4 -> conv4
I1209 13:54:14.000088  3198 net.cpp:150] Setting up conv4
I1209 13:54:14.000115  3198 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:54:14.000118  3198 net.cpp:165] Memory required for data: 771458048
I1209 13:54:14.000125  3198 layer_factory.hpp:76] Creating layer conv4_BN
I1209 13:54:14.000135  3198 net.cpp:106] Creating Layer conv4_BN
I1209 13:54:14.000149  3198 net.cpp:454] conv4_BN <- conv4
I1209 13:54:14.000154  3198 net.cpp:411] conv4_BN -> conv4_BN
I1209 13:54:14.000305  3198 net.cpp:150] Setting up conv4_BN
I1209 13:54:14.000310  3198 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:54:14.000313  3198 net.cpp:165] Memory required for data: 790725632
I1209 13:54:14.000326  3198 layer_factory.hpp:76] Creating layer ea_BN4
I1209 13:54:14.000332  3198 net.cpp:106] Creating Layer ea_BN4
I1209 13:54:14.000335  3198 net.cpp:454] ea_BN4 <- conv4_BN
I1209 13:54:14.000347  3198 net.cpp:411] ea_BN4 -> conv4_BN_ea
I1209 13:54:14.000461  3198 net.cpp:150] Setting up ea_BN4
I1209 13:54:14.000465  3198 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:54:14.000468  3198 net.cpp:165] Memory required for data: 809993216
I1209 13:54:14.000480  3198 layer_factory.hpp:76] Creating layer relu4
I1209 13:54:14.000485  3198 net.cpp:106] Creating Layer relu4
I1209 13:54:14.000488  3198 net.cpp:454] relu4 <- conv4_BN_ea
I1209 13:54:14.000490  3198 net.cpp:411] relu4 -> relu4
I1209 13:54:14.000738  3198 net.cpp:150] Setting up relu4
I1209 13:54:14.000746  3198 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:54:14.000757  3198 net.cpp:165] Memory required for data: 829260800
I1209 13:54:14.000759  3198 layer_factory.hpp:76] Creating layer conv5
I1209 13:54:14.000767  3198 net.cpp:106] Creating Layer conv5
I1209 13:54:14.000771  3198 net.cpp:454] conv5 <- relu4
I1209 13:54:14.000783  3198 net.cpp:411] conv5 -> conv5
I1209 13:54:14.012887  3198 net.cpp:150] Setting up conv5
I1209 13:54:14.012903  3198 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I1209 13:54:14.012905  3198 net.cpp:165] Memory required for data: 842105856
I1209 13:54:14.012912  3198 layer_factory.hpp:76] Creating layer conv5_BN
I1209 13:54:14.012931  3198 net.cpp:106] Creating Layer conv5_BN
I1209 13:54:14.012935  3198 net.cpp:454] conv5_BN <- conv5
I1209 13:54:14.012940  3198 net.cpp:411] conv5_BN -> conv5_BN
I1209 13:54:14.013090  3198 net.cpp:150] Setting up conv5_BN
I1209 13:54:14.013094  3198 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I1209 13:54:14.013097  3198 net.cpp:165] Memory required for data: 854950912
I1209 13:54:14.013113  3198 layer_factory.hpp:76] Creating layer ea_BN5
I1209 13:54:14.013119  3198 net.cpp:106] Creating Layer ea_BN5
I1209 13:54:14.013131  3198 net.cpp:454] ea_BN5 <- conv5_BN
I1209 13:54:14.013135  3198 net.cpp:411] ea_BN5 -> conv5_BN_ea
I1209 13:54:14.013545  3198 net.cpp:150] Setting up ea_BN5
I1209 13:54:14.013553  3198 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I1209 13:54:14.013566  3198 net.cpp:165] Memory required for data: 867795968
I1209 13:54:14.013591  3198 layer_factory.hpp:76] Creating layer relu5
I1209 13:54:14.013594  3198 net.cpp:106] Creating Layer relu5
I1209 13:54:14.013597  3198 net.cpp:454] relu5 <- conv5_BN_ea
I1209 13:54:14.013602  3198 net.cpp:411] relu5 -> relu5
I1209 13:54:14.013762  3198 net.cpp:150] Setting up relu5
I1209 13:54:14.013778  3198 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I1209 13:54:14.013780  3198 net.cpp:165] Memory required for data: 880641024
I1209 13:54:14.013792  3198 layer_factory.hpp:76] Creating layer pool5
I1209 13:54:14.013798  3198 net.cpp:106] Creating Layer pool5
I1209 13:54:14.013800  3198 net.cpp:454] pool5 <- relu5
I1209 13:54:14.013803  3198 net.cpp:411] pool5 -> pool5
I1209 13:54:14.014060  3198 net.cpp:150] Setting up pool5
I1209 13:54:14.014066  3198 net.cpp:157] Top shape: 256 256 3 3 (589824)
I1209 13:54:14.014077  3198 net.cpp:165] Memory required for data: 883000320
I1209 13:54:14.014081  3198 layer_factory.hpp:76] Creating layer fc6
I1209 13:54:14.014098  3198 net.cpp:106] Creating Layer fc6
I1209 13:54:14.014101  3198 net.cpp:454] fc6 <- pool5
I1209 13:54:14.014104  3198 net.cpp:411] fc6 -> fc6
I1209 13:54:14.124375  3198 net.cpp:150] Setting up fc6
I1209 13:54:14.124402  3198 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:54:14.124404  3198 net.cpp:165] Memory required for data: 885097472
I1209 13:54:14.124413  3198 layer_factory.hpp:76] Creating layer fc6_BN
I1209 13:54:14.124421  3198 net.cpp:106] Creating Layer fc6_BN
I1209 13:54:14.124435  3198 net.cpp:454] fc6_BN <- fc6
I1209 13:54:14.124441  3198 net.cpp:411] fc6_BN -> fc6_BN
I1209 13:54:14.124589  3198 net.cpp:150] Setting up fc6_BN
I1209 13:54:14.124594  3198 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:54:14.124605  3198 net.cpp:165] Memory required for data: 887194624
I1209 13:54:14.124610  3198 layer_factory.hpp:76] Creating layer ea_BN6
I1209 13:54:14.124616  3198 net.cpp:106] Creating Layer ea_BN6
I1209 13:54:14.124619  3198 net.cpp:454] ea_BN6 <- fc6_BN
I1209 13:54:14.124631  3198 net.cpp:411] ea_BN6 -> fc6_BN_ea
I1209 13:54:14.124734  3198 net.cpp:150] Setting up ea_BN6
I1209 13:54:14.124738  3198 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:54:14.124740  3198 net.cpp:165] Memory required for data: 889291776
I1209 13:54:14.124753  3198 layer_factory.hpp:76] Creating layer relu6
I1209 13:54:14.124758  3198 net.cpp:106] Creating Layer relu6
I1209 13:54:14.124759  3198 net.cpp:454] relu6 <- fc6_BN_ea
I1209 13:54:14.124763  3198 net.cpp:411] relu6 -> relu6
I1209 13:54:14.124965  3198 net.cpp:150] Setting up relu6
I1209 13:54:14.124970  3198 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:54:14.124982  3198 net.cpp:165] Memory required for data: 891388928
I1209 13:54:14.124984  3198 layer_factory.hpp:76] Creating layer drop6
I1209 13:54:14.124992  3198 net.cpp:106] Creating Layer drop6
I1209 13:54:14.124994  3198 net.cpp:454] drop6 <- relu6
I1209 13:54:14.125007  3198 net.cpp:397] drop6 -> relu6 (in-place)
I1209 13:54:14.125028  3198 net.cpp:150] Setting up drop6
I1209 13:54:14.125031  3198 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:54:14.125033  3198 net.cpp:165] Memory required for data: 893486080
I1209 13:54:14.125036  3198 layer_factory.hpp:76] Creating layer fc7
I1209 13:54:14.125043  3198 net.cpp:106] Creating Layer fc7
I1209 13:54:14.125046  3198 net.cpp:454] fc7 <- relu6
I1209 13:54:14.125048  3198 net.cpp:411] fc7 -> fc7
I1209 13:54:14.223424  3198 net.cpp:150] Setting up fc7
I1209 13:54:14.223450  3198 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:54:14.223453  3198 net.cpp:165] Memory required for data: 895583232
I1209 13:54:14.223460  3198 layer_factory.hpp:76] Creating layer fc7_BN
I1209 13:54:14.223469  3198 net.cpp:106] Creating Layer fc7_BN
I1209 13:54:14.223482  3198 net.cpp:454] fc7_BN <- fc7
I1209 13:54:14.223489  3198 net.cpp:411] fc7_BN -> fc7_BN
I1209 13:54:14.223671  3198 net.cpp:150] Setting up fc7_BN
I1209 13:54:14.223676  3198 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:54:14.223678  3198 net.cpp:165] Memory required for data: 897680384
I1209 13:54:14.223711  3198 layer_factory.hpp:76] Creating layer ea_BN7
I1209 13:54:14.223717  3198 net.cpp:106] Creating Layer ea_BN7
I1209 13:54:14.223719  3198 net.cpp:454] ea_BN7 <- fc7_BN
I1209 13:54:14.223723  3198 net.cpp:411] ea_BN7 -> fc7_BN_ea
I1209 13:54:14.223830  3198 net.cpp:150] Setting up ea_BN7
I1209 13:54:14.223835  3198 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:54:14.223837  3198 net.cpp:165] Memory required for data: 899777536
I1209 13:54:14.223841  3198 layer_factory.hpp:76] Creating layer relu7
I1209 13:54:14.223847  3198 net.cpp:106] Creating Layer relu7
I1209 13:54:14.223850  3198 net.cpp:454] relu7 <- fc7_BN_ea
I1209 13:54:14.223852  3198 net.cpp:411] relu7 -> relu7
I1209 13:54:14.224197  3198 net.cpp:150] Setting up relu7
I1209 13:54:14.224206  3198 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:54:14.224218  3198 net.cpp:165] Memory required for data: 901874688
I1209 13:54:14.224220  3198 layer_factory.hpp:76] Creating layer drop7
I1209 13:54:14.224225  3198 net.cpp:106] Creating Layer drop7
I1209 13:54:14.224227  3198 net.cpp:454] drop7 <- relu7
I1209 13:54:14.224241  3198 net.cpp:397] drop7 -> relu7 (in-place)
I1209 13:54:14.224257  3198 net.cpp:150] Setting up drop7
I1209 13:54:14.224263  3198 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:54:14.224266  3198 net.cpp:165] Memory required for data: 903971840
I1209 13:54:14.224267  3198 layer_factory.hpp:76] Creating layer fc8
I1209 13:54:14.224273  3198 net.cpp:106] Creating Layer fc8
I1209 13:54:14.224275  3198 net.cpp:454] fc8 <- relu7
I1209 13:54:14.224279  3198 net.cpp:411] fc8 -> fc8
I1209 13:54:14.272482  3198 net.cpp:150] Setting up fc8
I1209 13:54:14.272511  3198 net.cpp:157] Top shape: 256 1000 (256000)
I1209 13:54:14.272512  3198 net.cpp:165] Memory required for data: 904995840
I1209 13:54:14.272531  3198 layer_factory.hpp:76] Creating layer loss
I1209 13:54:14.272619  3198 net.cpp:106] Creating Layer loss
I1209 13:54:14.272627  3198 net.cpp:454] loss <- fc8
I1209 13:54:14.272632  3198 net.cpp:454] loss <- label
I1209 13:54:14.272637  3198 net.cpp:411] loss -> loss
I1209 13:54:14.272650  3198 layer_factory.hpp:76] Creating layer loss
I1209 13:54:14.273432  3198 net.cpp:150] Setting up loss
I1209 13:54:14.273452  3198 net.cpp:157] Top shape: (1)
I1209 13:54:14.273454  3198 net.cpp:160]     with loss weight 1
I1209 13:54:14.273473  3198 net.cpp:165] Memory required for data: 904995844
I1209 13:54:14.273476  3198 net.cpp:226] loss needs backward computation.
I1209 13:54:14.273479  3198 net.cpp:226] fc8 needs backward computation.
I1209 13:54:14.273481  3198 net.cpp:226] drop7 needs backward computation.
I1209 13:54:14.273483  3198 net.cpp:226] relu7 needs backward computation.
I1209 13:54:14.273486  3198 net.cpp:226] ea_BN7 needs backward computation.
I1209 13:54:14.273488  3198 net.cpp:226] fc7_BN needs backward computation.
I1209 13:54:14.273491  3198 net.cpp:226] fc7 needs backward computation.
I1209 13:54:14.273494  3198 net.cpp:226] drop6 needs backward computation.
I1209 13:54:14.273495  3198 net.cpp:226] relu6 needs backward computation.
I1209 13:54:14.273499  3198 net.cpp:226] ea_BN6 needs backward computation.
I1209 13:54:14.273500  3198 net.cpp:226] fc6_BN needs backward computation.
I1209 13:54:14.273502  3198 net.cpp:226] fc6 needs backward computation.
I1209 13:54:14.273505  3198 net.cpp:226] pool5 needs backward computation.
I1209 13:54:14.273507  3198 net.cpp:226] relu5 needs backward computation.
I1209 13:54:14.273510  3198 net.cpp:226] ea_BN5 needs backward computation.
I1209 13:54:14.273514  3198 net.cpp:226] conv5_BN needs backward computation.
I1209 13:54:14.273515  3198 net.cpp:226] conv5 needs backward computation.
I1209 13:54:14.273519  3198 net.cpp:226] relu4 needs backward computation.
I1209 13:54:14.273520  3198 net.cpp:226] ea_BN4 needs backward computation.
I1209 13:54:14.273522  3198 net.cpp:226] conv4_BN needs backward computation.
I1209 13:54:14.273525  3198 net.cpp:226] conv4 needs backward computation.
I1209 13:54:14.273527  3198 net.cpp:226] relu3 needs backward computation.
I1209 13:54:14.273545  3198 net.cpp:226] ea_BN3 needs backward computation.
I1209 13:54:14.273548  3198 net.cpp:226] conv3_BN needs backward computation.
I1209 13:54:14.273550  3198 net.cpp:226] conv3 needs backward computation.
I1209 13:54:14.273553  3198 net.cpp:226] pool2 needs backward computation.
I1209 13:54:14.273555  3198 net.cpp:226] relu2 needs backward computation.
I1209 13:54:14.273557  3198 net.cpp:226] ea_BN2 needs backward computation.
I1209 13:54:14.273561  3198 net.cpp:226] conv2_BN needs backward computation.
I1209 13:54:14.273562  3198 net.cpp:226] conv2 needs backward computation.
I1209 13:54:14.273564  3198 net.cpp:226] pool1 needs backward computation.
I1209 13:54:14.273567  3198 net.cpp:226] relu1 needs backward computation.
I1209 13:54:14.273569  3198 net.cpp:226] ea_BN1 needs backward computation.
I1209 13:54:14.273571  3198 net.cpp:226] conv1_BN needs backward computation.
I1209 13:54:14.273574  3198 net.cpp:226] conv1 needs backward computation.
I1209 13:54:14.273576  3198 net.cpp:228] data does not need backward computation.
I1209 13:54:14.273578  3198 net.cpp:270] This network produces output loss
I1209 13:54:14.273597  3198 net.cpp:283] Network initialization done.
I1209 13:54:14.273733  3198 solver.cpp:181] Creating test net (#0) specified by net_param
I1209 13:54:14.273777  3198 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1209 13:54:14.273782  3198 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer conv1_BN
I1209 13:54:14.273787  3198 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer conv2_BN
I1209 13:54:14.273792  3198 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer conv3_BN
I1209 13:54:14.273795  3198 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer conv4_BN
I1209 13:54:14.273798  3198 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer conv5_BN
I1209 13:54:14.273802  3198 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer fc6_BN
I1209 13:54:14.273805  3198 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer fc7_BN
I1209 13:54:14.274019  3198 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.04
    mirror: false
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 123
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: CUBIC
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/share/storage/datasets/imagenet/lmdb/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_BN"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN1"
  type: "EltwiseAffine"
  bottom: "conv1_BN"
  top: "conv1_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1_BN_ea"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv2_BN"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN2"
  type: "EltwiseAffine"
  bottom: "conv2_BN"
  top: "conv2_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2_BN_ea"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv3_BN"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN3"
  type: "EltwiseAffine"
  bottom: "conv3_BN"
  top: "conv3_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3_BN_ea"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv4_BN"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN4"
  type: "EltwiseAffine"
  bottom: "conv4_BN"
  top: "conv4_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4_BN_ea"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv5_BN"
  type: "BatchNorm"
  bottom: "conv5"
  top: "conv5_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN5"
  type: "EltwiseAffine"
  bottom: "conv5_BN"
  top: "conv5_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5_BN_ea"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "fc6_BN"
  type: "BatchNorm"
  bottom: "fc6"
  top: "fc6_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN6"
  type: "EltwiseAffine"
  bottom: "fc6_BN"
  top: "fc6_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6_BN_ea"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "relu6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "relu6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "fc7_BN"
  type: "BatchNorm"
  bottom: "fc7"
  top: "fc7_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN7"
  type: "EltwiseAffine"
  bottom: "fc7_BN"
  top: "fc7_BN_ea"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7_BN_ea"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "relu7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "relu7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1209 13:54:14.274135  3198 layer_factory.hpp:76] Creating layer data
I1209 13:54:14.274201  3198 net.cpp:106] Creating Layer data
I1209 13:54:14.274207  3198 net.cpp:411] data -> data
I1209 13:54:14.274214  3198 net.cpp:411] data -> label
I1209 13:54:14.274991  3211 db_lmdb.cpp:38] Opened lmdb /home/share/storage/datasets/imagenet/lmdb/ilsvrc12_val_lmdb
I1209 13:54:14.276197  3198 data_layer.cpp:41] output data size: 50,3,128,128
I1209 13:54:14.294785  3198 net.cpp:150] Setting up data
I1209 13:54:14.294805  3198 net.cpp:157] Top shape: 50 3 128 128 (2457600)
I1209 13:54:14.294807  3198 net.cpp:157] Top shape: 50 (50)
I1209 13:54:14.294811  3198 net.cpp:165] Memory required for data: 9830600
I1209 13:54:14.294816  3198 layer_factory.hpp:76] Creating layer label_data_1_split
I1209 13:54:14.294888  3198 net.cpp:106] Creating Layer label_data_1_split
I1209 13:54:14.294893  3198 net.cpp:454] label_data_1_split <- label
I1209 13:54:14.294908  3198 net.cpp:411] label_data_1_split -> label_data_1_split_0
I1209 13:54:14.294915  3198 net.cpp:411] label_data_1_split -> label_data_1_split_1
I1209 13:54:14.295008  3198 net.cpp:150] Setting up label_data_1_split
I1209 13:54:14.295014  3198 net.cpp:157] Top shape: 50 (50)
I1209 13:54:14.295030  3198 net.cpp:157] Top shape: 50 (50)
I1209 13:54:14.295034  3198 net.cpp:165] Memory required for data: 9831000
I1209 13:54:14.295037  3198 layer_factory.hpp:76] Creating layer conv1
I1209 13:54:14.295061  3198 net.cpp:106] Creating Layer conv1
I1209 13:54:14.295064  3198 net.cpp:454] conv1 <- data
I1209 13:54:14.295068  3198 net.cpp:411] conv1 -> conv1
I1209 13:54:14.305747  3198 net.cpp:150] Setting up conv1
I1209 13:54:14.305758  3198 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I1209 13:54:14.305770  3198 net.cpp:165] Memory required for data: 27111000
I1209 13:54:14.305778  3198 layer_factory.hpp:76] Creating layer conv1_BN
I1209 13:54:14.305784  3198 net.cpp:106] Creating Layer conv1_BN
I1209 13:54:14.305786  3198 net.cpp:454] conv1_BN <- conv1
I1209 13:54:14.305814  3198 net.cpp:411] conv1_BN -> conv1_BN
I1209 13:54:14.306013  3198 net.cpp:150] Setting up conv1_BN
I1209 13:54:14.306020  3198 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I1209 13:54:14.306021  3198 net.cpp:165] Memory required for data: 44391000
I1209 13:54:14.306038  3198 layer_factory.hpp:76] Creating layer ea_BN1
I1209 13:54:14.306046  3198 net.cpp:106] Creating Layer ea_BN1
I1209 13:54:14.306047  3198 net.cpp:454] ea_BN1 <- conv1_BN
I1209 13:54:14.306051  3198 net.cpp:411] ea_BN1 -> conv1_BN_ea
I1209 13:54:14.306237  3198 net.cpp:150] Setting up ea_BN1
I1209 13:54:14.306243  3198 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I1209 13:54:14.306255  3198 net.cpp:165] Memory required for data: 61671000
I1209 13:54:14.306259  3198 layer_factory.hpp:76] Creating layer relu1
I1209 13:54:14.306263  3198 net.cpp:106] Creating Layer relu1
I1209 13:54:14.306265  3198 net.cpp:454] relu1 <- conv1_BN_ea
I1209 13:54:14.306268  3198 net.cpp:411] relu1 -> relu1
I1209 13:54:14.306531  3198 net.cpp:150] Setting up relu1
I1209 13:54:14.306538  3198 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I1209 13:54:14.306550  3198 net.cpp:165] Memory required for data: 78951000
I1209 13:54:14.306553  3198 layer_factory.hpp:76] Creating layer pool1
I1209 13:54:14.306558  3198 net.cpp:106] Creating Layer pool1
I1209 13:54:14.306560  3198 net.cpp:454] pool1 <- relu1
I1209 13:54:14.306563  3198 net.cpp:411] pool1 -> pool1
I1209 13:54:14.306751  3198 net.cpp:150] Setting up pool1
I1209 13:54:14.306776  3198 net.cpp:157] Top shape: 50 96 15 15 (1080000)
I1209 13:54:14.306787  3198 net.cpp:165] Memory required for data: 83271000
I1209 13:54:14.306797  3198 layer_factory.hpp:76] Creating layer conv2
I1209 13:54:14.306809  3198 net.cpp:106] Creating Layer conv2
I1209 13:54:14.306819  3198 net.cpp:454] conv2 <- pool1
I1209 13:54:14.306833  3198 net.cpp:411] conv2 -> conv2
I1209 13:54:14.315697  3198 net.cpp:150] Setting up conv2
I1209 13:54:14.315747  3198 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I1209 13:54:14.315758  3198 net.cpp:165] Memory required for data: 94791000
I1209 13:54:14.315788  3198 layer_factory.hpp:76] Creating layer conv2_BN
I1209 13:54:14.315809  3198 net.cpp:106] Creating Layer conv2_BN
I1209 13:54:14.315820  3198 net.cpp:454] conv2_BN <- conv2
I1209 13:54:14.315834  3198 net.cpp:411] conv2_BN -> conv2_BN
I1209 13:54:14.315999  3198 net.cpp:150] Setting up conv2_BN
I1209 13:54:14.316014  3198 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I1209 13:54:14.316022  3198 net.cpp:165] Memory required for data: 106311000
I1209 13:54:14.316035  3198 layer_factory.hpp:76] Creating layer ea_BN2
I1209 13:54:14.316050  3198 net.cpp:106] Creating Layer ea_BN2
I1209 13:54:14.316061  3198 net.cpp:454] ea_BN2 <- conv2_BN
I1209 13:54:14.316071  3198 net.cpp:411] ea_BN2 -> conv2_BN_ea
I1209 13:54:14.316246  3198 net.cpp:150] Setting up ea_BN2
I1209 13:54:14.316262  3198 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I1209 13:54:14.316270  3198 net.cpp:165] Memory required for data: 117831000
I1209 13:54:14.316282  3198 layer_factory.hpp:76] Creating layer relu2
I1209 13:54:14.316294  3198 net.cpp:106] Creating Layer relu2
I1209 13:54:14.316304  3198 net.cpp:454] relu2 <- conv2_BN_ea
I1209 13:54:14.316314  3198 net.cpp:411] relu2 -> relu2
I1209 13:54:14.316472  3198 net.cpp:150] Setting up relu2
I1209 13:54:14.316488  3198 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I1209 13:54:14.316498  3198 net.cpp:165] Memory required for data: 129351000
I1209 13:54:14.316507  3198 layer_factory.hpp:76] Creating layer pool2
I1209 13:54:14.316521  3198 net.cpp:106] Creating Layer pool2
I1209 13:54:14.316532  3198 net.cpp:454] pool2 <- relu2
I1209 13:54:14.316543  3198 net.cpp:411] pool2 -> pool2
I1209 13:54:14.316813  3198 net.cpp:150] Setting up pool2
I1209 13:54:14.316834  3198 net.cpp:157] Top shape: 50 256 7 7 (627200)
I1209 13:54:14.316844  3198 net.cpp:165] Memory required for data: 131859800
I1209 13:54:14.316854  3198 layer_factory.hpp:76] Creating layer conv3
I1209 13:54:14.316869  3198 net.cpp:106] Creating Layer conv3
I1209 13:54:14.316880  3198 net.cpp:454] conv3 <- pool2
I1209 13:54:14.316891  3198 net.cpp:411] conv3 -> conv3
I1209 13:54:14.338969  3198 net.cpp:150] Setting up conv3
I1209 13:54:14.338995  3198 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:54:14.338999  3198 net.cpp:165] Memory required for data: 135623000
I1209 13:54:14.339005  3198 layer_factory.hpp:76] Creating layer conv3_BN
I1209 13:54:14.339015  3198 net.cpp:106] Creating Layer conv3_BN
I1209 13:54:14.339020  3198 net.cpp:454] conv3_BN <- conv3
I1209 13:54:14.339025  3198 net.cpp:411] conv3_BN -> conv3_BN
I1209 13:54:14.339185  3198 net.cpp:150] Setting up conv3_BN
I1209 13:54:14.339190  3198 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:54:14.339192  3198 net.cpp:165] Memory required for data: 139386200
I1209 13:54:14.339210  3198 layer_factory.hpp:76] Creating layer ea_BN3
I1209 13:54:14.339215  3198 net.cpp:106] Creating Layer ea_BN3
I1209 13:54:14.339217  3198 net.cpp:454] ea_BN3 <- conv3_BN
I1209 13:54:14.339222  3198 net.cpp:411] ea_BN3 -> conv3_BN_ea
I1209 13:54:14.339349  3198 net.cpp:150] Setting up ea_BN3
I1209 13:54:14.339354  3198 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:54:14.339365  3198 net.cpp:165] Memory required for data: 143149400
I1209 13:54:14.339368  3198 layer_factory.hpp:76] Creating layer relu3
I1209 13:54:14.339372  3198 net.cpp:106] Creating Layer relu3
I1209 13:54:14.339375  3198 net.cpp:454] relu3 <- conv3_BN_ea
I1209 13:54:14.339378  3198 net.cpp:411] relu3 -> relu3
I1209 13:54:14.339519  3198 net.cpp:150] Setting up relu3
I1209 13:54:14.339525  3198 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:54:14.339537  3198 net.cpp:165] Memory required for data: 146912600
I1209 13:54:14.339540  3198 layer_factory.hpp:76] Creating layer conv4
I1209 13:54:14.339546  3198 net.cpp:106] Creating Layer conv4
I1209 13:54:14.339548  3198 net.cpp:454] conv4 <- relu3
I1209 13:54:14.339553  3198 net.cpp:411] conv4 -> conv4
I1209 13:54:14.356967  3198 net.cpp:150] Setting up conv4
I1209 13:54:14.356993  3198 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:54:14.357019  3198 net.cpp:165] Memory required for data: 150675800
I1209 13:54:14.357028  3198 layer_factory.hpp:76] Creating layer conv4_BN
I1209 13:54:14.357036  3198 net.cpp:106] Creating Layer conv4_BN
I1209 13:54:14.357039  3198 net.cpp:454] conv4_BN <- conv4
I1209 13:54:14.357044  3198 net.cpp:411] conv4_BN -> conv4_BN
I1209 13:54:14.357199  3198 net.cpp:150] Setting up conv4_BN
I1209 13:54:14.357203  3198 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:54:14.357205  3198 net.cpp:165] Memory required for data: 154439000
I1209 13:54:14.357210  3198 layer_factory.hpp:76] Creating layer ea_BN4
I1209 13:54:14.357215  3198 net.cpp:106] Creating Layer ea_BN4
I1209 13:54:14.357218  3198 net.cpp:454] ea_BN4 <- conv4_BN
I1209 13:54:14.357223  3198 net.cpp:411] ea_BN4 -> conv4_BN_ea
I1209 13:54:14.357349  3198 net.cpp:150] Setting up ea_BN4
I1209 13:54:14.357354  3198 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:54:14.357357  3198 net.cpp:165] Memory required for data: 158202200
I1209 13:54:14.357360  3198 layer_factory.hpp:76] Creating layer relu4
I1209 13:54:14.357364  3198 net.cpp:106] Creating Layer relu4
I1209 13:54:14.357367  3198 net.cpp:454] relu4 <- conv4_BN_ea
I1209 13:54:14.357370  3198 net.cpp:411] relu4 -> relu4
I1209 13:54:14.357511  3198 net.cpp:150] Setting up relu4
I1209 13:54:14.357516  3198 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:54:14.357517  3198 net.cpp:165] Memory required for data: 161965400
I1209 13:54:14.357519  3198 layer_factory.hpp:76] Creating layer conv5
I1209 13:54:14.357527  3198 net.cpp:106] Creating Layer conv5
I1209 13:54:14.357528  3198 net.cpp:454] conv5 <- relu4
I1209 13:54:14.357532  3198 net.cpp:411] conv5 -> conv5
I1209 13:54:14.369550  3198 net.cpp:150] Setting up conv5
I1209 13:54:14.369576  3198 net.cpp:157] Top shape: 50 256 7 7 (627200)
I1209 13:54:14.369580  3198 net.cpp:165] Memory required for data: 164474200
I1209 13:54:14.369586  3198 layer_factory.hpp:76] Creating layer conv5_BN
I1209 13:54:14.369596  3198 net.cpp:106] Creating Layer conv5_BN
I1209 13:54:14.369599  3198 net.cpp:454] conv5_BN <- conv5
I1209 13:54:14.369604  3198 net.cpp:411] conv5_BN -> conv5_BN
I1209 13:54:14.369765  3198 net.cpp:150] Setting up conv5_BN
I1209 13:54:14.369770  3198 net.cpp:157] Top shape: 50 256 7 7 (627200)
I1209 13:54:14.369771  3198 net.cpp:165] Memory required for data: 166983000
I1209 13:54:14.369778  3198 layer_factory.hpp:76] Creating layer ea_BN5
I1209 13:54:14.369784  3198 net.cpp:106] Creating Layer ea_BN5
I1209 13:54:14.369787  3198 net.cpp:454] ea_BN5 <- conv5_BN
I1209 13:54:14.369791  3198 net.cpp:411] ea_BN5 -> conv5_BN_ea
I1209 13:54:14.369897  3198 net.cpp:150] Setting up ea_BN5
I1209 13:54:14.369901  3198 net.cpp:157] Top shape: 50 256 7 7 (627200)
I1209 13:54:14.369904  3198 net.cpp:165] Memory required for data: 169491800
I1209 13:54:14.369906  3198 layer_factory.hpp:76] Creating layer relu5
I1209 13:54:14.369911  3198 net.cpp:106] Creating Layer relu5
I1209 13:54:14.369913  3198 net.cpp:454] relu5 <- conv5_BN_ea
I1209 13:54:14.369916  3198 net.cpp:411] relu5 -> relu5
I1209 13:54:14.370053  3198 net.cpp:150] Setting up relu5
I1209 13:54:14.370057  3198 net.cpp:157] Top shape: 50 256 7 7 (627200)
I1209 13:54:14.370059  3198 net.cpp:165] Memory required for data: 172000600
I1209 13:54:14.370061  3198 layer_factory.hpp:76] Creating layer pool5
I1209 13:54:14.370066  3198 net.cpp:106] Creating Layer pool5
I1209 13:54:14.370069  3198 net.cpp:454] pool5 <- relu5
I1209 13:54:14.370071  3198 net.cpp:411] pool5 -> pool5
I1209 13:54:14.370327  3198 net.cpp:150] Setting up pool5
I1209 13:54:14.370333  3198 net.cpp:157] Top shape: 50 256 3 3 (115200)
I1209 13:54:14.370335  3198 net.cpp:165] Memory required for data: 172461400
I1209 13:54:14.370337  3198 layer_factory.hpp:76] Creating layer fc6
I1209 13:54:14.370342  3198 net.cpp:106] Creating Layer fc6
I1209 13:54:14.370344  3198 net.cpp:454] fc6 <- pool5
I1209 13:54:14.370349  3198 net.cpp:411] fc6 -> fc6
I1209 13:54:14.483741  3198 net.cpp:150] Setting up fc6
I1209 13:54:14.483769  3198 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:54:14.483794  3198 net.cpp:165] Memory required for data: 172871000
I1209 13:54:14.483803  3198 layer_factory.hpp:76] Creating layer fc6_BN
I1209 13:54:14.483813  3198 net.cpp:106] Creating Layer fc6_BN
I1209 13:54:14.483816  3198 net.cpp:454] fc6_BN <- fc6
I1209 13:54:14.483820  3198 net.cpp:411] fc6_BN -> fc6_BN
I1209 13:54:14.483975  3198 net.cpp:150] Setting up fc6_BN
I1209 13:54:14.483979  3198 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:54:14.483981  3198 net.cpp:165] Memory required for data: 173280600
I1209 13:54:14.483985  3198 layer_factory.hpp:76] Creating layer ea_BN6
I1209 13:54:14.483991  3198 net.cpp:106] Creating Layer ea_BN6
I1209 13:54:14.483994  3198 net.cpp:454] ea_BN6 <- fc6_BN
I1209 13:54:14.483999  3198 net.cpp:411] ea_BN6 -> fc6_BN_ea
I1209 13:54:14.484096  3198 net.cpp:150] Setting up ea_BN6
I1209 13:54:14.484098  3198 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:54:14.484100  3198 net.cpp:165] Memory required for data: 173690200
I1209 13:54:14.484104  3198 layer_factory.hpp:76] Creating layer relu6
I1209 13:54:14.484108  3198 net.cpp:106] Creating Layer relu6
I1209 13:54:14.484112  3198 net.cpp:454] relu6 <- fc6_BN_ea
I1209 13:54:14.484114  3198 net.cpp:411] relu6 -> relu6
I1209 13:54:14.484473  3198 net.cpp:150] Setting up relu6
I1209 13:54:14.484498  3198 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:54:14.484510  3198 net.cpp:165] Memory required for data: 174099800
I1209 13:54:14.484521  3198 layer_factory.hpp:76] Creating layer drop6
I1209 13:54:14.484534  3198 net.cpp:106] Creating Layer drop6
I1209 13:54:14.484544  3198 net.cpp:454] drop6 <- relu6
I1209 13:54:14.484555  3198 net.cpp:397] drop6 -> relu6 (in-place)
I1209 13:54:14.484587  3198 net.cpp:150] Setting up drop6
I1209 13:54:14.484601  3198 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:54:14.484611  3198 net.cpp:165] Memory required for data: 174509400
I1209 13:54:14.484621  3198 layer_factory.hpp:76] Creating layer fc7
I1209 13:54:14.484632  3198 net.cpp:106] Creating Layer fc7
I1209 13:54:14.484643  3198 net.cpp:454] fc7 <- relu6
I1209 13:54:14.484654  3198 net.cpp:411] fc7 -> fc7
I1209 13:54:14.584482  3198 net.cpp:150] Setting up fc7
I1209 13:54:14.584508  3198 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:54:14.584511  3198 net.cpp:165] Memory required for data: 174919000
I1209 13:54:14.584519  3198 layer_factory.hpp:76] Creating layer fc7_BN
I1209 13:54:14.584527  3198 net.cpp:106] Creating Layer fc7_BN
I1209 13:54:14.584540  3198 net.cpp:454] fc7_BN <- fc7
I1209 13:54:14.584547  3198 net.cpp:411] fc7_BN -> fc7_BN
I1209 13:54:14.584717  3198 net.cpp:150] Setting up fc7_BN
I1209 13:54:14.584722  3198 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:54:14.584733  3198 net.cpp:165] Memory required for data: 175328600
I1209 13:54:14.584738  3198 layer_factory.hpp:76] Creating layer ea_BN7
I1209 13:54:14.584754  3198 net.cpp:106] Creating Layer ea_BN7
I1209 13:54:14.584756  3198 net.cpp:454] ea_BN7 <- fc7_BN
I1209 13:54:14.584761  3198 net.cpp:411] ea_BN7 -> fc7_BN_ea
I1209 13:54:14.584861  3198 net.cpp:150] Setting up ea_BN7
I1209 13:54:14.584866  3198 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:54:14.584867  3198 net.cpp:165] Memory required for data: 175738200
I1209 13:54:14.584879  3198 layer_factory.hpp:76] Creating layer relu7
I1209 13:54:14.584884  3198 net.cpp:106] Creating Layer relu7
I1209 13:54:14.584897  3198 net.cpp:454] relu7 <- fc7_BN_ea
I1209 13:54:14.584899  3198 net.cpp:411] relu7 -> relu7
I1209 13:54:14.585096  3198 net.cpp:150] Setting up relu7
I1209 13:54:14.585103  3198 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:54:14.585114  3198 net.cpp:165] Memory required for data: 176147800
I1209 13:54:14.585116  3198 layer_factory.hpp:76] Creating layer drop7
I1209 13:54:14.585120  3198 net.cpp:106] Creating Layer drop7
I1209 13:54:14.585132  3198 net.cpp:454] drop7 <- relu7
I1209 13:54:14.585135  3198 net.cpp:397] drop7 -> relu7 (in-place)
I1209 13:54:14.585155  3198 net.cpp:150] Setting up drop7
I1209 13:54:14.585160  3198 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:54:14.585175  3198 net.cpp:165] Memory required for data: 176557400
I1209 13:54:14.585176  3198 layer_factory.hpp:76] Creating layer fc8
I1209 13:54:14.585182  3198 net.cpp:106] Creating Layer fc8
I1209 13:54:14.585185  3198 net.cpp:454] fc8 <- relu7
I1209 13:54:14.585188  3198 net.cpp:411] fc8 -> fc8
I1209 13:54:14.634188  3198 net.cpp:150] Setting up fc8
I1209 13:54:14.634214  3198 net.cpp:157] Top shape: 50 1000 (50000)
I1209 13:54:14.634217  3198 net.cpp:165] Memory required for data: 176757400
I1209 13:54:14.634224  3198 layer_factory.hpp:76] Creating layer fc8_fc8_0_split
I1209 13:54:14.634232  3198 net.cpp:106] Creating Layer fc8_fc8_0_split
I1209 13:54:14.634235  3198 net.cpp:454] fc8_fc8_0_split <- fc8
I1209 13:54:14.634240  3198 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1209 13:54:14.634255  3198 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1209 13:54:14.634317  3198 net.cpp:150] Setting up fc8_fc8_0_split
I1209 13:54:14.634322  3198 net.cpp:157] Top shape: 50 1000 (50000)
I1209 13:54:14.634335  3198 net.cpp:157] Top shape: 50 1000 (50000)
I1209 13:54:14.634337  3198 net.cpp:165] Memory required for data: 177157400
I1209 13:54:14.634340  3198 layer_factory.hpp:76] Creating layer accuracy
I1209 13:54:14.634346  3198 net.cpp:106] Creating Layer accuracy
I1209 13:54:14.634349  3198 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I1209 13:54:14.634352  3198 net.cpp:454] accuracy <- label_data_1_split_0
I1209 13:54:14.634356  3198 net.cpp:411] accuracy -> accuracy
I1209 13:54:14.634385  3198 net.cpp:150] Setting up accuracy
I1209 13:54:14.634389  3198 net.cpp:157] Top shape: (1)
I1209 13:54:14.634392  3198 net.cpp:165] Memory required for data: 177157404
I1209 13:54:14.634393  3198 layer_factory.hpp:76] Creating layer loss
I1209 13:54:14.634407  3198 net.cpp:106] Creating Layer loss
I1209 13:54:14.634409  3198 net.cpp:454] loss <- fc8_fc8_0_split_1
I1209 13:54:14.634412  3198 net.cpp:454] loss <- label_data_1_split_1
I1209 13:54:14.634428  3198 net.cpp:411] loss -> loss
I1209 13:54:14.634433  3198 layer_factory.hpp:76] Creating layer loss
I1209 13:54:14.635169  3198 net.cpp:150] Setting up loss
I1209 13:54:14.635177  3198 net.cpp:157] Top shape: (1)
I1209 13:54:14.635190  3198 net.cpp:160]     with loss weight 1
I1209 13:54:14.635196  3198 net.cpp:165] Memory required for data: 177157408
I1209 13:54:14.635200  3198 net.cpp:226] loss needs backward computation.
I1209 13:54:14.635201  3198 net.cpp:228] accuracy does not need backward computation.
I1209 13:54:14.635205  3198 net.cpp:226] fc8_fc8_0_split needs backward computation.
I1209 13:54:14.635216  3198 net.cpp:226] fc8 needs backward computation.
I1209 13:54:14.635218  3198 net.cpp:226] drop7 needs backward computation.
I1209 13:54:14.635221  3198 net.cpp:226] relu7 needs backward computation.
I1209 13:54:14.635223  3198 net.cpp:226] ea_BN7 needs backward computation.
I1209 13:54:14.635226  3198 net.cpp:226] fc7_BN needs backward computation.
I1209 13:54:14.635227  3198 net.cpp:226] fc7 needs backward computation.
I1209 13:54:14.635231  3198 net.cpp:226] drop6 needs backward computation.
I1209 13:54:14.635232  3198 net.cpp:226] relu6 needs backward computation.
I1209 13:54:14.635234  3198 net.cpp:226] ea_BN6 needs backward computation.
I1209 13:54:14.635236  3198 net.cpp:226] fc6_BN needs backward computation.
I1209 13:54:14.635239  3198 net.cpp:226] fc6 needs backward computation.
I1209 13:54:14.635257  3198 net.cpp:226] pool5 needs backward computation.
I1209 13:54:14.635269  3198 net.cpp:226] relu5 needs backward computation.
I1209 13:54:14.635272  3198 net.cpp:226] ea_BN5 needs backward computation.
I1209 13:54:14.635274  3198 net.cpp:226] conv5_BN needs backward computation.
I1209 13:54:14.635277  3198 net.cpp:226] conv5 needs backward computation.
I1209 13:54:14.635289  3198 net.cpp:226] relu4 needs backward computation.
I1209 13:54:14.635293  3198 net.cpp:226] ea_BN4 needs backward computation.
I1209 13:54:14.635294  3198 net.cpp:226] conv4_BN needs backward computation.
I1209 13:54:14.635296  3198 net.cpp:226] conv4 needs backward computation.
I1209 13:54:14.635319  3198 net.cpp:226] relu3 needs backward computation.
I1209 13:54:14.635323  3198 net.cpp:226] ea_BN3 needs backward computation.
I1209 13:54:14.635324  3198 net.cpp:226] conv3_BN needs backward computation.
I1209 13:54:14.635336  3198 net.cpp:226] conv3 needs backward computation.
I1209 13:54:14.635339  3198 net.cpp:226] pool2 needs backward computation.
I1209 13:54:14.635352  3198 net.cpp:226] relu2 needs backward computation.
I1209 13:54:14.635356  3198 net.cpp:226] ea_BN2 needs backward computation.
I1209 13:54:14.635359  3198 net.cpp:226] conv2_BN needs backward computation.
I1209 13:54:14.635370  3198 net.cpp:226] conv2 needs backward computation.
I1209 13:54:14.635371  3198 net.cpp:226] pool1 needs backward computation.
I1209 13:54:14.635375  3198 net.cpp:226] relu1 needs backward computation.
I1209 13:54:14.635387  3198 net.cpp:226] ea_BN1 needs backward computation.
I1209 13:54:14.635390  3198 net.cpp:226] conv1_BN needs backward computation.
I1209 13:54:14.635392  3198 net.cpp:226] conv1 needs backward computation.
I1209 13:54:14.635395  3198 net.cpp:228] label_data_1_split does not need backward computation.
I1209 13:54:14.635397  3198 net.cpp:228] data does not need backward computation.
I1209 13:54:14.635399  3198 net.cpp:270] This network produces output accuracy
I1209 13:54:14.635401  3198 net.cpp:270] This network produces output loss
I1209 13:54:14.635421  3198 net.cpp:283] Network initialization done.
I1209 13:54:14.635596  3198 solver.cpp:60] Solver scaffolding done.
I1209 13:54:14.636943  3198 caffe.cpp:128] Finetuning from ./caffenet128_lsuv_no_lrn_BatchNormBeforeReLU_EltwiseAffine.prototxt.caffemodel
I1209 13:54:14.773409  3198 caffe.cpp:212] Starting Optimization
I1209 13:54:14.773437  3198 solver.cpp:288] Solving CaffeNet
I1209 13:54:14.773439  3198 solver.cpp:289] Learning Rate Policy: step
I1209 13:54:14.775087  3198 solver.cpp:341] Iteration 0, Testing net (#0)
I1209 13:54:14.833355  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 13:54:43.758113  3212 blocking_queue.cpp:50] Waiting for data
I1209 13:55:37.397548  3198 solver.cpp:409]     Test net output #0: accuracy = 0.00108
I1209 13:55:37.397627  3198 solver.cpp:409]     Test net output #1: loss = 8.98755 (* 1 = 8.98755 loss)
I1209 13:55:37.610376  3198 solver.cpp:237] Iteration 0, loss = 7.3475
I1209 13:55:37.610414  3198 solver.cpp:253]     Train net output #0: loss = 7.3475 (* 1 = 7.3475 loss)
I1209 13:55:37.610435  3198 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 13:55:39.636734  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 13:55:44.526083  3198 solver.cpp:237] Iteration 20, loss = 7.30497
I1209 13:55:44.526118  3198 solver.cpp:253]     Train net output #0: loss = 7.30497 (* 1 = 7.30497 loss)
I1209 13:55:44.526124  3198 sgd_solver.cpp:106] Iteration 20, lr = 0.01
I1209 13:55:52.106433  3198 solver.cpp:237] Iteration 40, loss = 7.29416
I1209 13:55:52.106472  3198 solver.cpp:253]     Train net output #0: loss = 7.29416 (* 1 = 7.29416 loss)
I1209 13:55:52.106480  3198 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I1209 13:55:59.767992  3198 solver.cpp:237] Iteration 60, loss = 7.22115
I1209 13:55:59.768029  3198 solver.cpp:253]     Train net output #0: loss = 7.22115 (* 1 = 7.22115 loss)
I1209 13:55:59.768035  3198 sgd_solver.cpp:106] Iteration 60, lr = 0.01
I1209 13:56:07.463328  3198 solver.cpp:237] Iteration 80, loss = 7.12967
I1209 13:56:07.463480  3198 solver.cpp:253]     Train net output #0: loss = 7.12967 (* 1 = 7.12967 loss)
I1209 13:56:07.463488  3198 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I1209 13:56:15.123853  3198 solver.cpp:237] Iteration 100, loss = 7.02511
I1209 13:56:15.123889  3198 solver.cpp:253]     Train net output #0: loss = 7.02511 (* 1 = 7.02511 loss)
I1209 13:56:15.123895  3198 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I1209 13:56:22.707644  3198 solver.cpp:237] Iteration 120, loss = 7.12449
I1209 13:56:22.707681  3198 solver.cpp:253]     Train net output #0: loss = 7.12449 (* 1 = 7.12449 loss)
I1209 13:56:22.707687  3198 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I1209 13:56:30.296556  3198 solver.cpp:237] Iteration 140, loss = 7.04657
I1209 13:56:30.296591  3198 solver.cpp:253]     Train net output #0: loss = 7.04657 (* 1 = 7.04657 loss)
I1209 13:56:30.296597  3198 sgd_solver.cpp:106] Iteration 140, lr = 0.01
I1209 13:56:37.917454  3198 solver.cpp:237] Iteration 160, loss = 7.01286
I1209 13:56:37.917603  3198 solver.cpp:253]     Train net output #0: loss = 7.01286 (* 1 = 7.01286 loss)
I1209 13:56:37.917620  3198 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I1209 13:56:45.525828  3198 solver.cpp:237] Iteration 180, loss = 6.97989
I1209 13:56:45.525866  3198 solver.cpp:253]     Train net output #0: loss = 6.97989 (* 1 = 6.97989 loss)
I1209 13:56:45.525873  3198 sgd_solver.cpp:106] Iteration 180, lr = 0.01
I1209 13:56:53.155757  3198 solver.cpp:237] Iteration 200, loss = 6.93853
I1209 13:56:53.155793  3198 solver.cpp:253]     Train net output #0: loss = 6.93853 (* 1 = 6.93853 loss)
I1209 13:56:53.155798  3198 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I1209 13:57:00.805001  3198 solver.cpp:237] Iteration 220, loss = 7.02311
I1209 13:57:00.805037  3198 solver.cpp:253]     Train net output #0: loss = 7.02311 (* 1 = 7.02311 loss)
I1209 13:57:00.805043  3198 sgd_solver.cpp:106] Iteration 220, lr = 0.01
I1209 13:57:08.372030  3198 solver.cpp:237] Iteration 240, loss = 6.97786
I1209 13:57:08.372203  3198 solver.cpp:253]     Train net output #0: loss = 6.97786 (* 1 = 6.97786 loss)
I1209 13:57:08.372211  3198 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I1209 13:57:15.964790  3198 solver.cpp:237] Iteration 260, loss = 6.87331
I1209 13:57:15.964828  3198 solver.cpp:253]     Train net output #0: loss = 6.87331 (* 1 = 6.87331 loss)
I1209 13:57:15.964834  3198 sgd_solver.cpp:106] Iteration 260, lr = 0.01
I1209 13:57:23.557765  3198 solver.cpp:237] Iteration 280, loss = 6.92564
I1209 13:57:23.557801  3198 solver.cpp:253]     Train net output #0: loss = 6.92564 (* 1 = 6.92564 loss)
I1209 13:57:23.557808  3198 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I1209 13:57:31.192759  3198 solver.cpp:237] Iteration 300, loss = 6.97981
I1209 13:57:31.192795  3198 solver.cpp:253]     Train net output #0: loss = 6.97981 (* 1 = 6.97981 loss)
I1209 13:57:31.192800  3198 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I1209 13:57:38.782318  3198 solver.cpp:237] Iteration 320, loss = 6.92078
I1209 13:57:38.782487  3198 solver.cpp:253]     Train net output #0: loss = 6.92078 (* 1 = 6.92078 loss)
I1209 13:57:38.782495  3198 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I1209 13:57:46.446625  3198 solver.cpp:237] Iteration 340, loss = 6.76807
I1209 13:57:46.446662  3198 solver.cpp:253]     Train net output #0: loss = 6.76807 (* 1 = 6.76807 loss)
I1209 13:57:46.446669  3198 sgd_solver.cpp:106] Iteration 340, lr = 0.01
I1209 13:57:54.063634  3198 solver.cpp:237] Iteration 360, loss = 6.84638
I1209 13:57:54.063673  3198 solver.cpp:253]     Train net output #0: loss = 6.84638 (* 1 = 6.84638 loss)
I1209 13:57:54.063678  3198 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I1209 13:58:01.681483  3198 solver.cpp:237] Iteration 380, loss = 6.67539
I1209 13:58:01.681521  3198 solver.cpp:253]     Train net output #0: loss = 6.67539 (* 1 = 6.67539 loss)
I1209 13:58:01.681527  3198 sgd_solver.cpp:106] Iteration 380, lr = 0.01
I1209 13:58:09.300868  3198 solver.cpp:237] Iteration 400, loss = 6.74585
I1209 13:58:09.301023  3198 solver.cpp:253]     Train net output #0: loss = 6.74585 (* 1 = 6.74585 loss)
I1209 13:58:09.301030  3198 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I1209 13:58:16.906041  3198 solver.cpp:237] Iteration 420, loss = 6.57197
I1209 13:58:16.906077  3198 solver.cpp:253]     Train net output #0: loss = 6.57197 (* 1 = 6.57197 loss)
I1209 13:58:16.906083  3198 sgd_solver.cpp:106] Iteration 420, lr = 0.01
I1209 13:58:24.514832  3198 solver.cpp:237] Iteration 440, loss = 6.67376
I1209 13:58:24.514868  3198 solver.cpp:253]     Train net output #0: loss = 6.67376 (* 1 = 6.67376 loss)
I1209 13:58:24.514874  3198 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I1209 13:58:32.110018  3198 solver.cpp:237] Iteration 460, loss = 6.67629
I1209 13:58:32.110066  3198 solver.cpp:253]     Train net output #0: loss = 6.67629 (* 1 = 6.67629 loss)
I1209 13:58:32.110072  3198 sgd_solver.cpp:106] Iteration 460, lr = 0.01
I1209 13:58:39.705560  3198 solver.cpp:237] Iteration 480, loss = 6.70327
I1209 13:58:39.705718  3198 solver.cpp:253]     Train net output #0: loss = 6.70327 (* 1 = 6.70327 loss)
I1209 13:58:39.705725  3198 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I1209 13:58:47.303107  3198 solver.cpp:237] Iteration 500, loss = 6.72068
I1209 13:58:47.303174  3198 solver.cpp:253]     Train net output #0: loss = 6.72068 (* 1 = 6.72068 loss)
I1209 13:58:47.303190  3198 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I1209 13:58:54.935081  3198 solver.cpp:237] Iteration 520, loss = 6.69376
I1209 13:58:54.935117  3198 solver.cpp:253]     Train net output #0: loss = 6.69376 (* 1 = 6.69376 loss)
I1209 13:58:54.935123  3198 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I1209 13:59:02.546031  3198 solver.cpp:237] Iteration 540, loss = 6.60515
I1209 13:59:02.546067  3198 solver.cpp:253]     Train net output #0: loss = 6.60515 (* 1 = 6.60515 loss)
I1209 13:59:02.546073  3198 sgd_solver.cpp:106] Iteration 540, lr = 0.01
I1209 13:59:10.148061  3198 solver.cpp:237] Iteration 560, loss = 6.53358
I1209 13:59:10.148154  3198 solver.cpp:253]     Train net output #0: loss = 6.53358 (* 1 = 6.53358 loss)
I1209 13:59:10.148161  3198 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I1209 13:59:17.784596  3198 solver.cpp:237] Iteration 580, loss = 6.53363
I1209 13:59:17.784634  3198 solver.cpp:253]     Train net output #0: loss = 6.53363 (* 1 = 6.53363 loss)
I1209 13:59:17.784649  3198 sgd_solver.cpp:106] Iteration 580, lr = 0.01
I1209 13:59:25.422509  3198 solver.cpp:237] Iteration 600, loss = 6.59799
I1209 13:59:25.422547  3198 solver.cpp:253]     Train net output #0: loss = 6.59799 (* 1 = 6.59799 loss)
I1209 13:59:25.422552  3198 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I1209 13:59:33.062227  3198 solver.cpp:237] Iteration 620, loss = 6.43618
I1209 13:59:33.062264  3198 solver.cpp:253]     Train net output #0: loss = 6.43618 (* 1 = 6.43618 loss)
I1209 13:59:33.062270  3198 sgd_solver.cpp:106] Iteration 620, lr = 0.01
I1209 13:59:41.922054  3198 solver.cpp:237] Iteration 640, loss = 6.67284
I1209 13:59:41.922214  3198 solver.cpp:253]     Train net output #0: loss = 6.67284 (* 1 = 6.67284 loss)
I1209 13:59:41.922220  3198 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I1209 13:59:49.672248  3198 solver.cpp:237] Iteration 660, loss = 6.45453
I1209 13:59:49.672287  3198 solver.cpp:253]     Train net output #0: loss = 6.45453 (* 1 = 6.45453 loss)
I1209 13:59:49.672293  3198 sgd_solver.cpp:106] Iteration 660, lr = 0.01
I1209 13:59:57.300545  3198 solver.cpp:237] Iteration 680, loss = 6.42166
I1209 13:59:57.300582  3198 solver.cpp:253]     Train net output #0: loss = 6.42166 (* 1 = 6.42166 loss)
I1209 13:59:57.300588  3198 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I1209 14:00:04.944627  3198 solver.cpp:237] Iteration 700, loss = 6.58071
I1209 14:00:04.944663  3198 solver.cpp:253]     Train net output #0: loss = 6.58071 (* 1 = 6.58071 loss)
I1209 14:00:04.944669  3198 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I1209 14:00:12.642608  3198 solver.cpp:237] Iteration 720, loss = 6.42209
I1209 14:00:12.642770  3198 solver.cpp:253]     Train net output #0: loss = 6.42209 (* 1 = 6.42209 loss)
I1209 14:00:12.642776  3198 sgd_solver.cpp:106] Iteration 720, lr = 0.01
I1209 14:00:20.291271  3198 solver.cpp:237] Iteration 740, loss = 6.49071
I1209 14:00:20.291308  3198 solver.cpp:253]     Train net output #0: loss = 6.49071 (* 1 = 6.49071 loss)
I1209 14:00:20.291314  3198 sgd_solver.cpp:106] Iteration 740, lr = 0.01
I1209 14:00:27.959622  3198 solver.cpp:237] Iteration 760, loss = 6.53029
I1209 14:00:27.959661  3198 solver.cpp:253]     Train net output #0: loss = 6.53029 (* 1 = 6.53029 loss)
I1209 14:00:27.959667  3198 sgd_solver.cpp:106] Iteration 760, lr = 0.01
I1209 14:00:35.569210  3198 solver.cpp:237] Iteration 780, loss = 6.50146
I1209 14:00:35.569244  3198 solver.cpp:253]     Train net output #0: loss = 6.50146 (* 1 = 6.50146 loss)
I1209 14:00:35.569252  3198 sgd_solver.cpp:106] Iteration 780, lr = 0.01
I1209 14:00:43.184245  3198 solver.cpp:237] Iteration 800, loss = 6.40156
I1209 14:00:43.184401  3198 solver.cpp:253]     Train net output #0: loss = 6.40156 (* 1 = 6.40156 loss)
I1209 14:00:43.184408  3198 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I1209 14:00:50.886978  3198 solver.cpp:237] Iteration 820, loss = 6.42904
I1209 14:00:50.887013  3198 solver.cpp:253]     Train net output #0: loss = 6.42904 (* 1 = 6.42904 loss)
I1209 14:00:50.887020  3198 sgd_solver.cpp:106] Iteration 820, lr = 0.01
I1209 14:00:58.544384  3198 solver.cpp:237] Iteration 840, loss = 6.46884
I1209 14:00:58.544420  3198 solver.cpp:253]     Train net output #0: loss = 6.46884 (* 1 = 6.46884 loss)
I1209 14:00:58.544426  3198 sgd_solver.cpp:106] Iteration 840, lr = 0.01
I1209 14:01:06.226773  3198 solver.cpp:237] Iteration 860, loss = 6.2106
I1209 14:01:06.226809  3198 solver.cpp:253]     Train net output #0: loss = 6.2106 (* 1 = 6.2106 loss)
I1209 14:01:06.226814  3198 sgd_solver.cpp:106] Iteration 860, lr = 0.01
I1209 14:01:13.854543  3198 solver.cpp:237] Iteration 880, loss = 6.28143
I1209 14:01:13.854697  3198 solver.cpp:253]     Train net output #0: loss = 6.28143 (* 1 = 6.28143 loss)
I1209 14:01:13.854704  3198 sgd_solver.cpp:106] Iteration 880, lr = 0.01
I1209 14:01:21.539121  3198 solver.cpp:237] Iteration 900, loss = 6.3553
I1209 14:01:21.539158  3198 solver.cpp:253]     Train net output #0: loss = 6.3553 (* 1 = 6.3553 loss)
I1209 14:01:21.539165  3198 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I1209 14:01:29.213253  3198 solver.cpp:237] Iteration 920, loss = 6.31328
I1209 14:01:29.213325  3198 solver.cpp:253]     Train net output #0: loss = 6.31328 (* 1 = 6.31328 loss)
I1209 14:01:29.213343  3198 sgd_solver.cpp:106] Iteration 920, lr = 0.01
I1209 14:01:36.888104  3198 solver.cpp:237] Iteration 940, loss = 6.25754
I1209 14:01:36.888140  3198 solver.cpp:253]     Train net output #0: loss = 6.25754 (* 1 = 6.25754 loss)
I1209 14:01:36.888146  3198 sgd_solver.cpp:106] Iteration 940, lr = 0.01
I1209 14:01:44.529811  3198 solver.cpp:237] Iteration 960, loss = 6.35169
I1209 14:01:44.529980  3198 solver.cpp:253]     Train net output #0: loss = 6.35169 (* 1 = 6.35169 loss)
I1209 14:01:44.529989  3198 sgd_solver.cpp:106] Iteration 960, lr = 0.01
I1209 14:01:52.153010  3198 solver.cpp:237] Iteration 980, loss = 6.35092
I1209 14:01:52.153046  3198 solver.cpp:253]     Train net output #0: loss = 6.35092 (* 1 = 6.35092 loss)
I1209 14:01:52.153062  3198 sgd_solver.cpp:106] Iteration 980, lr = 0.01
I1209 14:01:59.483219  3198 solver.cpp:341] Iteration 1000, Testing net (#0)
I1209 14:02:00.131736  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:03:16.191043  3198 solver.cpp:409]     Test net output #0: accuracy = 0.0259802
I1209 14:03:16.191170  3198 solver.cpp:409]     Test net output #1: loss = 6.04308 (* 1 = 6.04308 loss)
I1209 14:03:16.386055  3198 solver.cpp:237] Iteration 1000, loss = 6.50704
I1209 14:03:16.386092  3198 solver.cpp:253]     Train net output #0: loss = 6.50704 (* 1 = 6.50704 loss)
I1209 14:03:16.386099  3198 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I1209 14:03:21.478449  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:03:23.341135  3198 solver.cpp:237] Iteration 1020, loss = 6.11826
I1209 14:03:23.341173  3198 solver.cpp:253]     Train net output #0: loss = 6.11826 (* 1 = 6.11826 loss)
I1209 14:03:23.341179  3198 sgd_solver.cpp:106] Iteration 1020, lr = 0.01
I1209 14:03:30.960085  3198 solver.cpp:237] Iteration 1040, loss = 6.31224
I1209 14:03:30.960122  3198 solver.cpp:253]     Train net output #0: loss = 6.31224 (* 1 = 6.31224 loss)
I1209 14:03:30.960129  3198 sgd_solver.cpp:106] Iteration 1040, lr = 0.01
I1209 14:03:38.637437  3198 solver.cpp:237] Iteration 1060, loss = 6.20925
I1209 14:03:38.637475  3198 solver.cpp:253]     Train net output #0: loss = 6.20925 (* 1 = 6.20925 loss)
I1209 14:03:38.637482  3198 sgd_solver.cpp:106] Iteration 1060, lr = 0.01
I1209 14:03:46.320691  3198 solver.cpp:237] Iteration 1080, loss = 6.23884
I1209 14:03:46.320847  3198 solver.cpp:253]     Train net output #0: loss = 6.23884 (* 1 = 6.23884 loss)
I1209 14:03:46.320855  3198 sgd_solver.cpp:106] Iteration 1080, lr = 0.01
I1209 14:03:54.000336  3198 solver.cpp:237] Iteration 1100, loss = 6.26391
I1209 14:03:54.000372  3198 solver.cpp:253]     Train net output #0: loss = 6.26391 (* 1 = 6.26391 loss)
I1209 14:03:54.000378  3198 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I1209 14:04:01.652463  3198 solver.cpp:237] Iteration 1120, loss = 6.11724
I1209 14:04:01.652499  3198 solver.cpp:253]     Train net output #0: loss = 6.11724 (* 1 = 6.11724 loss)
I1209 14:04:01.652505  3198 sgd_solver.cpp:106] Iteration 1120, lr = 0.01
I1209 14:04:09.305130  3198 solver.cpp:237] Iteration 1140, loss = 6.36805
I1209 14:04:09.305163  3198 solver.cpp:253]     Train net output #0: loss = 6.36805 (* 1 = 6.36805 loss)
I1209 14:04:09.305169  3198 sgd_solver.cpp:106] Iteration 1140, lr = 0.01
I1209 14:04:17.004254  3198 solver.cpp:237] Iteration 1160, loss = 6.18356
I1209 14:04:17.004410  3198 solver.cpp:253]     Train net output #0: loss = 6.18356 (* 1 = 6.18356 loss)
I1209 14:04:17.004417  3198 sgd_solver.cpp:106] Iteration 1160, lr = 0.01
I1209 14:04:24.651197  3198 solver.cpp:237] Iteration 1180, loss = 6.08856
I1209 14:04:24.651234  3198 solver.cpp:253]     Train net output #0: loss = 6.08856 (* 1 = 6.08856 loss)
I1209 14:04:24.651240  3198 sgd_solver.cpp:106] Iteration 1180, lr = 0.01
I1209 14:04:32.302914  3198 solver.cpp:237] Iteration 1200, loss = 6.1188
I1209 14:04:32.302950  3198 solver.cpp:253]     Train net output #0: loss = 6.1188 (* 1 = 6.1188 loss)
I1209 14:04:32.302958  3198 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I1209 14:04:39.970507  3198 solver.cpp:237] Iteration 1220, loss = 6.08367
I1209 14:04:39.970546  3198 solver.cpp:253]     Train net output #0: loss = 6.08367 (* 1 = 6.08367 loss)
I1209 14:04:39.970551  3198 sgd_solver.cpp:106] Iteration 1220, lr = 0.01
I1209 14:04:47.632953  3198 solver.cpp:237] Iteration 1240, loss = 6.02708
I1209 14:04:47.633095  3198 solver.cpp:253]     Train net output #0: loss = 6.02708 (* 1 = 6.02708 loss)
I1209 14:04:47.633103  3198 sgd_solver.cpp:106] Iteration 1240, lr = 0.01
I1209 14:04:56.647552  3198 solver.cpp:237] Iteration 1260, loss = 5.96856
I1209 14:04:56.647588  3198 solver.cpp:253]     Train net output #0: loss = 5.96856 (* 1 = 5.96856 loss)
I1209 14:04:56.647594  3198 sgd_solver.cpp:106] Iteration 1260, lr = 0.01
I1209 14:05:04.331353  3198 solver.cpp:237] Iteration 1280, loss = 6.27919
I1209 14:05:04.331390  3198 solver.cpp:253]     Train net output #0: loss = 6.27919 (* 1 = 6.27919 loss)
I1209 14:05:04.331398  3198 sgd_solver.cpp:106] Iteration 1280, lr = 0.01
I1209 14:05:12.000766  3198 solver.cpp:237] Iteration 1300, loss = 6.30975
I1209 14:05:12.000802  3198 solver.cpp:253]     Train net output #0: loss = 6.30975 (* 1 = 6.30975 loss)
I1209 14:05:12.000808  3198 sgd_solver.cpp:106] Iteration 1300, lr = 0.01
I1209 14:05:19.630774  3198 solver.cpp:237] Iteration 1320, loss = 6.14134
I1209 14:05:19.630944  3198 solver.cpp:253]     Train net output #0: loss = 6.14134 (* 1 = 6.14134 loss)
I1209 14:05:19.630950  3198 sgd_solver.cpp:106] Iteration 1320, lr = 0.01
I1209 14:05:27.315722  3198 solver.cpp:237] Iteration 1340, loss = 5.95195
I1209 14:05:27.315758  3198 solver.cpp:253]     Train net output #0: loss = 5.95195 (* 1 = 5.95195 loss)
I1209 14:05:27.315764  3198 sgd_solver.cpp:106] Iteration 1340, lr = 0.01
I1209 14:05:34.974647  3198 solver.cpp:237] Iteration 1360, loss = 6.21973
I1209 14:05:34.974732  3198 solver.cpp:253]     Train net output #0: loss = 6.21973 (* 1 = 6.21973 loss)
I1209 14:05:34.974756  3198 sgd_solver.cpp:106] Iteration 1360, lr = 0.01
I1209 14:05:42.620439  3198 solver.cpp:237] Iteration 1380, loss = 6.1892
I1209 14:05:42.620496  3198 solver.cpp:253]     Train net output #0: loss = 6.1892 (* 1 = 6.1892 loss)
I1209 14:05:42.620508  3198 sgd_solver.cpp:106] Iteration 1380, lr = 0.01
I1209 14:05:50.321545  3198 solver.cpp:237] Iteration 1400, loss = 6.1691
I1209 14:05:50.321703  3198 solver.cpp:253]     Train net output #0: loss = 6.1691 (* 1 = 6.1691 loss)
I1209 14:05:50.321712  3198 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I1209 14:05:57.986533  3198 solver.cpp:237] Iteration 1420, loss = 6.14766
I1209 14:05:57.986569  3198 solver.cpp:253]     Train net output #0: loss = 6.14766 (* 1 = 6.14766 loss)
I1209 14:05:57.986577  3198 sgd_solver.cpp:106] Iteration 1420, lr = 0.01
I1209 14:06:05.646839  3198 solver.cpp:237] Iteration 1440, loss = 6.20483
I1209 14:06:05.646878  3198 solver.cpp:253]     Train net output #0: loss = 6.20483 (* 1 = 6.20483 loss)
I1209 14:06:05.646883  3198 sgd_solver.cpp:106] Iteration 1440, lr = 0.01
I1209 14:06:13.297906  3198 solver.cpp:237] Iteration 1460, loss = 6.25385
I1209 14:06:13.297943  3198 solver.cpp:253]     Train net output #0: loss = 6.25385 (* 1 = 6.25385 loss)
I1209 14:06:13.297950  3198 sgd_solver.cpp:106] Iteration 1460, lr = 0.01
I1209 14:06:20.865494  3198 solver.cpp:237] Iteration 1480, loss = 6.11529
I1209 14:06:20.865619  3198 solver.cpp:253]     Train net output #0: loss = 6.11529 (* 1 = 6.11529 loss)
I1209 14:06:20.865634  3198 sgd_solver.cpp:106] Iteration 1480, lr = 0.01
I1209 14:06:28.555723  3198 solver.cpp:237] Iteration 1500, loss = 6.1388
I1209 14:06:28.555758  3198 solver.cpp:253]     Train net output #0: loss = 6.1388 (* 1 = 6.1388 loss)
I1209 14:06:28.555764  3198 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I1209 14:06:36.236405  3198 solver.cpp:237] Iteration 1520, loss = 5.94486
I1209 14:06:36.236443  3198 solver.cpp:253]     Train net output #0: loss = 5.94486 (* 1 = 5.94486 loss)
I1209 14:06:36.236449  3198 sgd_solver.cpp:106] Iteration 1520, lr = 0.01
I1209 14:06:43.916575  3198 solver.cpp:237] Iteration 1540, loss = 6.17826
I1209 14:06:43.916610  3198 solver.cpp:253]     Train net output #0: loss = 6.17826 (* 1 = 6.17826 loss)
I1209 14:06:43.916617  3198 sgd_solver.cpp:106] Iteration 1540, lr = 0.01
I1209 14:06:51.592437  3198 solver.cpp:237] Iteration 1560, loss = 6.0623
I1209 14:06:51.592572  3198 solver.cpp:253]     Train net output #0: loss = 6.0623 (* 1 = 6.0623 loss)
I1209 14:06:51.592592  3198 sgd_solver.cpp:106] Iteration 1560, lr = 0.01
I1209 14:06:59.227394  3198 solver.cpp:237] Iteration 1580, loss = 5.80975
I1209 14:06:59.227432  3198 solver.cpp:253]     Train net output #0: loss = 5.80975 (* 1 = 5.80975 loss)
I1209 14:06:59.227437  3198 sgd_solver.cpp:106] Iteration 1580, lr = 0.01
I1209 14:07:06.937985  3198 solver.cpp:237] Iteration 1600, loss = 6.23815
I1209 14:07:06.938032  3198 solver.cpp:253]     Train net output #0: loss = 6.23815 (* 1 = 6.23815 loss)
I1209 14:07:06.938038  3198 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I1209 14:07:14.574573  3198 solver.cpp:237] Iteration 1620, loss = 6.13137
I1209 14:07:14.574607  3198 solver.cpp:253]     Train net output #0: loss = 6.13137 (* 1 = 6.13137 loss)
I1209 14:07:14.574614  3198 sgd_solver.cpp:106] Iteration 1620, lr = 0.01
I1209 14:07:22.268844  3198 solver.cpp:237] Iteration 1640, loss = 5.98211
I1209 14:07:22.269026  3198 solver.cpp:253]     Train net output #0: loss = 5.98211 (* 1 = 5.98211 loss)
I1209 14:07:22.269033  3198 sgd_solver.cpp:106] Iteration 1640, lr = 0.01
I1209 14:07:29.912363  3198 solver.cpp:237] Iteration 1660, loss = 6.19771
I1209 14:07:29.912400  3198 solver.cpp:253]     Train net output #0: loss = 6.19771 (* 1 = 6.19771 loss)
I1209 14:07:29.912406  3198 sgd_solver.cpp:106] Iteration 1660, lr = 0.01
I1209 14:07:37.589027  3198 solver.cpp:237] Iteration 1680, loss = 6.00736
I1209 14:07:37.589064  3198 solver.cpp:253]     Train net output #0: loss = 6.00736 (* 1 = 6.00736 loss)
I1209 14:07:37.589071  3198 sgd_solver.cpp:106] Iteration 1680, lr = 0.01
I1209 14:07:45.232372  3198 solver.cpp:237] Iteration 1700, loss = 6.01939
I1209 14:07:45.232409  3198 solver.cpp:253]     Train net output #0: loss = 6.01939 (* 1 = 6.01939 loss)
I1209 14:07:45.232415  3198 sgd_solver.cpp:106] Iteration 1700, lr = 0.01
I1209 14:07:52.898752  3198 solver.cpp:237] Iteration 1720, loss = 6.11826
I1209 14:07:52.898864  3198 solver.cpp:253]     Train net output #0: loss = 6.11826 (* 1 = 6.11826 loss)
I1209 14:07:52.898880  3198 sgd_solver.cpp:106] Iteration 1720, lr = 0.01
I1209 14:08:00.539700  3198 solver.cpp:237] Iteration 1740, loss = 6.1055
I1209 14:08:00.539736  3198 solver.cpp:253]     Train net output #0: loss = 6.1055 (* 1 = 6.1055 loss)
I1209 14:08:00.539742  3198 sgd_solver.cpp:106] Iteration 1740, lr = 0.01
I1209 14:08:08.201606  3198 solver.cpp:237] Iteration 1760, loss = 6.08308
I1209 14:08:08.201643  3198 solver.cpp:253]     Train net output #0: loss = 6.08308 (* 1 = 6.08308 loss)
I1209 14:08:08.201649  3198 sgd_solver.cpp:106] Iteration 1760, lr = 0.01
I1209 14:08:15.803112  3198 solver.cpp:237] Iteration 1780, loss = 5.95208
I1209 14:08:15.803149  3198 solver.cpp:253]     Train net output #0: loss = 5.95208 (* 1 = 5.95208 loss)
I1209 14:08:15.803155  3198 sgd_solver.cpp:106] Iteration 1780, lr = 0.01
I1209 14:08:23.475875  3198 solver.cpp:237] Iteration 1800, loss = 6.02192
I1209 14:08:23.476052  3198 solver.cpp:253]     Train net output #0: loss = 6.02192 (* 1 = 6.02192 loss)
I1209 14:08:23.476070  3198 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I1209 14:08:31.158905  3198 solver.cpp:237] Iteration 1820, loss = 5.84893
I1209 14:08:31.158937  3198 solver.cpp:253]     Train net output #0: loss = 5.84893 (* 1 = 5.84893 loss)
I1209 14:08:31.158943  3198 sgd_solver.cpp:106] Iteration 1820, lr = 0.01
I1209 14:08:38.820960  3198 solver.cpp:237] Iteration 1840, loss = 6.13729
I1209 14:08:38.820996  3198 solver.cpp:253]     Train net output #0: loss = 6.13729 (* 1 = 6.13729 loss)
I1209 14:08:38.821002  3198 sgd_solver.cpp:106] Iteration 1840, lr = 0.01
I1209 14:08:46.443898  3198 solver.cpp:237] Iteration 1860, loss = 5.86843
I1209 14:08:46.443940  3198 solver.cpp:253]     Train net output #0: loss = 5.86843 (* 1 = 5.86843 loss)
I1209 14:08:46.443946  3198 sgd_solver.cpp:106] Iteration 1860, lr = 0.01
I1209 14:08:54.103449  3198 solver.cpp:237] Iteration 1880, loss = 5.97384
I1209 14:08:54.103595  3198 solver.cpp:253]     Train net output #0: loss = 5.97384 (* 1 = 5.97384 loss)
I1209 14:08:54.103602  3198 sgd_solver.cpp:106] Iteration 1880, lr = 0.01
I1209 14:09:01.749675  3198 solver.cpp:237] Iteration 1900, loss = 6.16893
I1209 14:09:01.749711  3198 solver.cpp:253]     Train net output #0: loss = 6.16893 (* 1 = 6.16893 loss)
I1209 14:09:01.749716  3198 sgd_solver.cpp:106] Iteration 1900, lr = 0.01
I1209 14:09:09.393637  3198 solver.cpp:237] Iteration 1920, loss = 6.07644
I1209 14:09:09.393674  3198 solver.cpp:253]     Train net output #0: loss = 6.07644 (* 1 = 6.07644 loss)
I1209 14:09:09.393681  3198 sgd_solver.cpp:106] Iteration 1920, lr = 0.01
I1209 14:09:17.060616  3198 solver.cpp:237] Iteration 1940, loss = 5.9046
I1209 14:09:17.060654  3198 solver.cpp:253]     Train net output #0: loss = 5.9046 (* 1 = 5.9046 loss)
I1209 14:09:17.060660  3198 sgd_solver.cpp:106] Iteration 1940, lr = 0.01
I1209 14:09:24.742147  3198 solver.cpp:237] Iteration 1960, loss = 5.97532
I1209 14:09:24.742267  3198 solver.cpp:253]     Train net output #0: loss = 5.97532 (* 1 = 5.97532 loss)
I1209 14:09:24.742274  3198 sgd_solver.cpp:106] Iteration 1960, lr = 0.01
I1209 14:09:32.460675  3198 solver.cpp:237] Iteration 1980, loss = 5.78833
I1209 14:09:32.460711  3198 solver.cpp:253]     Train net output #0: loss = 5.78833 (* 1 = 5.78833 loss)
I1209 14:09:32.460717  3198 sgd_solver.cpp:106] Iteration 1980, lr = 0.01
I1209 14:09:39.783978  3198 solver.cpp:341] Iteration 2000, Testing net (#0)
I1209 14:09:41.081377  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:10:58.515270  3198 solver.cpp:409]     Test net output #0: accuracy = 0.0500204
I1209 14:10:58.515393  3198 solver.cpp:409]     Test net output #1: loss = 5.60375 (* 1 = 5.60375 loss)
I1209 14:10:58.713754  3198 solver.cpp:237] Iteration 2000, loss = 5.9041
I1209 14:10:58.713791  3198 solver.cpp:253]     Train net output #0: loss = 5.9041 (* 1 = 5.9041 loss)
I1209 14:10:58.713798  3198 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I1209 14:11:05.738054  3198 solver.cpp:237] Iteration 2020, loss = 6.15662
I1209 14:11:05.738090  3198 solver.cpp:253]     Train net output #0: loss = 6.15662 (* 1 = 6.15662 loss)
I1209 14:11:05.738096  3198 sgd_solver.cpp:106] Iteration 2020, lr = 0.01
I1209 14:11:06.914386  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:11:13.398110  3198 solver.cpp:237] Iteration 2040, loss = 5.8822
I1209 14:11:13.398149  3198 solver.cpp:253]     Train net output #0: loss = 5.8822 (* 1 = 5.8822 loss)
I1209 14:11:13.398154  3198 sgd_solver.cpp:106] Iteration 2040, lr = 0.01
I1209 14:11:21.049190  3198 solver.cpp:237] Iteration 2060, loss = 6.00483
I1209 14:11:21.049235  3198 solver.cpp:253]     Train net output #0: loss = 6.00483 (* 1 = 6.00483 loss)
I1209 14:11:21.049242  3198 sgd_solver.cpp:106] Iteration 2060, lr = 0.01
I1209 14:11:28.609555  3198 solver.cpp:237] Iteration 2080, loss = 6.08118
I1209 14:11:28.609643  3198 solver.cpp:253]     Train net output #0: loss = 6.08118 (* 1 = 6.08118 loss)
I1209 14:11:28.609650  3198 sgd_solver.cpp:106] Iteration 2080, lr = 0.01
I1209 14:11:36.276777  3198 solver.cpp:237] Iteration 2100, loss = 6.12866
I1209 14:11:36.276813  3198 solver.cpp:253]     Train net output #0: loss = 6.12866 (* 1 = 6.12866 loss)
I1209 14:11:36.276819  3198 sgd_solver.cpp:106] Iteration 2100, lr = 0.01
I1209 14:11:43.873606  3198 solver.cpp:237] Iteration 2120, loss = 5.8019
I1209 14:11:43.873642  3198 solver.cpp:253]     Train net output #0: loss = 5.8019 (* 1 = 5.8019 loss)
I1209 14:11:43.873648  3198 sgd_solver.cpp:106] Iteration 2120, lr = 0.01
I1209 14:11:51.547552  3198 solver.cpp:237] Iteration 2140, loss = 5.77456
I1209 14:11:51.547591  3198 solver.cpp:253]     Train net output #0: loss = 5.77456 (* 1 = 5.77456 loss)
I1209 14:11:51.547598  3198 sgd_solver.cpp:106] Iteration 2140, lr = 0.01
I1209 14:11:59.163285  3198 solver.cpp:237] Iteration 2160, loss = 5.9447
I1209 14:11:59.163400  3198 solver.cpp:253]     Train net output #0: loss = 5.9447 (* 1 = 5.9447 loss)
I1209 14:11:59.163406  3198 sgd_solver.cpp:106] Iteration 2160, lr = 0.01
I1209 14:12:06.752387  3198 solver.cpp:237] Iteration 2180, loss = 5.89913
I1209 14:12:06.752423  3198 solver.cpp:253]     Train net output #0: loss = 5.89913 (* 1 = 5.89913 loss)
I1209 14:12:06.752429  3198 sgd_solver.cpp:106] Iteration 2180, lr = 0.01
I1209 14:12:14.354003  3198 solver.cpp:237] Iteration 2200, loss = 5.93643
I1209 14:12:14.354038  3198 solver.cpp:253]     Train net output #0: loss = 5.93643 (* 1 = 5.93643 loss)
I1209 14:12:14.354045  3198 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I1209 14:12:21.955451  3198 solver.cpp:237] Iteration 2220, loss = 5.738
I1209 14:12:21.955489  3198 solver.cpp:253]     Train net output #0: loss = 5.738 (* 1 = 5.738 loss)
I1209 14:12:21.955495  3198 sgd_solver.cpp:106] Iteration 2220, lr = 0.01
I1209 14:12:29.588062  3198 solver.cpp:237] Iteration 2240, loss = 6.09577
I1209 14:12:29.588189  3198 solver.cpp:253]     Train net output #0: loss = 6.09577 (* 1 = 6.09577 loss)
I1209 14:12:29.588207  3198 sgd_solver.cpp:106] Iteration 2240, lr = 0.01
I1209 14:12:37.314401  3198 solver.cpp:237] Iteration 2260, loss = 6.00738
I1209 14:12:37.314438  3198 solver.cpp:253]     Train net output #0: loss = 6.00738 (* 1 = 6.00738 loss)
I1209 14:12:37.314445  3198 sgd_solver.cpp:106] Iteration 2260, lr = 0.01
I1209 14:12:44.962749  3198 solver.cpp:237] Iteration 2280, loss = 5.57272
I1209 14:12:44.962785  3198 solver.cpp:253]     Train net output #0: loss = 5.57272 (* 1 = 5.57272 loss)
I1209 14:12:44.962792  3198 sgd_solver.cpp:106] Iteration 2280, lr = 0.01
I1209 14:12:52.632262  3198 solver.cpp:237] Iteration 2300, loss = 5.71837
I1209 14:12:52.632299  3198 solver.cpp:253]     Train net output #0: loss = 5.71837 (* 1 = 5.71837 loss)
I1209 14:12:52.632307  3198 sgd_solver.cpp:106] Iteration 2300, lr = 0.01
I1209 14:13:00.303465  3198 solver.cpp:237] Iteration 2320, loss = 6.06527
I1209 14:13:00.303555  3198 solver.cpp:253]     Train net output #0: loss = 6.06527 (* 1 = 6.06527 loss)
I1209 14:13:00.303561  3198 sgd_solver.cpp:106] Iteration 2320, lr = 0.01
I1209 14:13:07.994685  3198 solver.cpp:237] Iteration 2340, loss = 5.64713
I1209 14:13:07.994719  3198 solver.cpp:253]     Train net output #0: loss = 5.64713 (* 1 = 5.64713 loss)
I1209 14:13:07.994725  3198 sgd_solver.cpp:106] Iteration 2340, lr = 0.01
I1209 14:13:15.634532  3198 solver.cpp:237] Iteration 2360, loss = 5.95217
I1209 14:13:15.634568  3198 solver.cpp:253]     Train net output #0: loss = 5.95217 (* 1 = 5.95217 loss)
I1209 14:13:15.634574  3198 sgd_solver.cpp:106] Iteration 2360, lr = 0.01
I1209 14:13:23.269839  3198 solver.cpp:237] Iteration 2380, loss = 5.92368
I1209 14:13:23.269876  3198 solver.cpp:253]     Train net output #0: loss = 5.92368 (* 1 = 5.92368 loss)
I1209 14:13:23.269881  3198 sgd_solver.cpp:106] Iteration 2380, lr = 0.01
I1209 14:13:30.930146  3198 solver.cpp:237] Iteration 2400, loss = 6.02985
I1209 14:13:30.930287  3198 solver.cpp:253]     Train net output #0: loss = 6.02985 (* 1 = 6.02985 loss)
I1209 14:13:30.930305  3198 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
I1209 14:13:38.585660  3198 solver.cpp:237] Iteration 2420, loss = 5.96831
I1209 14:13:38.585697  3198 solver.cpp:253]     Train net output #0: loss = 5.96831 (* 1 = 5.96831 loss)
I1209 14:13:38.585705  3198 sgd_solver.cpp:106] Iteration 2420, lr = 0.01
I1209 14:13:46.267952  3198 solver.cpp:237] Iteration 2440, loss = 5.9478
I1209 14:13:46.267988  3198 solver.cpp:253]     Train net output #0: loss = 5.9478 (* 1 = 5.9478 loss)
I1209 14:13:46.267994  3198 sgd_solver.cpp:106] Iteration 2440, lr = 0.01
I1209 14:13:53.926515  3198 solver.cpp:237] Iteration 2460, loss = 6.14997
I1209 14:13:53.926568  3198 solver.cpp:253]     Train net output #0: loss = 6.14997 (* 1 = 6.14997 loss)
I1209 14:13:53.926575  3198 sgd_solver.cpp:106] Iteration 2460, lr = 0.01
I1209 14:14:01.600589  3198 solver.cpp:237] Iteration 2480, loss = 5.8955
I1209 14:14:01.600776  3198 solver.cpp:253]     Train net output #0: loss = 5.8955 (* 1 = 5.8955 loss)
I1209 14:14:01.600785  3198 sgd_solver.cpp:106] Iteration 2480, lr = 0.01
I1209 14:14:09.267645  3198 solver.cpp:237] Iteration 2500, loss = 5.75552
I1209 14:14:09.267681  3198 solver.cpp:253]     Train net output #0: loss = 5.75552 (* 1 = 5.75552 loss)
I1209 14:14:09.267688  3198 sgd_solver.cpp:106] Iteration 2500, lr = 0.01
I1209 14:14:17.093029  3198 solver.cpp:237] Iteration 2520, loss = 5.70769
I1209 14:14:17.093066  3198 solver.cpp:253]     Train net output #0: loss = 5.70769 (* 1 = 5.70769 loss)
I1209 14:14:17.093072  3198 sgd_solver.cpp:106] Iteration 2520, lr = 0.01
I1209 14:14:25.643646  3198 solver.cpp:237] Iteration 2540, loss = 5.92156
I1209 14:14:25.643683  3198 solver.cpp:253]     Train net output #0: loss = 5.92156 (* 1 = 5.92156 loss)
I1209 14:14:25.643689  3198 sgd_solver.cpp:106] Iteration 2540, lr = 0.01
I1209 14:14:33.316190  3198 solver.cpp:237] Iteration 2560, loss = 5.99242
I1209 14:14:33.316342  3198 solver.cpp:253]     Train net output #0: loss = 5.99242 (* 1 = 5.99242 loss)
I1209 14:14:33.316350  3198 sgd_solver.cpp:106] Iteration 2560, lr = 0.01
I1209 14:14:40.873085  3198 solver.cpp:237] Iteration 2580, loss = 5.99834
I1209 14:14:40.873121  3198 solver.cpp:253]     Train net output #0: loss = 5.99834 (* 1 = 5.99834 loss)
I1209 14:14:40.873126  3198 sgd_solver.cpp:106] Iteration 2580, lr = 0.01
I1209 14:14:48.443248  3198 solver.cpp:237] Iteration 2600, loss = 5.92249
I1209 14:14:48.443284  3198 solver.cpp:253]     Train net output #0: loss = 5.92249 (* 1 = 5.92249 loss)
I1209 14:14:48.443289  3198 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I1209 14:14:56.001725  3198 solver.cpp:237] Iteration 2620, loss = 5.97888
I1209 14:14:56.001761  3198 solver.cpp:253]     Train net output #0: loss = 5.97888 (* 1 = 5.97888 loss)
I1209 14:14:56.001767  3198 sgd_solver.cpp:106] Iteration 2620, lr = 0.01
I1209 14:15:03.633297  3198 solver.cpp:237] Iteration 2640, loss = 5.91434
I1209 14:15:03.633455  3198 solver.cpp:253]     Train net output #0: loss = 5.91434 (* 1 = 5.91434 loss)
I1209 14:15:03.633463  3198 sgd_solver.cpp:106] Iteration 2640, lr = 0.01
I1209 14:15:11.232815  3198 solver.cpp:237] Iteration 2660, loss = 5.76417
I1209 14:15:11.232852  3198 solver.cpp:253]     Train net output #0: loss = 5.76417 (* 1 = 5.76417 loss)
I1209 14:15:11.232858  3198 sgd_solver.cpp:106] Iteration 2660, lr = 0.01
I1209 14:15:18.813760  3198 solver.cpp:237] Iteration 2680, loss = 6.07395
I1209 14:15:18.813796  3198 solver.cpp:253]     Train net output #0: loss = 6.07395 (* 1 = 6.07395 loss)
I1209 14:15:18.813802  3198 sgd_solver.cpp:106] Iteration 2680, lr = 0.01
I1209 14:15:26.488646  3198 solver.cpp:237] Iteration 2700, loss = 5.89022
I1209 14:15:26.488682  3198 solver.cpp:253]     Train net output #0: loss = 5.89022 (* 1 = 5.89022 loss)
I1209 14:15:26.488688  3198 sgd_solver.cpp:106] Iteration 2700, lr = 0.01
I1209 14:15:34.099097  3198 solver.cpp:237] Iteration 2720, loss = 5.91663
I1209 14:15:34.099264  3198 solver.cpp:253]     Train net output #0: loss = 5.91663 (* 1 = 5.91663 loss)
I1209 14:15:34.099272  3198 sgd_solver.cpp:106] Iteration 2720, lr = 0.01
I1209 14:15:41.762470  3198 solver.cpp:237] Iteration 2740, loss = 5.98008
I1209 14:15:41.762506  3198 solver.cpp:253]     Train net output #0: loss = 5.98008 (* 1 = 5.98008 loss)
I1209 14:15:41.762511  3198 sgd_solver.cpp:106] Iteration 2740, lr = 0.01
I1209 14:15:49.375674  3198 solver.cpp:237] Iteration 2760, loss = 5.72237
I1209 14:15:49.375710  3198 solver.cpp:253]     Train net output #0: loss = 5.72237 (* 1 = 5.72237 loss)
I1209 14:15:49.375716  3198 sgd_solver.cpp:106] Iteration 2760, lr = 0.01
I1209 14:15:56.971539  3198 solver.cpp:237] Iteration 2780, loss = 6.01056
I1209 14:15:56.971575  3198 solver.cpp:253]     Train net output #0: loss = 6.01056 (* 1 = 6.01056 loss)
I1209 14:15:56.971581  3198 sgd_solver.cpp:106] Iteration 2780, lr = 0.01
I1209 14:16:04.595309  3198 solver.cpp:237] Iteration 2800, loss = 5.84421
I1209 14:16:04.595459  3198 solver.cpp:253]     Train net output #0: loss = 5.84421 (* 1 = 5.84421 loss)
I1209 14:16:04.595468  3198 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I1209 14:16:12.268836  3198 solver.cpp:237] Iteration 2820, loss = 6.04342
I1209 14:16:12.268872  3198 solver.cpp:253]     Train net output #0: loss = 6.04342 (* 1 = 6.04342 loss)
I1209 14:16:12.268878  3198 sgd_solver.cpp:106] Iteration 2820, lr = 0.01
I1209 14:16:19.941900  3198 solver.cpp:237] Iteration 2840, loss = 5.83679
I1209 14:16:19.941937  3198 solver.cpp:253]     Train net output #0: loss = 5.83679 (* 1 = 5.83679 loss)
I1209 14:16:19.941943  3198 sgd_solver.cpp:106] Iteration 2840, lr = 0.01
I1209 14:16:27.548965  3198 solver.cpp:237] Iteration 2860, loss = 5.72838
I1209 14:16:27.549003  3198 solver.cpp:253]     Train net output #0: loss = 5.72838 (* 1 = 5.72838 loss)
I1209 14:16:27.549008  3198 sgd_solver.cpp:106] Iteration 2860, lr = 0.01
I1209 14:16:35.199990  3198 solver.cpp:237] Iteration 2880, loss = 6.16319
I1209 14:16:35.200145  3198 solver.cpp:253]     Train net output #0: loss = 6.16319 (* 1 = 6.16319 loss)
I1209 14:16:35.200155  3198 sgd_solver.cpp:106] Iteration 2880, lr = 0.01
I1209 14:16:42.841766  3198 solver.cpp:237] Iteration 2900, loss = 5.91027
I1209 14:16:42.841801  3198 solver.cpp:253]     Train net output #0: loss = 5.91027 (* 1 = 5.91027 loss)
I1209 14:16:42.841809  3198 sgd_solver.cpp:106] Iteration 2900, lr = 0.01
I1209 14:16:50.480366  3198 solver.cpp:237] Iteration 2920, loss = 5.80557
I1209 14:16:50.480402  3198 solver.cpp:253]     Train net output #0: loss = 5.80557 (* 1 = 5.80557 loss)
I1209 14:16:50.480408  3198 sgd_solver.cpp:106] Iteration 2920, lr = 0.01
I1209 14:16:58.125438  3198 solver.cpp:237] Iteration 2940, loss = 5.79318
I1209 14:16:58.125475  3198 solver.cpp:253]     Train net output #0: loss = 5.79318 (* 1 = 5.79318 loss)
I1209 14:16:58.125481  3198 sgd_solver.cpp:106] Iteration 2940, lr = 0.01
I1209 14:17:05.770318  3198 solver.cpp:237] Iteration 2960, loss = 5.75168
I1209 14:17:05.770474  3198 solver.cpp:253]     Train net output #0: loss = 5.75168 (* 1 = 5.75168 loss)
I1209 14:17:05.770483  3198 sgd_solver.cpp:106] Iteration 2960, lr = 0.01
I1209 14:17:13.440487  3198 solver.cpp:237] Iteration 2980, loss = 5.83425
I1209 14:17:13.440523  3198 solver.cpp:253]     Train net output #0: loss = 5.83425 (* 1 = 5.83425 loss)
I1209 14:17:13.440529  3198 sgd_solver.cpp:106] Iteration 2980, lr = 0.01
I1209 14:17:20.759196  3198 solver.cpp:341] Iteration 3000, Testing net (#0)
I1209 14:17:22.643219  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:18:37.669952  3198 solver.cpp:409]     Test net output #0: accuracy = 0.0708802
I1209 14:18:37.670065  3198 solver.cpp:409]     Test net output #1: loss = 5.45906 (* 1 = 5.45906 loss)
I1209 14:18:37.871510  3198 solver.cpp:237] Iteration 3000, loss = 5.88079
I1209 14:18:37.871546  3198 solver.cpp:253]     Train net output #0: loss = 5.88079 (* 1 = 5.88079 loss)
I1209 14:18:37.871553  3198 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I1209 14:18:44.895861  3198 solver.cpp:237] Iteration 3020, loss = 5.67637
I1209 14:18:44.895930  3198 solver.cpp:253]     Train net output #0: loss = 5.67637 (* 1 = 5.67637 loss)
I1209 14:18:44.895946  3198 sgd_solver.cpp:106] Iteration 3020, lr = 0.01
I1209 14:18:49.148591  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:18:52.570893  3198 solver.cpp:237] Iteration 3040, loss = 5.79918
I1209 14:18:52.570927  3198 solver.cpp:253]     Train net output #0: loss = 5.79918 (* 1 = 5.79918 loss)
I1209 14:18:52.570933  3198 sgd_solver.cpp:106] Iteration 3040, lr = 0.01
I1209 14:19:00.209952  3198 solver.cpp:237] Iteration 3060, loss = 5.77791
I1209 14:19:00.209988  3198 solver.cpp:253]     Train net output #0: loss = 5.77791 (* 1 = 5.77791 loss)
I1209 14:19:00.209995  3198 sgd_solver.cpp:106] Iteration 3060, lr = 0.01
I1209 14:19:07.859136  3198 solver.cpp:237] Iteration 3080, loss = 5.87531
I1209 14:19:07.859305  3198 solver.cpp:253]     Train net output #0: loss = 5.87531 (* 1 = 5.87531 loss)
I1209 14:19:07.859313  3198 sgd_solver.cpp:106] Iteration 3080, lr = 0.01
I1209 14:19:15.569746  3198 solver.cpp:237] Iteration 3100, loss = 5.66325
I1209 14:19:15.569795  3198 solver.cpp:253]     Train net output #0: loss = 5.66325 (* 1 = 5.66325 loss)
I1209 14:19:15.569802  3198 sgd_solver.cpp:106] Iteration 3100, lr = 0.01
I1209 14:19:23.497402  3198 solver.cpp:237] Iteration 3120, loss = 5.68157
I1209 14:19:23.497437  3198 solver.cpp:253]     Train net output #0: loss = 5.68157 (* 1 = 5.68157 loss)
I1209 14:19:23.497444  3198 sgd_solver.cpp:106] Iteration 3120, lr = 0.01
I1209 14:19:32.270776  3198 solver.cpp:237] Iteration 3140, loss = 5.83429
I1209 14:19:32.270813  3198 solver.cpp:253]     Train net output #0: loss = 5.83429 (* 1 = 5.83429 loss)
I1209 14:19:32.270819  3198 sgd_solver.cpp:106] Iteration 3140, lr = 0.01
I1209 14:19:39.925573  3198 solver.cpp:237] Iteration 3160, loss = 5.80554
I1209 14:19:39.925746  3198 solver.cpp:253]     Train net output #0: loss = 5.80554 (* 1 = 5.80554 loss)
I1209 14:19:39.925755  3198 sgd_solver.cpp:106] Iteration 3160, lr = 0.01
I1209 14:19:47.622555  3198 solver.cpp:237] Iteration 3180, loss = 5.64225
I1209 14:19:47.622591  3198 solver.cpp:253]     Train net output #0: loss = 5.64225 (* 1 = 5.64225 loss)
I1209 14:19:47.622597  3198 sgd_solver.cpp:106] Iteration 3180, lr = 0.01
I1209 14:19:55.236520  3198 solver.cpp:237] Iteration 3200, loss = 5.78691
I1209 14:19:55.236557  3198 solver.cpp:253]     Train net output #0: loss = 5.78691 (* 1 = 5.78691 loss)
I1209 14:19:55.236562  3198 sgd_solver.cpp:106] Iteration 3200, lr = 0.01
I1209 14:20:02.906544  3198 solver.cpp:237] Iteration 3220, loss = 5.91365
I1209 14:20:02.906591  3198 solver.cpp:253]     Train net output #0: loss = 5.91365 (* 1 = 5.91365 loss)
I1209 14:20:02.906597  3198 sgd_solver.cpp:106] Iteration 3220, lr = 0.01
I1209 14:20:10.543541  3198 solver.cpp:237] Iteration 3240, loss = 5.9179
I1209 14:20:10.543658  3198 solver.cpp:253]     Train net output #0: loss = 5.9179 (* 1 = 5.9179 loss)
I1209 14:20:10.543664  3198 sgd_solver.cpp:106] Iteration 3240, lr = 0.01
I1209 14:20:18.233963  3198 solver.cpp:237] Iteration 3260, loss = 5.69788
I1209 14:20:18.233999  3198 solver.cpp:253]     Train net output #0: loss = 5.69788 (* 1 = 5.69788 loss)
I1209 14:20:18.234005  3198 sgd_solver.cpp:106] Iteration 3260, lr = 0.01
I1209 14:20:25.881973  3198 solver.cpp:237] Iteration 3280, loss = 5.87207
I1209 14:20:25.882006  3198 solver.cpp:253]     Train net output #0: loss = 5.87207 (* 1 = 5.87207 loss)
I1209 14:20:25.882012  3198 sgd_solver.cpp:106] Iteration 3280, lr = 0.01
I1209 14:20:33.510212  3198 solver.cpp:237] Iteration 3300, loss = 5.89279
I1209 14:20:33.510248  3198 solver.cpp:253]     Train net output #0: loss = 5.89279 (* 1 = 5.89279 loss)
I1209 14:20:33.510256  3198 sgd_solver.cpp:106] Iteration 3300, lr = 0.01
I1209 14:20:41.166082  3198 solver.cpp:237] Iteration 3320, loss = 5.80147
I1209 14:20:41.166229  3198 solver.cpp:253]     Train net output #0: loss = 5.80147 (* 1 = 5.80147 loss)
I1209 14:20:41.166236  3198 sgd_solver.cpp:106] Iteration 3320, lr = 0.01
I1209 14:20:48.804869  3198 solver.cpp:237] Iteration 3340, loss = 5.72992
I1209 14:20:48.804908  3198 solver.cpp:253]     Train net output #0: loss = 5.72992 (* 1 = 5.72992 loss)
I1209 14:20:48.804914  3198 sgd_solver.cpp:106] Iteration 3340, lr = 0.01
I1209 14:20:56.511209  3198 solver.cpp:237] Iteration 3360, loss = 5.91742
I1209 14:20:56.511246  3198 solver.cpp:253]     Train net output #0: loss = 5.91742 (* 1 = 5.91742 loss)
I1209 14:20:56.511253  3198 sgd_solver.cpp:106] Iteration 3360, lr = 0.01
I1209 14:21:04.142647  3198 solver.cpp:237] Iteration 3380, loss = 5.95983
I1209 14:21:04.142683  3198 solver.cpp:253]     Train net output #0: loss = 5.95983 (* 1 = 5.95983 loss)
I1209 14:21:04.142689  3198 sgd_solver.cpp:106] Iteration 3380, lr = 0.01
I1209 14:21:11.774149  3198 solver.cpp:237] Iteration 3400, loss = 5.6713
I1209 14:21:11.774322  3198 solver.cpp:253]     Train net output #0: loss = 5.6713 (* 1 = 5.6713 loss)
I1209 14:21:11.774329  3198 sgd_solver.cpp:106] Iteration 3400, lr = 0.01
I1209 14:21:19.420945  3198 solver.cpp:237] Iteration 3420, loss = 5.55462
I1209 14:21:19.420981  3198 solver.cpp:253]     Train net output #0: loss = 5.55462 (* 1 = 5.55462 loss)
I1209 14:21:19.420989  3198 sgd_solver.cpp:106] Iteration 3420, lr = 0.01
I1209 14:21:27.104847  3198 solver.cpp:237] Iteration 3440, loss = 5.85939
I1209 14:21:27.104884  3198 solver.cpp:253]     Train net output #0: loss = 5.85939 (* 1 = 5.85939 loss)
I1209 14:21:27.104889  3198 sgd_solver.cpp:106] Iteration 3440, lr = 0.01
I1209 14:21:34.758934  3198 solver.cpp:237] Iteration 3460, loss = 5.61928
I1209 14:21:34.758970  3198 solver.cpp:253]     Train net output #0: loss = 5.61928 (* 1 = 5.61928 loss)
I1209 14:21:34.758975  3198 sgd_solver.cpp:106] Iteration 3460, lr = 0.01
I1209 14:21:42.434732  3198 solver.cpp:237] Iteration 3480, loss = 5.85411
I1209 14:21:42.434903  3198 solver.cpp:253]     Train net output #0: loss = 5.85411 (* 1 = 5.85411 loss)
I1209 14:21:42.434911  3198 sgd_solver.cpp:106] Iteration 3480, lr = 0.01
I1209 14:21:50.099237  3198 solver.cpp:237] Iteration 3500, loss = 5.948
I1209 14:21:50.099275  3198 solver.cpp:253]     Train net output #0: loss = 5.948 (* 1 = 5.948 loss)
I1209 14:21:50.099282  3198 sgd_solver.cpp:106] Iteration 3500, lr = 0.01
I1209 14:21:57.738149  3198 solver.cpp:237] Iteration 3520, loss = 5.6172
I1209 14:21:57.738185  3198 solver.cpp:253]     Train net output #0: loss = 5.6172 (* 1 = 5.6172 loss)
I1209 14:21:57.738191  3198 sgd_solver.cpp:106] Iteration 3520, lr = 0.01
I1209 14:22:05.422178  3198 solver.cpp:237] Iteration 3540, loss = 5.68124
I1209 14:22:05.422214  3198 solver.cpp:253]     Train net output #0: loss = 5.68124 (* 1 = 5.68124 loss)
I1209 14:22:05.422221  3198 sgd_solver.cpp:106] Iteration 3540, lr = 0.01
I1209 14:22:13.059918  3198 solver.cpp:237] Iteration 3560, loss = 5.88634
I1209 14:22:13.060142  3198 solver.cpp:253]     Train net output #0: loss = 5.88634 (* 1 = 5.88634 loss)
I1209 14:22:13.060163  3198 sgd_solver.cpp:106] Iteration 3560, lr = 0.01
I1209 14:22:20.726003  3198 solver.cpp:237] Iteration 3580, loss = 5.70246
I1209 14:22:20.726037  3198 solver.cpp:253]     Train net output #0: loss = 5.70246 (* 1 = 5.70246 loss)
I1209 14:22:20.726043  3198 sgd_solver.cpp:106] Iteration 3580, lr = 0.01
I1209 14:22:28.388665  3198 solver.cpp:237] Iteration 3600, loss = 5.47613
I1209 14:22:28.388700  3198 solver.cpp:253]     Train net output #0: loss = 5.47613 (* 1 = 5.47613 loss)
I1209 14:22:28.388706  3198 sgd_solver.cpp:106] Iteration 3600, lr = 0.01
I1209 14:22:36.093106  3198 solver.cpp:237] Iteration 3620, loss = 5.61604
I1209 14:22:36.093144  3198 solver.cpp:253]     Train net output #0: loss = 5.61604 (* 1 = 5.61604 loss)
I1209 14:22:36.093150  3198 sgd_solver.cpp:106] Iteration 3620, lr = 0.01
I1209 14:22:43.741598  3198 solver.cpp:237] Iteration 3640, loss = 5.77071
I1209 14:22:43.741704  3198 solver.cpp:253]     Train net output #0: loss = 5.77071 (* 1 = 5.77071 loss)
I1209 14:22:43.741711  3198 sgd_solver.cpp:106] Iteration 3640, lr = 0.01
I1209 14:22:51.372890  3198 solver.cpp:237] Iteration 3660, loss = 5.73449
I1209 14:22:51.372926  3198 solver.cpp:253]     Train net output #0: loss = 5.73449 (* 1 = 5.73449 loss)
I1209 14:22:51.372933  3198 sgd_solver.cpp:106] Iteration 3660, lr = 0.01
I1209 14:22:59.021069  3198 solver.cpp:237] Iteration 3680, loss = 5.6402
I1209 14:22:59.021126  3198 solver.cpp:253]     Train net output #0: loss = 5.6402 (* 1 = 5.6402 loss)
I1209 14:22:59.021138  3198 sgd_solver.cpp:106] Iteration 3680, lr = 0.01
I1209 14:23:06.685322  3198 solver.cpp:237] Iteration 3700, loss = 5.55807
I1209 14:23:06.685358  3198 solver.cpp:253]     Train net output #0: loss = 5.55807 (* 1 = 5.55807 loss)
I1209 14:23:06.685364  3198 sgd_solver.cpp:106] Iteration 3700, lr = 0.01
I1209 14:23:14.345463  3198 solver.cpp:237] Iteration 3720, loss = 5.35694
I1209 14:23:14.345630  3198 solver.cpp:253]     Train net output #0: loss = 5.35694 (* 1 = 5.35694 loss)
I1209 14:23:14.345639  3198 sgd_solver.cpp:106] Iteration 3720, lr = 0.01
I1209 14:23:22.030102  3198 solver.cpp:237] Iteration 3740, loss = 5.81736
I1209 14:23:22.030139  3198 solver.cpp:253]     Train net output #0: loss = 5.81736 (* 1 = 5.81736 loss)
I1209 14:23:22.030145  3198 sgd_solver.cpp:106] Iteration 3740, lr = 0.01
I1209 14:23:29.678056  3198 solver.cpp:237] Iteration 3760, loss = 5.58159
I1209 14:23:29.678092  3198 solver.cpp:253]     Train net output #0: loss = 5.58159 (* 1 = 5.58159 loss)
I1209 14:23:29.678098  3198 sgd_solver.cpp:106] Iteration 3760, lr = 0.01
I1209 14:23:37.320361  3198 solver.cpp:237] Iteration 3780, loss = 5.66434
I1209 14:23:37.320397  3198 solver.cpp:253]     Train net output #0: loss = 5.66434 (* 1 = 5.66434 loss)
I1209 14:23:37.320405  3198 sgd_solver.cpp:106] Iteration 3780, lr = 0.01
I1209 14:23:44.949455  3198 solver.cpp:237] Iteration 3800, loss = 5.42759
I1209 14:23:44.950235  3198 solver.cpp:253]     Train net output #0: loss = 5.42759 (* 1 = 5.42759 loss)
I1209 14:23:44.950243  3198 sgd_solver.cpp:106] Iteration 3800, lr = 0.01
I1209 14:23:52.599010  3198 solver.cpp:237] Iteration 3820, loss = 5.47779
I1209 14:23:52.599047  3198 solver.cpp:253]     Train net output #0: loss = 5.47779 (* 1 = 5.47779 loss)
I1209 14:23:52.599053  3198 sgd_solver.cpp:106] Iteration 3820, lr = 0.01
I1209 14:24:00.287825  3198 solver.cpp:237] Iteration 3840, loss = 5.5973
I1209 14:24:00.287864  3198 solver.cpp:253]     Train net output #0: loss = 5.5973 (* 1 = 5.5973 loss)
I1209 14:24:00.287870  3198 sgd_solver.cpp:106] Iteration 3840, lr = 0.01
I1209 14:24:07.934311  3198 solver.cpp:237] Iteration 3860, loss = 5.69295
I1209 14:24:07.934350  3198 solver.cpp:253]     Train net output #0: loss = 5.69295 (* 1 = 5.69295 loss)
I1209 14:24:07.934366  3198 sgd_solver.cpp:106] Iteration 3860, lr = 0.01
I1209 14:24:15.621577  3198 solver.cpp:237] Iteration 3880, loss = 5.71354
I1209 14:24:15.621747  3198 solver.cpp:253]     Train net output #0: loss = 5.71354 (* 1 = 5.71354 loss)
I1209 14:24:15.621754  3198 sgd_solver.cpp:106] Iteration 3880, lr = 0.01
I1209 14:24:23.263880  3198 solver.cpp:237] Iteration 3900, loss = 5.51674
I1209 14:24:23.263919  3198 solver.cpp:253]     Train net output #0: loss = 5.51674 (* 1 = 5.51674 loss)
I1209 14:24:23.263926  3198 sgd_solver.cpp:106] Iteration 3900, lr = 0.01
I1209 14:24:32.202872  3198 solver.cpp:237] Iteration 3920, loss = 5.79669
I1209 14:24:32.202908  3198 solver.cpp:253]     Train net output #0: loss = 5.79669 (* 1 = 5.79669 loss)
I1209 14:24:32.202913  3198 sgd_solver.cpp:106] Iteration 3920, lr = 0.01
I1209 14:24:39.777611  3198 solver.cpp:237] Iteration 3940, loss = 5.68049
I1209 14:24:39.777649  3198 solver.cpp:253]     Train net output #0: loss = 5.68049 (* 1 = 5.68049 loss)
I1209 14:24:39.777657  3198 sgd_solver.cpp:106] Iteration 3940, lr = 0.01
I1209 14:24:47.434783  3198 solver.cpp:237] Iteration 3960, loss = 5.75029
I1209 14:24:47.434942  3198 solver.cpp:253]     Train net output #0: loss = 5.75029 (* 1 = 5.75029 loss)
I1209 14:24:47.434949  3198 sgd_solver.cpp:106] Iteration 3960, lr = 0.01
I1209 14:24:55.040874  3198 solver.cpp:237] Iteration 3980, loss = 5.53459
I1209 14:24:55.040912  3198 solver.cpp:253]     Train net output #0: loss = 5.53459 (* 1 = 5.53459 loss)
I1209 14:24:55.040920  3198 sgd_solver.cpp:106] Iteration 3980, lr = 0.01
I1209 14:25:02.323619  3198 solver.cpp:341] Iteration 4000, Testing net (#0)
I1209 14:25:04.840320  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:26:19.205863  3198 solver.cpp:409]     Test net output #0: accuracy = 0.0843201
I1209 14:26:19.206008  3198 solver.cpp:409]     Test net output #1: loss = 5.23821 (* 1 = 5.23821 loss)
I1209 14:26:19.406913  3198 solver.cpp:237] Iteration 4000, loss = 5.36554
I1209 14:26:19.406950  3198 solver.cpp:253]     Train net output #0: loss = 5.36554 (* 1 = 5.36554 loss)
I1209 14:26:19.406956  3198 sgd_solver.cpp:106] Iteration 4000, lr = 0.01
I1209 14:26:26.396029  3198 solver.cpp:237] Iteration 4020, loss = 5.53918
I1209 14:26:26.396067  3198 solver.cpp:253]     Train net output #0: loss = 5.53918 (* 1 = 5.53918 loss)
I1209 14:26:26.396073  3198 sgd_solver.cpp:106] Iteration 4020, lr = 0.01
I1209 14:26:33.687686  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:26:34.033351  3198 solver.cpp:237] Iteration 4040, loss = 5.39429
I1209 14:26:34.033388  3198 solver.cpp:253]     Train net output #0: loss = 5.39429 (* 1 = 5.39429 loss)
I1209 14:26:34.033396  3198 sgd_solver.cpp:106] Iteration 4040, lr = 0.01
I1209 14:26:41.700242  3198 solver.cpp:237] Iteration 4060, loss = 5.47137
I1209 14:26:41.700279  3198 solver.cpp:253]     Train net output #0: loss = 5.47137 (* 1 = 5.47137 loss)
I1209 14:26:41.700285  3198 sgd_solver.cpp:106] Iteration 4060, lr = 0.01
I1209 14:26:49.321681  3198 solver.cpp:237] Iteration 4080, loss = 5.5194
I1209 14:26:49.321854  3198 solver.cpp:253]     Train net output #0: loss = 5.5194 (* 1 = 5.5194 loss)
I1209 14:26:49.321862  3198 sgd_solver.cpp:106] Iteration 4080, lr = 0.01
I1209 14:26:56.976609  3198 solver.cpp:237] Iteration 4100, loss = 5.34942
I1209 14:26:56.976645  3198 solver.cpp:253]     Train net output #0: loss = 5.34942 (* 1 = 5.34942 loss)
I1209 14:26:56.976652  3198 sgd_solver.cpp:106] Iteration 4100, lr = 0.01
I1209 14:27:04.649549  3198 solver.cpp:237] Iteration 4120, loss = 5.50116
I1209 14:27:04.649585  3198 solver.cpp:253]     Train net output #0: loss = 5.50116 (* 1 = 5.50116 loss)
I1209 14:27:04.649591  3198 sgd_solver.cpp:106] Iteration 4120, lr = 0.01
I1209 14:27:12.309312  3198 solver.cpp:237] Iteration 4140, loss = 5.54879
I1209 14:27:12.309350  3198 solver.cpp:253]     Train net output #0: loss = 5.54879 (* 1 = 5.54879 loss)
I1209 14:27:12.309356  3198 sgd_solver.cpp:106] Iteration 4140, lr = 0.01
I1209 14:27:19.961369  3198 solver.cpp:237] Iteration 4160, loss = 5.4131
I1209 14:27:19.961459  3198 solver.cpp:253]     Train net output #0: loss = 5.4131 (* 1 = 5.4131 loss)
I1209 14:27:19.961467  3198 sgd_solver.cpp:106] Iteration 4160, lr = 0.01
I1209 14:27:27.647641  3198 solver.cpp:237] Iteration 4180, loss = 5.64503
I1209 14:27:27.647677  3198 solver.cpp:253]     Train net output #0: loss = 5.64503 (* 1 = 5.64503 loss)
I1209 14:27:27.647683  3198 sgd_solver.cpp:106] Iteration 4180, lr = 0.01
I1209 14:27:35.255573  3198 solver.cpp:237] Iteration 4200, loss = 5.65862
I1209 14:27:35.255611  3198 solver.cpp:253]     Train net output #0: loss = 5.65862 (* 1 = 5.65862 loss)
I1209 14:27:35.255617  3198 sgd_solver.cpp:106] Iteration 4200, lr = 0.01
I1209 14:27:42.847079  3198 solver.cpp:237] Iteration 4220, loss = 5.44882
I1209 14:27:42.847112  3198 solver.cpp:253]     Train net output #0: loss = 5.44882 (* 1 = 5.44882 loss)
I1209 14:27:42.847118  3198 sgd_solver.cpp:106] Iteration 4220, lr = 0.01
I1209 14:27:50.515686  3198 solver.cpp:237] Iteration 4240, loss = 5.30049
I1209 14:27:50.515869  3198 solver.cpp:253]     Train net output #0: loss = 5.30049 (* 1 = 5.30049 loss)
I1209 14:27:50.515877  3198 sgd_solver.cpp:106] Iteration 4240, lr = 0.01
I1209 14:27:58.205049  3198 solver.cpp:237] Iteration 4260, loss = 5.54242
I1209 14:27:58.205085  3198 solver.cpp:253]     Train net output #0: loss = 5.54242 (* 1 = 5.54242 loss)
I1209 14:27:58.205090  3198 sgd_solver.cpp:106] Iteration 4260, lr = 0.01
I1209 14:28:05.860234  3198 solver.cpp:237] Iteration 4280, loss = 5.1841
I1209 14:28:05.860270  3198 solver.cpp:253]     Train net output #0: loss = 5.1841 (* 1 = 5.1841 loss)
I1209 14:28:05.860276  3198 sgd_solver.cpp:106] Iteration 4280, lr = 0.01
I1209 14:28:13.515586  3198 solver.cpp:237] Iteration 4300, loss = 5.61602
I1209 14:28:13.515625  3198 solver.cpp:253]     Train net output #0: loss = 5.61602 (* 1 = 5.61602 loss)
I1209 14:28:13.515631  3198 sgd_solver.cpp:106] Iteration 4300, lr = 0.01
I1209 14:28:21.173281  3198 solver.cpp:237] Iteration 4320, loss = 5.61565
I1209 14:28:21.173444  3198 solver.cpp:253]     Train net output #0: loss = 5.61565 (* 1 = 5.61565 loss)
I1209 14:28:21.173454  3198 sgd_solver.cpp:106] Iteration 4320, lr = 0.01
I1209 14:28:28.701525  3198 solver.cpp:237] Iteration 4340, loss = 5.32732
I1209 14:28:28.701561  3198 solver.cpp:253]     Train net output #0: loss = 5.32732 (* 1 = 5.32732 loss)
I1209 14:28:28.701567  3198 sgd_solver.cpp:106] Iteration 4340, lr = 0.01
I1209 14:28:36.316985  3198 solver.cpp:237] Iteration 4360, loss = 5.48599
I1209 14:28:36.317021  3198 solver.cpp:253]     Train net output #0: loss = 5.48599 (* 1 = 5.48599 loss)
I1209 14:28:36.317028  3198 sgd_solver.cpp:106] Iteration 4360, lr = 0.01
I1209 14:28:43.903775  3198 solver.cpp:237] Iteration 4380, loss = 5.27401
I1209 14:28:43.903813  3198 solver.cpp:253]     Train net output #0: loss = 5.27401 (* 1 = 5.27401 loss)
I1209 14:28:43.903820  3198 sgd_solver.cpp:106] Iteration 4380, lr = 0.01
I1209 14:28:51.587883  3198 solver.cpp:237] Iteration 4400, loss = 5.14575
I1209 14:28:51.587990  3198 solver.cpp:253]     Train net output #0: loss = 5.14575 (* 1 = 5.14575 loss)
I1209 14:28:51.588007  3198 sgd_solver.cpp:106] Iteration 4400, lr = 0.01
I1209 14:28:59.234381  3198 solver.cpp:237] Iteration 4420, loss = 5.40141
I1209 14:28:59.234418  3198 solver.cpp:253]     Train net output #0: loss = 5.40141 (* 1 = 5.40141 loss)
I1209 14:28:59.234424  3198 sgd_solver.cpp:106] Iteration 4420, lr = 0.01
I1209 14:29:06.878015  3198 solver.cpp:237] Iteration 4440, loss = 5.46756
I1209 14:29:06.878052  3198 solver.cpp:253]     Train net output #0: loss = 5.46756 (* 1 = 5.46756 loss)
I1209 14:29:06.878059  3198 sgd_solver.cpp:106] Iteration 4440, lr = 0.01
I1209 14:29:14.504575  3198 solver.cpp:237] Iteration 4460, loss = 5.69083
I1209 14:29:14.504612  3198 solver.cpp:253]     Train net output #0: loss = 5.69083 (* 1 = 5.69083 loss)
I1209 14:29:14.504618  3198 sgd_solver.cpp:106] Iteration 4460, lr = 0.01
I1209 14:29:22.187034  3198 solver.cpp:237] Iteration 4480, loss = 5.50853
I1209 14:29:22.187206  3198 solver.cpp:253]     Train net output #0: loss = 5.50853 (* 1 = 5.50853 loss)
I1209 14:29:22.187212  3198 sgd_solver.cpp:106] Iteration 4480, lr = 0.01
I1209 14:29:29.930510  3198 solver.cpp:237] Iteration 4500, loss = 5.42424
I1209 14:29:29.930537  3198 solver.cpp:253]     Train net output #0: loss = 5.42424 (* 1 = 5.42424 loss)
I1209 14:29:29.930543  3198 sgd_solver.cpp:106] Iteration 4500, lr = 0.01
I1209 14:29:38.704774  3198 solver.cpp:237] Iteration 4520, loss = 5.46236
I1209 14:29:38.704810  3198 solver.cpp:253]     Train net output #0: loss = 5.46236 (* 1 = 5.46236 loss)
I1209 14:29:38.704816  3198 sgd_solver.cpp:106] Iteration 4520, lr = 0.01
I1209 14:29:46.303647  3198 solver.cpp:237] Iteration 4540, loss = 5.22687
I1209 14:29:46.303681  3198 solver.cpp:253]     Train net output #0: loss = 5.22687 (* 1 = 5.22687 loss)
I1209 14:29:46.303688  3198 sgd_solver.cpp:106] Iteration 4540, lr = 0.01
I1209 14:29:53.899924  3198 solver.cpp:237] Iteration 4560, loss = 5.53674
I1209 14:29:53.900075  3198 solver.cpp:253]     Train net output #0: loss = 5.53674 (* 1 = 5.53674 loss)
I1209 14:29:53.900084  3198 sgd_solver.cpp:106] Iteration 4560, lr = 0.01
I1209 14:30:01.492133  3198 solver.cpp:237] Iteration 4580, loss = 5.32615
I1209 14:30:01.492169  3198 solver.cpp:253]     Train net output #0: loss = 5.32615 (* 1 = 5.32615 loss)
I1209 14:30:01.492175  3198 sgd_solver.cpp:106] Iteration 4580, lr = 0.01
I1209 14:30:09.127115  3198 solver.cpp:237] Iteration 4600, loss = 5.42169
I1209 14:30:09.127151  3198 solver.cpp:253]     Train net output #0: loss = 5.42169 (* 1 = 5.42169 loss)
I1209 14:30:09.127156  3198 sgd_solver.cpp:106] Iteration 4600, lr = 0.01
I1209 14:30:16.750396  3198 solver.cpp:237] Iteration 4620, loss = 5.47515
I1209 14:30:16.750432  3198 solver.cpp:253]     Train net output #0: loss = 5.47515 (* 1 = 5.47515 loss)
I1209 14:30:16.750438  3198 sgd_solver.cpp:106] Iteration 4620, lr = 0.01
I1209 14:30:24.310474  3198 solver.cpp:237] Iteration 4640, loss = 5.30744
I1209 14:30:24.310614  3198 solver.cpp:253]     Train net output #0: loss = 5.30744 (* 1 = 5.30744 loss)
I1209 14:30:24.310621  3198 sgd_solver.cpp:106] Iteration 4640, lr = 0.01
I1209 14:30:31.915084  3198 solver.cpp:237] Iteration 4660, loss = 5.40511
I1209 14:30:31.915122  3198 solver.cpp:253]     Train net output #0: loss = 5.40511 (* 1 = 5.40511 loss)
I1209 14:30:31.915127  3198 sgd_solver.cpp:106] Iteration 4660, lr = 0.01
I1209 14:30:39.544392  3198 solver.cpp:237] Iteration 4680, loss = 5.40351
I1209 14:30:39.544427  3198 solver.cpp:253]     Train net output #0: loss = 5.40351 (* 1 = 5.40351 loss)
I1209 14:30:39.544433  3198 sgd_solver.cpp:106] Iteration 4680, lr = 0.01
I1209 14:30:47.158929  3198 solver.cpp:237] Iteration 4700, loss = 5.49277
I1209 14:30:47.158967  3198 solver.cpp:253]     Train net output #0: loss = 5.49277 (* 1 = 5.49277 loss)
I1209 14:30:47.158972  3198 sgd_solver.cpp:106] Iteration 4700, lr = 0.01
I1209 14:30:54.805052  3198 solver.cpp:237] Iteration 4720, loss = 5.11469
I1209 14:30:54.805146  3198 solver.cpp:253]     Train net output #0: loss = 5.11469 (* 1 = 5.11469 loss)
I1209 14:30:54.805162  3198 sgd_solver.cpp:106] Iteration 4720, lr = 0.01
I1209 14:31:02.441189  3198 solver.cpp:237] Iteration 4740, loss = 5.29186
I1209 14:31:02.441226  3198 solver.cpp:253]     Train net output #0: loss = 5.29186 (* 1 = 5.29186 loss)
I1209 14:31:02.441231  3198 sgd_solver.cpp:106] Iteration 4740, lr = 0.01
I1209 14:31:10.137847  3198 solver.cpp:237] Iteration 4760, loss = 5.10564
I1209 14:31:10.137882  3198 solver.cpp:253]     Train net output #0: loss = 5.10564 (* 1 = 5.10564 loss)
I1209 14:31:10.137888  3198 sgd_solver.cpp:106] Iteration 4760, lr = 0.01
I1209 14:31:17.751022  3198 solver.cpp:237] Iteration 4780, loss = 5.08046
I1209 14:31:17.751047  3198 solver.cpp:253]     Train net output #0: loss = 5.08046 (* 1 = 5.08046 loss)
I1209 14:31:17.751054  3198 sgd_solver.cpp:106] Iteration 4780, lr = 0.01
I1209 14:31:25.389158  3198 solver.cpp:237] Iteration 4800, loss = 5.22519
I1209 14:31:25.389288  3198 solver.cpp:253]     Train net output #0: loss = 5.22519 (* 1 = 5.22519 loss)
I1209 14:31:25.389304  3198 sgd_solver.cpp:106] Iteration 4800, lr = 0.01
I1209 14:31:33.025871  3198 solver.cpp:237] Iteration 4820, loss = 5.24037
I1209 14:31:33.025908  3198 solver.cpp:253]     Train net output #0: loss = 5.24037 (* 1 = 5.24037 loss)
I1209 14:31:33.025914  3198 sgd_solver.cpp:106] Iteration 4820, lr = 0.01
I1209 14:31:40.653967  3198 solver.cpp:237] Iteration 4840, loss = 5.48959
I1209 14:31:40.654005  3198 solver.cpp:253]     Train net output #0: loss = 5.48959 (* 1 = 5.48959 loss)
I1209 14:31:40.654011  3198 sgd_solver.cpp:106] Iteration 4840, lr = 0.01
I1209 14:31:48.336603  3198 solver.cpp:237] Iteration 4860, loss = 5.11712
I1209 14:31:48.336649  3198 solver.cpp:253]     Train net output #0: loss = 5.11712 (* 1 = 5.11712 loss)
I1209 14:31:48.336665  3198 sgd_solver.cpp:106] Iteration 4860, lr = 0.01
I1209 14:31:56.014448  3198 solver.cpp:237] Iteration 4880, loss = 5.36071
I1209 14:31:56.014605  3198 solver.cpp:253]     Train net output #0: loss = 5.36071 (* 1 = 5.36071 loss)
I1209 14:31:56.014611  3198 sgd_solver.cpp:106] Iteration 4880, lr = 0.01
I1209 14:32:03.657428  3198 solver.cpp:237] Iteration 4900, loss = 5.20477
I1209 14:32:03.657464  3198 solver.cpp:253]     Train net output #0: loss = 5.20477 (* 1 = 5.20477 loss)
I1209 14:32:03.657470  3198 sgd_solver.cpp:106] Iteration 4900, lr = 0.01
I1209 14:32:11.331495  3198 solver.cpp:237] Iteration 4920, loss = 5.11372
I1209 14:32:11.331532  3198 solver.cpp:253]     Train net output #0: loss = 5.11372 (* 1 = 5.11372 loss)
I1209 14:32:11.331538  3198 sgd_solver.cpp:106] Iteration 4920, lr = 0.01
I1209 14:32:19.002329  3198 solver.cpp:237] Iteration 4940, loss = 5.22983
I1209 14:32:19.002364  3198 solver.cpp:253]     Train net output #0: loss = 5.22983 (* 1 = 5.22983 loss)
I1209 14:32:19.002370  3198 sgd_solver.cpp:106] Iteration 4940, lr = 0.01
I1209 14:32:26.628372  3198 solver.cpp:237] Iteration 4960, loss = 5.25453
I1209 14:32:26.628514  3198 solver.cpp:253]     Train net output #0: loss = 5.25453 (* 1 = 5.25453 loss)
I1209 14:32:26.628521  3198 sgd_solver.cpp:106] Iteration 4960, lr = 0.01
I1209 14:32:34.301904  3198 solver.cpp:237] Iteration 4980, loss = 5.46578
I1209 14:32:34.301941  3198 solver.cpp:253]     Train net output #0: loss = 5.46578 (* 1 = 5.46578 loss)
I1209 14:32:34.301947  3198 sgd_solver.cpp:106] Iteration 4980, lr = 0.01
I1209 14:32:41.588194  3198 solver.cpp:341] Iteration 5000, Testing net (#0)
I1209 14:32:44.703148  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:33:58.420096  3198 solver.cpp:409]     Test net output #0: accuracy = 0.11156
I1209 14:33:58.420233  3198 solver.cpp:409]     Test net output #1: loss = 4.83065 (* 1 = 4.83065 loss)
I1209 14:33:58.622098  3198 solver.cpp:237] Iteration 5000, loss = 5.32629
I1209 14:33:58.622135  3198 solver.cpp:253]     Train net output #0: loss = 5.32629 (* 1 = 5.32629 loss)
I1209 14:33:58.622143  3198 sgd_solver.cpp:106] Iteration 5000, lr = 0.01
I1209 14:34:05.570260  3198 solver.cpp:237] Iteration 5020, loss = 5.48929
I1209 14:34:05.570296  3198 solver.cpp:253]     Train net output #0: loss = 5.48929 (* 1 = 5.48929 loss)
I1209 14:34:05.570302  3198 sgd_solver.cpp:106] Iteration 5020, lr = 0.01
I1209 14:34:13.183320  3198 solver.cpp:237] Iteration 5040, loss = 5.27739
I1209 14:34:13.183364  3198 solver.cpp:253]     Train net output #0: loss = 5.27739 (* 1 = 5.27739 loss)
I1209 14:34:13.183370  3198 sgd_solver.cpp:106] Iteration 5040, lr = 0.01
I1209 14:34:15.864943  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:34:20.817241  3198 solver.cpp:237] Iteration 5060, loss = 5.34574
I1209 14:34:20.817284  3198 solver.cpp:253]     Train net output #0: loss = 5.34574 (* 1 = 5.34574 loss)
I1209 14:34:20.817291  3198 sgd_solver.cpp:106] Iteration 5060, lr = 0.01
I1209 14:34:28.491647  3198 solver.cpp:237] Iteration 5080, loss = 5.37892
I1209 14:34:28.491801  3198 solver.cpp:253]     Train net output #0: loss = 5.37892 (* 1 = 5.37892 loss)
I1209 14:34:28.491807  3198 sgd_solver.cpp:106] Iteration 5080, lr = 0.01
I1209 14:34:36.174401  3198 solver.cpp:237] Iteration 5100, loss = 5.07209
I1209 14:34:36.174437  3198 solver.cpp:253]     Train net output #0: loss = 5.07209 (* 1 = 5.07209 loss)
I1209 14:34:36.174443  3198 sgd_solver.cpp:106] Iteration 5100, lr = 0.01
I1209 14:34:43.901465  3198 solver.cpp:237] Iteration 5120, loss = 5.00279
I1209 14:34:43.901502  3198 solver.cpp:253]     Train net output #0: loss = 5.00279 (* 1 = 5.00279 loss)
I1209 14:34:43.901509  3198 sgd_solver.cpp:106] Iteration 5120, lr = 0.01
I1209 14:34:52.420330  3198 solver.cpp:237] Iteration 5140, loss = 5.19683
I1209 14:34:52.420367  3198 solver.cpp:253]     Train net output #0: loss = 5.19683 (* 1 = 5.19683 loss)
I1209 14:34:52.420372  3198 sgd_solver.cpp:106] Iteration 5140, lr = 0.01
I1209 14:35:00.174875  3198 solver.cpp:237] Iteration 5160, loss = 4.96421
I1209 14:35:00.175007  3198 solver.cpp:253]     Train net output #0: loss = 4.96421 (* 1 = 4.96421 loss)
I1209 14:35:00.175024  3198 sgd_solver.cpp:106] Iteration 5160, lr = 0.01
I1209 14:35:07.825727  3198 solver.cpp:237] Iteration 5180, loss = 4.95694
I1209 14:35:07.825765  3198 solver.cpp:253]     Train net output #0: loss = 4.95694 (* 1 = 4.95694 loss)
I1209 14:35:07.825772  3198 sgd_solver.cpp:106] Iteration 5180, lr = 0.01
I1209 14:35:15.509047  3198 solver.cpp:237] Iteration 5200, loss = 5.28458
I1209 14:35:15.509081  3198 solver.cpp:253]     Train net output #0: loss = 5.28458 (* 1 = 5.28458 loss)
I1209 14:35:15.509088  3198 sgd_solver.cpp:106] Iteration 5200, lr = 0.01
I1209 14:35:23.196521  3198 solver.cpp:237] Iteration 5220, loss = 5.17469
I1209 14:35:23.196558  3198 solver.cpp:253]     Train net output #0: loss = 5.17469 (* 1 = 5.17469 loss)
I1209 14:35:23.196564  3198 sgd_solver.cpp:106] Iteration 5220, lr = 0.01
I1209 14:35:30.837718  3198 solver.cpp:237] Iteration 5240, loss = 4.98765
I1209 14:35:30.837832  3198 solver.cpp:253]     Train net output #0: loss = 4.98765 (* 1 = 4.98765 loss)
I1209 14:35:30.837839  3198 sgd_solver.cpp:106] Iteration 5240, lr = 0.01
I1209 14:35:38.448346  3198 solver.cpp:237] Iteration 5260, loss = 5.13047
I1209 14:35:38.448384  3198 solver.cpp:253]     Train net output #0: loss = 5.13047 (* 1 = 5.13047 loss)
I1209 14:35:38.448390  3198 sgd_solver.cpp:106] Iteration 5260, lr = 0.01
I1209 14:35:46.058688  3198 solver.cpp:237] Iteration 5280, loss = 4.89228
I1209 14:35:46.058724  3198 solver.cpp:253]     Train net output #0: loss = 4.89228 (* 1 = 4.89228 loss)
I1209 14:35:46.058730  3198 sgd_solver.cpp:106] Iteration 5280, lr = 0.01
I1209 14:35:53.726766  3198 solver.cpp:237] Iteration 5300, loss = 5.33534
I1209 14:35:53.726802  3198 solver.cpp:253]     Train net output #0: loss = 5.33534 (* 1 = 5.33534 loss)
I1209 14:35:53.726809  3198 sgd_solver.cpp:106] Iteration 5300, lr = 0.01
I1209 14:36:01.391211  3198 solver.cpp:237] Iteration 5320, loss = 4.96509
I1209 14:36:01.391367  3198 solver.cpp:253]     Train net output #0: loss = 4.96509 (* 1 = 4.96509 loss)
I1209 14:36:01.391384  3198 sgd_solver.cpp:106] Iteration 5320, lr = 0.01
I1209 14:36:09.099385  3198 solver.cpp:237] Iteration 5340, loss = 4.83132
I1209 14:36:09.099412  3198 solver.cpp:253]     Train net output #0: loss = 4.83132 (* 1 = 4.83132 loss)
I1209 14:36:09.099417  3198 sgd_solver.cpp:106] Iteration 5340, lr = 0.01
I1209 14:36:16.744459  3198 solver.cpp:237] Iteration 5360, loss = 5.27323
I1209 14:36:16.744495  3198 solver.cpp:253]     Train net output #0: loss = 5.27323 (* 1 = 5.27323 loss)
I1209 14:36:16.744501  3198 sgd_solver.cpp:106] Iteration 5360, lr = 0.01
I1209 14:36:24.432796  3198 solver.cpp:237] Iteration 5380, loss = 5.09512
I1209 14:36:24.432833  3198 solver.cpp:253]     Train net output #0: loss = 5.09512 (* 1 = 5.09512 loss)
I1209 14:36:24.432839  3198 sgd_solver.cpp:106] Iteration 5380, lr = 0.01
I1209 14:36:32.079746  3198 solver.cpp:237] Iteration 5400, loss = 5.11119
I1209 14:36:32.079918  3198 solver.cpp:253]     Train net output #0: loss = 5.11119 (* 1 = 5.11119 loss)
I1209 14:36:32.079928  3198 sgd_solver.cpp:106] Iteration 5400, lr = 0.01
I1209 14:36:39.727875  3198 solver.cpp:237] Iteration 5420, loss = 4.90996
I1209 14:36:39.727912  3198 solver.cpp:253]     Train net output #0: loss = 4.90996 (* 1 = 4.90996 loss)
I1209 14:36:39.727918  3198 sgd_solver.cpp:106] Iteration 5420, lr = 0.01
I1209 14:36:47.353844  3198 solver.cpp:237] Iteration 5440, loss = 5.00688
I1209 14:36:47.353879  3198 solver.cpp:253]     Train net output #0: loss = 5.00688 (* 1 = 5.00688 loss)
I1209 14:36:47.353885  3198 sgd_solver.cpp:106] Iteration 5440, lr = 0.01
I1209 14:36:55.001380  3198 solver.cpp:237] Iteration 5460, loss = 5.41212
I1209 14:36:55.001416  3198 solver.cpp:253]     Train net output #0: loss = 5.41212 (* 1 = 5.41212 loss)
I1209 14:36:55.001422  3198 sgd_solver.cpp:106] Iteration 5460, lr = 0.01
I1209 14:37:02.615660  3198 solver.cpp:237] Iteration 5480, loss = 5.04431
I1209 14:37:02.615806  3198 solver.cpp:253]     Train net output #0: loss = 5.04431 (* 1 = 5.04431 loss)
I1209 14:37:02.615814  3198 sgd_solver.cpp:106] Iteration 5480, lr = 0.01
I1209 14:37:10.290688  3198 solver.cpp:237] Iteration 5500, loss = 5.00674
I1209 14:37:10.290724  3198 solver.cpp:253]     Train net output #0: loss = 5.00674 (* 1 = 5.00674 loss)
I1209 14:37:10.290729  3198 sgd_solver.cpp:106] Iteration 5500, lr = 0.01
I1209 14:37:17.942183  3198 solver.cpp:237] Iteration 5520, loss = 4.90719
I1209 14:37:17.942219  3198 solver.cpp:253]     Train net output #0: loss = 4.90719 (* 1 = 4.90719 loss)
I1209 14:37:17.942225  3198 sgd_solver.cpp:106] Iteration 5520, lr = 0.01
I1209 14:37:25.588232  3198 solver.cpp:237] Iteration 5540, loss = 4.94951
I1209 14:37:25.588268  3198 solver.cpp:253]     Train net output #0: loss = 4.94951 (* 1 = 4.94951 loss)
I1209 14:37:25.588274  3198 sgd_solver.cpp:106] Iteration 5540, lr = 0.01
I1209 14:37:33.251008  3198 solver.cpp:237] Iteration 5560, loss = 4.81457
I1209 14:37:33.251144  3198 solver.cpp:253]     Train net output #0: loss = 4.81457 (* 1 = 4.81457 loss)
I1209 14:37:33.251162  3198 sgd_solver.cpp:106] Iteration 5560, lr = 0.01
I1209 14:37:40.916712  3198 solver.cpp:237] Iteration 5580, loss = 4.85987
I1209 14:37:40.916749  3198 solver.cpp:253]     Train net output #0: loss = 4.85987 (* 1 = 4.85987 loss)
I1209 14:37:40.916755  3198 sgd_solver.cpp:106] Iteration 5580, lr = 0.01
I1209 14:37:48.593719  3198 solver.cpp:237] Iteration 5600, loss = 5.14585
I1209 14:37:48.593755  3198 solver.cpp:253]     Train net output #0: loss = 5.14585 (* 1 = 5.14585 loss)
I1209 14:37:48.593760  3198 sgd_solver.cpp:106] Iteration 5600, lr = 0.01
I1209 14:37:56.253371  3198 solver.cpp:237] Iteration 5620, loss = 5.10257
I1209 14:37:56.253407  3198 solver.cpp:253]     Train net output #0: loss = 5.10257 (* 1 = 5.10257 loss)
I1209 14:37:56.253412  3198 sgd_solver.cpp:106] Iteration 5620, lr = 0.01
I1209 14:38:03.846086  3198 solver.cpp:237] Iteration 5640, loss = 5.12464
I1209 14:38:03.846228  3198 solver.cpp:253]     Train net output #0: loss = 5.12464 (* 1 = 5.12464 loss)
I1209 14:38:03.846246  3198 sgd_solver.cpp:106] Iteration 5640, lr = 0.01
I1209 14:38:11.441366  3198 solver.cpp:237] Iteration 5660, loss = 5.16814
I1209 14:38:11.441401  3198 solver.cpp:253]     Train net output #0: loss = 5.16814 (* 1 = 5.16814 loss)
I1209 14:38:11.441407  3198 sgd_solver.cpp:106] Iteration 5660, lr = 0.01
I1209 14:38:19.061197  3198 solver.cpp:237] Iteration 5680, loss = 5.03643
I1209 14:38:19.061233  3198 solver.cpp:253]     Train net output #0: loss = 5.03643 (* 1 = 5.03643 loss)
I1209 14:38:19.061239  3198 sgd_solver.cpp:106] Iteration 5680, lr = 0.01
I1209 14:38:26.716253  3198 solver.cpp:237] Iteration 5700, loss = 5.07226
I1209 14:38:26.716291  3198 solver.cpp:253]     Train net output #0: loss = 5.07226 (* 1 = 5.07226 loss)
I1209 14:38:26.716296  3198 sgd_solver.cpp:106] Iteration 5700, lr = 0.01
I1209 14:38:34.427837  3198 solver.cpp:237] Iteration 5720, loss = 5.21688
I1209 14:38:34.428009  3198 solver.cpp:253]     Train net output #0: loss = 5.21688 (* 1 = 5.21688 loss)
I1209 14:38:34.428015  3198 sgd_solver.cpp:106] Iteration 5720, lr = 0.01
I1209 14:38:42.107655  3198 solver.cpp:237] Iteration 5740, loss = 5.32263
I1209 14:38:42.107691  3198 solver.cpp:253]     Train net output #0: loss = 5.32263 (* 1 = 5.32263 loss)
I1209 14:38:42.107697  3198 sgd_solver.cpp:106] Iteration 5740, lr = 0.01
I1209 14:38:49.809149  3198 solver.cpp:237] Iteration 5760, loss = 5.31274
I1209 14:38:49.809188  3198 solver.cpp:253]     Train net output #0: loss = 5.31274 (* 1 = 5.31274 loss)
I1209 14:38:49.809195  3198 sgd_solver.cpp:106] Iteration 5760, lr = 0.01
I1209 14:38:57.462435  3198 solver.cpp:237] Iteration 5780, loss = 5.00029
I1209 14:38:57.462471  3198 solver.cpp:253]     Train net output #0: loss = 5.00029 (* 1 = 5.00029 loss)
I1209 14:38:57.462477  3198 sgd_solver.cpp:106] Iteration 5780, lr = 0.01
I1209 14:39:05.455226  3198 solver.cpp:237] Iteration 5800, loss = 4.91577
I1209 14:39:05.455382  3198 solver.cpp:253]     Train net output #0: loss = 4.91577 (* 1 = 4.91577 loss)
I1209 14:39:05.455400  3198 sgd_solver.cpp:106] Iteration 5800, lr = 0.01
I1209 14:39:13.657363  3198 solver.cpp:237] Iteration 5820, loss = 4.87424
I1209 14:39:13.657426  3198 solver.cpp:253]     Train net output #0: loss = 4.87424 (* 1 = 4.87424 loss)
I1209 14:39:13.657438  3198 sgd_solver.cpp:106] Iteration 5820, lr = 0.01
I1209 14:39:21.450371  3198 solver.cpp:237] Iteration 5840, loss = 5.05483
I1209 14:39:21.450408  3198 solver.cpp:253]     Train net output #0: loss = 5.05483 (* 1 = 5.05483 loss)
I1209 14:39:21.450414  3198 sgd_solver.cpp:106] Iteration 5840, lr = 0.01
I1209 14:39:29.118417  3198 solver.cpp:237] Iteration 5860, loss = 4.91511
I1209 14:39:29.118484  3198 solver.cpp:253]     Train net output #0: loss = 4.91511 (* 1 = 4.91511 loss)
I1209 14:39:29.118501  3198 sgd_solver.cpp:106] Iteration 5860, lr = 0.01
I1209 14:39:36.784195  3198 solver.cpp:237] Iteration 5880, loss = 4.80914
I1209 14:39:36.784307  3198 solver.cpp:253]     Train net output #0: loss = 4.80914 (* 1 = 4.80914 loss)
I1209 14:39:36.784313  3198 sgd_solver.cpp:106] Iteration 5880, lr = 0.01
I1209 14:39:44.442603  3198 solver.cpp:237] Iteration 5900, loss = 4.82184
I1209 14:39:44.442639  3198 solver.cpp:253]     Train net output #0: loss = 4.82184 (* 1 = 4.82184 loss)
I1209 14:39:44.442646  3198 sgd_solver.cpp:106] Iteration 5900, lr = 0.01
I1209 14:39:52.119382  3198 solver.cpp:237] Iteration 5920, loss = 4.83378
I1209 14:39:52.119418  3198 solver.cpp:253]     Train net output #0: loss = 4.83378 (* 1 = 4.83378 loss)
I1209 14:39:52.119423  3198 sgd_solver.cpp:106] Iteration 5920, lr = 0.01
I1209 14:39:59.812121  3198 solver.cpp:237] Iteration 5940, loss = 4.86392
I1209 14:39:59.812151  3198 solver.cpp:253]     Train net output #0: loss = 4.86392 (* 1 = 4.86392 loss)
I1209 14:39:59.812158  3198 sgd_solver.cpp:106] Iteration 5940, lr = 0.01
I1209 14:40:07.472864  3198 solver.cpp:237] Iteration 5960, loss = 4.84489
I1209 14:40:07.473049  3198 solver.cpp:253]     Train net output #0: loss = 4.84489 (* 1 = 4.84489 loss)
I1209 14:40:07.473058  3198 sgd_solver.cpp:106] Iteration 5960, lr = 0.01
I1209 14:40:15.102239  3198 solver.cpp:237] Iteration 5980, loss = 4.66533
I1209 14:40:15.102275  3198 solver.cpp:253]     Train net output #0: loss = 4.66533 (* 1 = 4.66533 loss)
I1209 14:40:15.102282  3198 sgd_solver.cpp:106] Iteration 5980, lr = 0.01
I1209 14:40:22.391093  3198 solver.cpp:341] Iteration 6000, Testing net (#0)
I1209 14:40:27.550750  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:41:40.598588  3198 solver.cpp:409]     Test net output #0: accuracy = 0.12546
I1209 14:41:40.598737  3198 solver.cpp:409]     Test net output #1: loss = 4.6766 (* 1 = 4.6766 loss)
I1209 14:41:40.800490  3198 solver.cpp:237] Iteration 6000, loss = 5.02835
I1209 14:41:40.800524  3198 solver.cpp:253]     Train net output #0: loss = 5.02835 (* 1 = 5.02835 loss)
I1209 14:41:40.800531  3198 sgd_solver.cpp:106] Iteration 6000, lr = 0.01
I1209 14:41:47.786928  3198 solver.cpp:237] Iteration 6020, loss = 4.84981
I1209 14:41:47.786967  3198 solver.cpp:253]     Train net output #0: loss = 4.84981 (* 1 = 4.84981 loss)
I1209 14:41:47.786973  3198 sgd_solver.cpp:106] Iteration 6020, lr = 0.01
I1209 14:41:55.393553  3198 solver.cpp:237] Iteration 6040, loss = 4.71215
I1209 14:41:55.393589  3198 solver.cpp:253]     Train net output #0: loss = 4.71215 (* 1 = 4.71215 loss)
I1209 14:41:55.393594  3198 sgd_solver.cpp:106] Iteration 6040, lr = 0.01
I1209 14:42:01.162482  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:42:03.060256  3198 solver.cpp:237] Iteration 6060, loss = 5.00658
I1209 14:42:03.060293  3198 solver.cpp:253]     Train net output #0: loss = 5.00658 (* 1 = 5.00658 loss)
I1209 14:42:03.060299  3198 sgd_solver.cpp:106] Iteration 6060, lr = 0.01
I1209 14:42:10.722308  3198 solver.cpp:237] Iteration 6080, loss = 4.94699
I1209 14:42:10.722443  3198 solver.cpp:253]     Train net output #0: loss = 4.94699 (* 1 = 4.94699 loss)
I1209 14:42:10.722460  3198 sgd_solver.cpp:106] Iteration 6080, lr = 0.01
I1209 14:42:18.367424  3198 solver.cpp:237] Iteration 6100, loss = 4.76382
I1209 14:42:18.367460  3198 solver.cpp:253]     Train net output #0: loss = 4.76382 (* 1 = 4.76382 loss)
I1209 14:42:18.367465  3198 sgd_solver.cpp:106] Iteration 6100, lr = 0.01
I1209 14:42:26.039796  3198 solver.cpp:237] Iteration 6120, loss = 4.60282
I1209 14:42:26.039832  3198 solver.cpp:253]     Train net output #0: loss = 4.60282 (* 1 = 4.60282 loss)
I1209 14:42:26.039839  3198 sgd_solver.cpp:106] Iteration 6120, lr = 0.01
I1209 14:42:33.721544  3198 solver.cpp:237] Iteration 6140, loss = 4.79441
I1209 14:42:33.721578  3198 solver.cpp:253]     Train net output #0: loss = 4.79441 (* 1 = 4.79441 loss)
I1209 14:42:33.721585  3198 sgd_solver.cpp:106] Iteration 6140, lr = 0.01
I1209 14:42:41.389879  3198 solver.cpp:237] Iteration 6160, loss = 4.73615
I1209 14:42:41.389964  3198 solver.cpp:253]     Train net output #0: loss = 4.73615 (* 1 = 4.73615 loss)
I1209 14:42:41.389979  3198 sgd_solver.cpp:106] Iteration 6160, lr = 0.01
I1209 14:42:49.051586  3198 solver.cpp:237] Iteration 6180, loss = 4.64619
I1209 14:42:49.051622  3198 solver.cpp:253]     Train net output #0: loss = 4.64619 (* 1 = 4.64619 loss)
I1209 14:42:49.051628  3198 sgd_solver.cpp:106] Iteration 6180, lr = 0.01
I1209 14:42:56.723155  3198 solver.cpp:237] Iteration 6200, loss = 4.72283
I1209 14:42:56.723191  3198 solver.cpp:253]     Train net output #0: loss = 4.72283 (* 1 = 4.72283 loss)
I1209 14:42:56.723196  3198 sgd_solver.cpp:106] Iteration 6200, lr = 0.01
I1209 14:43:04.369735  3198 solver.cpp:237] Iteration 6220, loss = 4.68937
I1209 14:43:04.369771  3198 solver.cpp:253]     Train net output #0: loss = 4.68937 (* 1 = 4.68937 loss)
I1209 14:43:04.369776  3198 sgd_solver.cpp:106] Iteration 6220, lr = 0.01
I1209 14:43:12.038950  3198 solver.cpp:237] Iteration 6240, loss = 4.76508
I1209 14:43:12.039119  3198 solver.cpp:253]     Train net output #0: loss = 4.76508 (* 1 = 4.76508 loss)
I1209 14:43:12.039126  3198 sgd_solver.cpp:106] Iteration 6240, lr = 0.01
I1209 14:43:19.746343  3198 solver.cpp:237] Iteration 6260, loss = 4.61945
I1209 14:43:19.746369  3198 solver.cpp:253]     Train net output #0: loss = 4.61945 (* 1 = 4.61945 loss)
I1209 14:43:19.746376  3198 sgd_solver.cpp:106] Iteration 6260, lr = 0.01
I1209 14:43:27.432303  3198 solver.cpp:237] Iteration 6280, loss = 4.83817
I1209 14:43:27.432340  3198 solver.cpp:253]     Train net output #0: loss = 4.83817 (* 1 = 4.83817 loss)
I1209 14:43:27.432346  3198 sgd_solver.cpp:106] Iteration 6280, lr = 0.01
I1209 14:43:35.129845  3198 solver.cpp:237] Iteration 6300, loss = 4.90235
I1209 14:43:35.129881  3198 solver.cpp:253]     Train net output #0: loss = 4.90235 (* 1 = 4.90235 loss)
I1209 14:43:35.129889  3198 sgd_solver.cpp:106] Iteration 6300, lr = 0.01
I1209 14:43:42.791882  3198 solver.cpp:237] Iteration 6320, loss = 4.79847
I1209 14:43:42.792258  3198 solver.cpp:253]     Train net output #0: loss = 4.79847 (* 1 = 4.79847 loss)
I1209 14:43:42.792268  3198 sgd_solver.cpp:106] Iteration 6320, lr = 0.01
I1209 14:43:50.475517  3198 solver.cpp:237] Iteration 6340, loss = 4.63545
I1209 14:43:50.475553  3198 solver.cpp:253]     Train net output #0: loss = 4.63545 (* 1 = 4.63545 loss)
I1209 14:43:50.475559  3198 sgd_solver.cpp:106] Iteration 6340, lr = 0.01
I1209 14:43:58.122455  3198 solver.cpp:237] Iteration 6360, loss = 4.73132
I1209 14:43:58.122490  3198 solver.cpp:253]     Train net output #0: loss = 4.73132 (* 1 = 4.73132 loss)
I1209 14:43:58.122498  3198 sgd_solver.cpp:106] Iteration 6360, lr = 0.01
I1209 14:44:06.613407  3198 solver.cpp:237] Iteration 6380, loss = 4.77617
I1209 14:44:06.613443  3198 solver.cpp:253]     Train net output #0: loss = 4.77617 (* 1 = 4.77617 loss)
I1209 14:44:06.613459  3198 sgd_solver.cpp:106] Iteration 6380, lr = 0.01
I1209 14:44:14.627272  3198 solver.cpp:237] Iteration 6400, loss = 4.5878
I1209 14:44:14.627403  3198 solver.cpp:253]     Train net output #0: loss = 4.5878 (* 1 = 4.5878 loss)
I1209 14:44:14.627418  3198 sgd_solver.cpp:106] Iteration 6400, lr = 0.01
I1209 14:44:22.342336  3198 solver.cpp:237] Iteration 6420, loss = 4.6652
I1209 14:44:22.342371  3198 solver.cpp:253]     Train net output #0: loss = 4.6652 (* 1 = 4.6652 loss)
I1209 14:44:22.342377  3198 sgd_solver.cpp:106] Iteration 6420, lr = 0.01
I1209 14:44:30.018611  3198 solver.cpp:237] Iteration 6440, loss = 4.89697
I1209 14:44:30.018647  3198 solver.cpp:253]     Train net output #0: loss = 4.89697 (* 1 = 4.89697 loss)
I1209 14:44:30.018653  3198 sgd_solver.cpp:106] Iteration 6440, lr = 0.01
I1209 14:44:37.682063  3198 solver.cpp:237] Iteration 6460, loss = 4.78946
I1209 14:44:37.682106  3198 solver.cpp:253]     Train net output #0: loss = 4.78946 (* 1 = 4.78946 loss)
I1209 14:44:37.682123  3198 sgd_solver.cpp:106] Iteration 6460, lr = 0.01
I1209 14:44:45.261474  3198 solver.cpp:237] Iteration 6480, loss = 4.86179
I1209 14:44:45.261566  3198 solver.cpp:253]     Train net output #0: loss = 4.86179 (* 1 = 4.86179 loss)
I1209 14:44:45.261574  3198 sgd_solver.cpp:106] Iteration 6480, lr = 0.01
I1209 14:44:52.950002  3198 solver.cpp:237] Iteration 6500, loss = 4.86325
I1209 14:44:52.950039  3198 solver.cpp:253]     Train net output #0: loss = 4.86325 (* 1 = 4.86325 loss)
I1209 14:44:52.950047  3198 sgd_solver.cpp:106] Iteration 6500, lr = 0.01
I1209 14:45:00.604276  3198 solver.cpp:237] Iteration 6520, loss = 4.84876
I1209 14:45:00.604321  3198 solver.cpp:253]     Train net output #0: loss = 4.84876 (* 1 = 4.84876 loss)
I1209 14:45:00.604327  3198 sgd_solver.cpp:106] Iteration 6520, lr = 0.01
I1209 14:45:08.248852  3198 solver.cpp:237] Iteration 6540, loss = 4.47835
I1209 14:45:08.248886  3198 solver.cpp:253]     Train net output #0: loss = 4.47835 (* 1 = 4.47835 loss)
I1209 14:45:08.248893  3198 sgd_solver.cpp:106] Iteration 6540, lr = 0.01
I1209 14:45:15.930281  3198 solver.cpp:237] Iteration 6560, loss = 4.46503
I1209 14:45:15.930433  3198 solver.cpp:253]     Train net output #0: loss = 4.46503 (* 1 = 4.46503 loss)
I1209 14:45:15.930440  3198 sgd_solver.cpp:106] Iteration 6560, lr = 0.01
I1209 14:45:23.591181  3198 solver.cpp:237] Iteration 6580, loss = 4.67099
I1209 14:45:23.591217  3198 solver.cpp:253]     Train net output #0: loss = 4.67099 (* 1 = 4.67099 loss)
I1209 14:45:23.591223  3198 sgd_solver.cpp:106] Iteration 6580, lr = 0.01
I1209 14:45:31.264508  3198 solver.cpp:237] Iteration 6600, loss = 4.60414
I1209 14:45:31.264545  3198 solver.cpp:253]     Train net output #0: loss = 4.60414 (* 1 = 4.60414 loss)
I1209 14:45:31.264551  3198 sgd_solver.cpp:106] Iteration 6600, lr = 0.01
I1209 14:45:38.926820  3198 solver.cpp:237] Iteration 6620, loss = 4.58212
I1209 14:45:38.926863  3198 solver.cpp:253]     Train net output #0: loss = 4.58212 (* 1 = 4.58212 loss)
I1209 14:45:38.926869  3198 sgd_solver.cpp:106] Iteration 6620, lr = 0.01
I1209 14:45:46.610644  3198 solver.cpp:237] Iteration 6640, loss = 4.75254
I1209 14:45:46.610805  3198 solver.cpp:253]     Train net output #0: loss = 4.75254 (* 1 = 4.75254 loss)
I1209 14:45:46.610812  3198 sgd_solver.cpp:106] Iteration 6640, lr = 0.01
I1209 14:45:54.275377  3198 solver.cpp:237] Iteration 6660, loss = 4.58446
I1209 14:45:54.275411  3198 solver.cpp:253]     Train net output #0: loss = 4.58446 (* 1 = 4.58446 loss)
I1209 14:45:54.275416  3198 sgd_solver.cpp:106] Iteration 6660, lr = 0.01
I1209 14:46:01.915562  3198 solver.cpp:237] Iteration 6680, loss = 4.63425
I1209 14:46:01.915599  3198 solver.cpp:253]     Train net output #0: loss = 4.63425 (* 1 = 4.63425 loss)
I1209 14:46:01.915606  3198 sgd_solver.cpp:106] Iteration 6680, lr = 0.01
I1209 14:46:09.585427  3198 solver.cpp:237] Iteration 6700, loss = 4.66587
I1209 14:46:09.585463  3198 solver.cpp:253]     Train net output #0: loss = 4.66587 (* 1 = 4.66587 loss)
I1209 14:46:09.585469  3198 sgd_solver.cpp:106] Iteration 6700, lr = 0.01
I1209 14:46:17.230625  3198 solver.cpp:237] Iteration 6720, loss = 4.8365
I1209 14:46:17.230747  3198 solver.cpp:253]     Train net output #0: loss = 4.8365 (* 1 = 4.8365 loss)
I1209 14:46:17.230754  3198 sgd_solver.cpp:106] Iteration 6720, lr = 0.01
I1209 14:46:24.875031  3198 solver.cpp:237] Iteration 6740, loss = 4.7549
I1209 14:46:24.875067  3198 solver.cpp:253]     Train net output #0: loss = 4.7549 (* 1 = 4.7549 loss)
I1209 14:46:24.875083  3198 sgd_solver.cpp:106] Iteration 6740, lr = 0.01
I1209 14:46:32.518628  3198 solver.cpp:237] Iteration 6760, loss = 4.62413
I1209 14:46:32.518664  3198 solver.cpp:253]     Train net output #0: loss = 4.62413 (* 1 = 4.62413 loss)
I1209 14:46:32.518671  3198 sgd_solver.cpp:106] Iteration 6760, lr = 0.01
I1209 14:46:40.114404  3198 solver.cpp:237] Iteration 6780, loss = 4.78233
I1209 14:46:40.114442  3198 solver.cpp:253]     Train net output #0: loss = 4.78233 (* 1 = 4.78233 loss)
I1209 14:46:40.114449  3198 sgd_solver.cpp:106] Iteration 6780, lr = 0.01
I1209 14:46:47.759907  3198 solver.cpp:237] Iteration 6800, loss = 4.69849
I1209 14:46:47.760012  3198 solver.cpp:253]     Train net output #0: loss = 4.69849 (* 1 = 4.69849 loss)
I1209 14:46:47.760030  3198 sgd_solver.cpp:106] Iteration 6800, lr = 0.01
I1209 14:46:55.444175  3198 solver.cpp:237] Iteration 6820, loss = 4.49002
I1209 14:46:55.444211  3198 solver.cpp:253]     Train net output #0: loss = 4.49002 (* 1 = 4.49002 loss)
I1209 14:46:55.444217  3198 sgd_solver.cpp:106] Iteration 6820, lr = 0.01
I1209 14:47:03.139148  3198 solver.cpp:237] Iteration 6840, loss = 4.45966
I1209 14:47:03.139185  3198 solver.cpp:253]     Train net output #0: loss = 4.45966 (* 1 = 4.45966 loss)
I1209 14:47:03.139191  3198 sgd_solver.cpp:106] Iteration 6840, lr = 0.01
I1209 14:47:10.757849  3198 solver.cpp:237] Iteration 6860, loss = 4.65262
I1209 14:47:10.757885  3198 solver.cpp:253]     Train net output #0: loss = 4.65262 (* 1 = 4.65262 loss)
I1209 14:47:10.757891  3198 sgd_solver.cpp:106] Iteration 6860, lr = 0.01
I1209 14:47:18.445909  3198 solver.cpp:237] Iteration 6880, loss = 4.72719
I1209 14:47:18.446095  3198 solver.cpp:253]     Train net output #0: loss = 4.72719 (* 1 = 4.72719 loss)
I1209 14:47:18.446104  3198 sgd_solver.cpp:106] Iteration 6880, lr = 0.01
I1209 14:47:26.086077  3198 solver.cpp:237] Iteration 6900, loss = 4.38299
I1209 14:47:26.086114  3198 solver.cpp:253]     Train net output #0: loss = 4.38299 (* 1 = 4.38299 loss)
I1209 14:47:26.086119  3198 sgd_solver.cpp:106] Iteration 6900, lr = 0.01
I1209 14:47:33.738535  3198 solver.cpp:237] Iteration 6920, loss = 4.39824
I1209 14:47:33.738570  3198 solver.cpp:253]     Train net output #0: loss = 4.39824 (* 1 = 4.39824 loss)
I1209 14:47:33.738577  3198 sgd_solver.cpp:106] Iteration 6920, lr = 0.01
I1209 14:47:41.409584  3198 solver.cpp:237] Iteration 6940, loss = 4.57131
I1209 14:47:41.409622  3198 solver.cpp:253]     Train net output #0: loss = 4.57131 (* 1 = 4.57131 loss)
I1209 14:47:41.409628  3198 sgd_solver.cpp:106] Iteration 6940, lr = 0.01
I1209 14:47:49.096271  3198 solver.cpp:237] Iteration 6960, loss = 4.50841
I1209 14:47:49.096422  3198 solver.cpp:253]     Train net output #0: loss = 4.50841 (* 1 = 4.50841 loss)
I1209 14:47:49.096429  3198 sgd_solver.cpp:106] Iteration 6960, lr = 0.01
I1209 14:47:56.801978  3198 solver.cpp:237] Iteration 6980, loss = 4.49882
I1209 14:47:56.802014  3198 solver.cpp:253]     Train net output #0: loss = 4.49882 (* 1 = 4.49882 loss)
I1209 14:47:56.802021  3198 sgd_solver.cpp:106] Iteration 6980, lr = 0.01
I1209 14:48:04.119076  3198 solver.cpp:341] Iteration 7000, Testing net (#0)
I1209 14:48:08.456070  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:49:22.105299  3198 solver.cpp:409]     Test net output #0: accuracy = 0.15056
I1209 14:49:22.105403  3198 solver.cpp:409]     Test net output #1: loss = 4.44399 (* 1 = 4.44399 loss)
I1209 14:49:22.307467  3198 solver.cpp:237] Iteration 7000, loss = 4.42062
I1209 14:49:22.307507  3198 solver.cpp:253]     Train net output #0: loss = 4.42062 (* 1 = 4.42062 loss)
I1209 14:49:22.307512  3198 sgd_solver.cpp:106] Iteration 7000, lr = 0.01
I1209 14:49:29.297927  3198 solver.cpp:237] Iteration 7020, loss = 4.6012
I1209 14:49:29.297966  3198 solver.cpp:253]     Train net output #0: loss = 4.6012 (* 1 = 4.6012 loss)
I1209 14:49:29.297971  3198 sgd_solver.cpp:106] Iteration 7020, lr = 0.01
I1209 14:49:36.959215  3198 solver.cpp:237] Iteration 7040, loss = 4.47724
I1209 14:49:36.959250  3198 solver.cpp:253]     Train net output #0: loss = 4.47724 (* 1 = 4.47724 loss)
I1209 14:49:36.959257  3198 sgd_solver.cpp:106] Iteration 7040, lr = 0.01
I1209 14:49:44.609078  3198 solver.cpp:237] Iteration 7060, loss = 4.55536
I1209 14:49:44.609113  3198 solver.cpp:253]     Train net output #0: loss = 4.55536 (* 1 = 4.55536 loss)
I1209 14:49:44.609119  3198 sgd_solver.cpp:106] Iteration 7060, lr = 0.01
I1209 14:49:45.779247  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:49:52.176581  3198 solver.cpp:237] Iteration 7080, loss = 4.56161
I1209 14:49:52.176736  3198 solver.cpp:253]     Train net output #0: loss = 4.56161 (* 1 = 4.56161 loss)
I1209 14:49:52.176753  3198 sgd_solver.cpp:106] Iteration 7080, lr = 0.01
I1209 14:49:59.792456  3198 solver.cpp:237] Iteration 7100, loss = 4.74774
I1209 14:49:59.792495  3198 solver.cpp:253]     Train net output #0: loss = 4.74774 (* 1 = 4.74774 loss)
I1209 14:49:59.792500  3198 sgd_solver.cpp:106] Iteration 7100, lr = 0.01
I1209 14:50:07.396697  3198 solver.cpp:237] Iteration 7120, loss = 4.45242
I1209 14:50:07.396731  3198 solver.cpp:253]     Train net output #0: loss = 4.45242 (* 1 = 4.45242 loss)
I1209 14:50:07.396738  3198 sgd_solver.cpp:106] Iteration 7120, lr = 0.01
I1209 14:50:15.054406  3198 solver.cpp:237] Iteration 7140, loss = 4.5128
I1209 14:50:15.054441  3198 solver.cpp:253]     Train net output #0: loss = 4.5128 (* 1 = 4.5128 loss)
I1209 14:50:15.054447  3198 sgd_solver.cpp:106] Iteration 7140, lr = 0.01
I1209 14:50:22.660545  3198 solver.cpp:237] Iteration 7160, loss = 4.53605
I1209 14:50:22.660717  3198 solver.cpp:253]     Train net output #0: loss = 4.53605 (* 1 = 4.53605 loss)
I1209 14:50:22.660725  3198 sgd_solver.cpp:106] Iteration 7160, lr = 0.01
I1209 14:50:30.231235  3198 solver.cpp:237] Iteration 7180, loss = 4.56227
I1209 14:50:30.231272  3198 solver.cpp:253]     Train net output #0: loss = 4.56227 (* 1 = 4.56227 loss)
I1209 14:50:30.231278  3198 sgd_solver.cpp:106] Iteration 7180, lr = 0.01
I1209 14:50:37.824172  3198 solver.cpp:237] Iteration 7200, loss = 4.4086
I1209 14:50:37.824209  3198 solver.cpp:253]     Train net output #0: loss = 4.4086 (* 1 = 4.4086 loss)
I1209 14:50:37.824215  3198 sgd_solver.cpp:106] Iteration 7200, lr = 0.01
I1209 14:50:45.433403  3198 solver.cpp:237] Iteration 7220, loss = 4.80189
I1209 14:50:45.433439  3198 solver.cpp:253]     Train net output #0: loss = 4.80189 (* 1 = 4.80189 loss)
I1209 14:50:45.433444  3198 sgd_solver.cpp:106] Iteration 7220, lr = 0.01
I1209 14:50:53.043704  3198 solver.cpp:237] Iteration 7240, loss = 4.42912
I1209 14:50:53.043869  3198 solver.cpp:253]     Train net output #0: loss = 4.42912 (* 1 = 4.42912 loss)
I1209 14:50:53.043876  3198 sgd_solver.cpp:106] Iteration 7240, lr = 0.01
I1209 14:51:00.781901  3198 solver.cpp:237] Iteration 7260, loss = 4.44873
I1209 14:51:00.781939  3198 solver.cpp:253]     Train net output #0: loss = 4.44873 (* 1 = 4.44873 loss)
I1209 14:51:00.781945  3198 sgd_solver.cpp:106] Iteration 7260, lr = 0.01
I1209 14:51:08.417366  3198 solver.cpp:237] Iteration 7280, loss = 4.63064
I1209 14:51:08.417402  3198 solver.cpp:253]     Train net output #0: loss = 4.63064 (* 1 = 4.63064 loss)
I1209 14:51:08.417408  3198 sgd_solver.cpp:106] Iteration 7280, lr = 0.01
I1209 14:51:16.072464  3198 solver.cpp:237] Iteration 7300, loss = 4.60103
I1209 14:51:16.072502  3198 solver.cpp:253]     Train net output #0: loss = 4.60103 (* 1 = 4.60103 loss)
I1209 14:51:16.072509  3198 sgd_solver.cpp:106] Iteration 7300, lr = 0.01
I1209 14:51:23.772568  3198 solver.cpp:237] Iteration 7320, loss = 4.32299
I1209 14:51:23.772727  3198 solver.cpp:253]     Train net output #0: loss = 4.32299 (* 1 = 4.32299 loss)
I1209 14:51:23.772734  3198 sgd_solver.cpp:106] Iteration 7320, lr = 0.01
I1209 14:51:31.463663  3198 solver.cpp:237] Iteration 7340, loss = 4.63589
I1209 14:51:31.463701  3198 solver.cpp:253]     Train net output #0: loss = 4.63589 (* 1 = 4.63589 loss)
I1209 14:51:31.463706  3198 sgd_solver.cpp:106] Iteration 7340, lr = 0.01
I1209 14:51:39.109417  3198 solver.cpp:237] Iteration 7360, loss = 4.5815
I1209 14:51:39.109453  3198 solver.cpp:253]     Train net output #0: loss = 4.5815 (* 1 = 4.5815 loss)
I1209 14:51:39.109459  3198 sgd_solver.cpp:106] Iteration 7360, lr = 0.01
I1209 14:51:46.752478  3198 solver.cpp:237] Iteration 7380, loss = 4.47495
I1209 14:51:46.752514  3198 solver.cpp:253]     Train net output #0: loss = 4.47495 (* 1 = 4.47495 loss)
I1209 14:51:46.752521  3198 sgd_solver.cpp:106] Iteration 7380, lr = 0.01
I1209 14:51:54.421406  3198 solver.cpp:237] Iteration 7400, loss = 4.45095
I1209 14:51:54.421592  3198 solver.cpp:253]     Train net output #0: loss = 4.45095 (* 1 = 4.45095 loss)
I1209 14:51:54.421600  3198 sgd_solver.cpp:106] Iteration 7400, lr = 0.01
I1209 14:52:02.093396  3198 solver.cpp:237] Iteration 7420, loss = 4.47493
I1209 14:52:02.093436  3198 solver.cpp:253]     Train net output #0: loss = 4.47493 (* 1 = 4.47493 loss)
I1209 14:52:02.093441  3198 sgd_solver.cpp:106] Iteration 7420, lr = 0.01
I1209 14:52:09.782596  3198 solver.cpp:237] Iteration 7440, loss = 4.49181
I1209 14:52:09.782634  3198 solver.cpp:253]     Train net output #0: loss = 4.49181 (* 1 = 4.49181 loss)
I1209 14:52:09.782640  3198 sgd_solver.cpp:106] Iteration 7440, lr = 0.01
I1209 14:52:17.457847  3198 solver.cpp:237] Iteration 7460, loss = 4.32315
I1209 14:52:17.457881  3198 solver.cpp:253]     Train net output #0: loss = 4.32315 (* 1 = 4.32315 loss)
I1209 14:52:17.457887  3198 sgd_solver.cpp:106] Iteration 7460, lr = 0.01
I1209 14:52:25.119916  3198 solver.cpp:237] Iteration 7480, loss = 4.38695
I1209 14:52:25.120077  3198 solver.cpp:253]     Train net output #0: loss = 4.38695 (* 1 = 4.38695 loss)
I1209 14:52:25.120085  3198 sgd_solver.cpp:106] Iteration 7480, lr = 0.01
I1209 14:52:32.800112  3198 solver.cpp:237] Iteration 7500, loss = 4.59212
I1209 14:52:32.800149  3198 solver.cpp:253]     Train net output #0: loss = 4.59212 (* 1 = 4.59212 loss)
I1209 14:52:32.800155  3198 sgd_solver.cpp:106] Iteration 7500, lr = 0.01
I1209 14:52:40.472234  3198 solver.cpp:237] Iteration 7520, loss = 4.43237
I1209 14:52:40.472272  3198 solver.cpp:253]     Train net output #0: loss = 4.43237 (* 1 = 4.43237 loss)
I1209 14:52:40.472278  3198 sgd_solver.cpp:106] Iteration 7520, lr = 0.01
I1209 14:52:48.106899  3198 solver.cpp:237] Iteration 7540, loss = 4.45567
I1209 14:52:48.106936  3198 solver.cpp:253]     Train net output #0: loss = 4.45567 (* 1 = 4.45567 loss)
I1209 14:52:48.106942  3198 sgd_solver.cpp:106] Iteration 7540, lr = 0.01
I1209 14:52:55.791381  3198 solver.cpp:237] Iteration 7560, loss = 4.3529
I1209 14:52:55.791540  3198 solver.cpp:253]     Train net output #0: loss = 4.3529 (* 1 = 4.3529 loss)
I1209 14:52:55.791548  3198 sgd_solver.cpp:106] Iteration 7560, lr = 0.01
I1209 14:53:03.371937  3198 solver.cpp:237] Iteration 7580, loss = 4.34665
I1209 14:53:03.371973  3198 solver.cpp:253]     Train net output #0: loss = 4.34665 (* 1 = 4.34665 loss)
I1209 14:53:03.371979  3198 sgd_solver.cpp:106] Iteration 7580, lr = 0.01
I1209 14:53:10.947175  3198 solver.cpp:237] Iteration 7600, loss = 4.52552
I1209 14:53:10.947209  3198 solver.cpp:253]     Train net output #0: loss = 4.52552 (* 1 = 4.52552 loss)
I1209 14:53:10.947216  3198 sgd_solver.cpp:106] Iteration 7600, lr = 0.01
I1209 14:53:18.523473  3198 solver.cpp:237] Iteration 7620, loss = 4.62285
I1209 14:53:18.523509  3198 solver.cpp:253]     Train net output #0: loss = 4.62285 (* 1 = 4.62285 loss)
I1209 14:53:18.523515  3198 sgd_solver.cpp:106] Iteration 7620, lr = 0.01
I1209 14:53:26.169028  3198 solver.cpp:237] Iteration 7640, loss = 4.44211
I1209 14:53:26.169144  3198 solver.cpp:253]     Train net output #0: loss = 4.44211 (* 1 = 4.44211 loss)
I1209 14:53:26.169152  3198 sgd_solver.cpp:106] Iteration 7640, lr = 0.01
I1209 14:53:33.831830  3198 solver.cpp:237] Iteration 7660, loss = 4.37804
I1209 14:53:33.831866  3198 solver.cpp:253]     Train net output #0: loss = 4.37804 (* 1 = 4.37804 loss)
I1209 14:53:33.831872  3198 sgd_solver.cpp:106] Iteration 7660, lr = 0.01
I1209 14:53:41.482631  3198 solver.cpp:237] Iteration 7680, loss = 4.34536
I1209 14:53:41.482669  3198 solver.cpp:253]     Train net output #0: loss = 4.34536 (* 1 = 4.34536 loss)
I1209 14:53:41.482676  3198 sgd_solver.cpp:106] Iteration 7680, lr = 0.01
I1209 14:53:49.162276  3198 solver.cpp:237] Iteration 7700, loss = 4.46387
I1209 14:53:49.162312  3198 solver.cpp:253]     Train net output #0: loss = 4.46387 (* 1 = 4.46387 loss)
I1209 14:53:49.162318  3198 sgd_solver.cpp:106] Iteration 7700, lr = 0.01
I1209 14:53:57.400894  3198 solver.cpp:237] Iteration 7720, loss = 4.53584
I1209 14:53:57.401046  3198 solver.cpp:253]     Train net output #0: loss = 4.53584 (* 1 = 4.53584 loss)
I1209 14:53:57.401053  3198 sgd_solver.cpp:106] Iteration 7720, lr = 0.01
I1209 14:54:05.415807  3198 solver.cpp:237] Iteration 7740, loss = 4.31073
I1209 14:54:05.415843  3198 solver.cpp:253]     Train net output #0: loss = 4.31073 (* 1 = 4.31073 loss)
I1209 14:54:05.415849  3198 sgd_solver.cpp:106] Iteration 7740, lr = 0.01
I1209 14:54:13.146055  3198 solver.cpp:237] Iteration 7760, loss = 4.32657
I1209 14:54:13.146091  3198 solver.cpp:253]     Train net output #0: loss = 4.32657 (* 1 = 4.32657 loss)
I1209 14:54:13.146097  3198 sgd_solver.cpp:106] Iteration 7760, lr = 0.01
I1209 14:54:20.785456  3198 solver.cpp:237] Iteration 7780, loss = 4.43802
I1209 14:54:20.785493  3198 solver.cpp:253]     Train net output #0: loss = 4.43802 (* 1 = 4.43802 loss)
I1209 14:54:20.785500  3198 sgd_solver.cpp:106] Iteration 7780, lr = 0.01
I1209 14:54:28.446151  3198 solver.cpp:237] Iteration 7800, loss = 4.30794
I1209 14:54:28.446322  3198 solver.cpp:253]     Train net output #0: loss = 4.30794 (* 1 = 4.30794 loss)
I1209 14:54:28.446331  3198 sgd_solver.cpp:106] Iteration 7800, lr = 0.01
I1209 14:54:36.104763  3198 solver.cpp:237] Iteration 7820, loss = 4.55052
I1209 14:54:36.104799  3198 solver.cpp:253]     Train net output #0: loss = 4.55052 (* 1 = 4.55052 loss)
I1209 14:54:36.104804  3198 sgd_solver.cpp:106] Iteration 7820, lr = 0.01
I1209 14:54:43.786036  3198 solver.cpp:237] Iteration 7840, loss = 4.2414
I1209 14:54:43.786152  3198 solver.cpp:253]     Train net output #0: loss = 4.2414 (* 1 = 4.2414 loss)
I1209 14:54:43.786186  3198 sgd_solver.cpp:106] Iteration 7840, lr = 0.01
I1209 14:54:51.384585  3198 solver.cpp:237] Iteration 7860, loss = 4.27802
I1209 14:54:51.384623  3198 solver.cpp:253]     Train net output #0: loss = 4.27802 (* 1 = 4.27802 loss)
I1209 14:54:51.384629  3198 sgd_solver.cpp:106] Iteration 7860, lr = 0.01
I1209 14:54:59.032078  3198 solver.cpp:237] Iteration 7880, loss = 4.46832
I1209 14:54:59.032186  3198 solver.cpp:253]     Train net output #0: loss = 4.46832 (* 1 = 4.46832 loss)
I1209 14:54:59.032202  3198 sgd_solver.cpp:106] Iteration 7880, lr = 0.01
I1209 14:55:06.666401  3198 solver.cpp:237] Iteration 7900, loss = 4.40744
I1209 14:55:06.666437  3198 solver.cpp:253]     Train net output #0: loss = 4.40744 (* 1 = 4.40744 loss)
I1209 14:55:06.666443  3198 sgd_solver.cpp:106] Iteration 7900, lr = 0.01
I1209 14:55:14.313277  3198 solver.cpp:237] Iteration 7920, loss = 4.3533
I1209 14:55:14.313313  3198 solver.cpp:253]     Train net output #0: loss = 4.3533 (* 1 = 4.3533 loss)
I1209 14:55:14.313319  3198 sgd_solver.cpp:106] Iteration 7920, lr = 0.01
I1209 14:55:21.983229  3198 solver.cpp:237] Iteration 7940, loss = 4.40504
I1209 14:55:21.983265  3198 solver.cpp:253]     Train net output #0: loss = 4.40504 (* 1 = 4.40504 loss)
I1209 14:55:21.983271  3198 sgd_solver.cpp:106] Iteration 7940, lr = 0.01
I1209 14:55:29.622313  3198 solver.cpp:237] Iteration 7960, loss = 4.27311
I1209 14:55:29.622478  3198 solver.cpp:253]     Train net output #0: loss = 4.27311 (* 1 = 4.27311 loss)
I1209 14:55:29.622484  3198 sgd_solver.cpp:106] Iteration 7960, lr = 0.01
I1209 14:55:37.305786  3198 solver.cpp:237] Iteration 7980, loss = 4.40003
I1209 14:55:37.305824  3198 solver.cpp:253]     Train net output #0: loss = 4.40003 (* 1 = 4.40003 loss)
I1209 14:55:37.305830  3198 sgd_solver.cpp:106] Iteration 7980, lr = 0.01
I1209 14:55:44.624671  3198 solver.cpp:341] Iteration 8000, Testing net (#0)
I1209 14:55:49.624104  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:57:01.506867  3198 solver.cpp:409]     Test net output #0: accuracy = 0.17134
I1209 14:57:01.507024  3198 solver.cpp:409]     Test net output #1: loss = 4.29441 (* 1 = 4.29441 loss)
I1209 14:57:01.707098  3198 solver.cpp:237] Iteration 8000, loss = 4.31089
I1209 14:57:01.707134  3198 solver.cpp:253]     Train net output #0: loss = 4.31089 (* 1 = 4.31089 loss)
I1209 14:57:01.707141  3198 sgd_solver.cpp:106] Iteration 8000, lr = 0.01
I1209 14:57:08.678079  3198 solver.cpp:237] Iteration 8020, loss = 4.52014
I1209 14:57:08.678117  3198 solver.cpp:253]     Train net output #0: loss = 4.52014 (* 1 = 4.52014 loss)
I1209 14:57:08.678122  3198 sgd_solver.cpp:106] Iteration 8020, lr = 0.01
I1209 14:57:16.297991  3198 solver.cpp:237] Iteration 8040, loss = 4.36103
I1209 14:57:16.298027  3198 solver.cpp:253]     Train net output #0: loss = 4.36103 (* 1 = 4.36103 loss)
I1209 14:57:16.298033  3198 sgd_solver.cpp:106] Iteration 8040, lr = 0.01
I1209 14:57:23.894193  3198 solver.cpp:237] Iteration 8060, loss = 4.16632
I1209 14:57:23.894229  3198 solver.cpp:253]     Train net output #0: loss = 4.16632 (* 1 = 4.16632 loss)
I1209 14:57:23.894237  3198 sgd_solver.cpp:106] Iteration 8060, lr = 0.01
I1209 14:57:28.079648  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 14:57:31.498922  3198 solver.cpp:237] Iteration 8080, loss = 4.49331
I1209 14:57:31.498968  3198 solver.cpp:253]     Train net output #0: loss = 4.49331 (* 1 = 4.49331 loss)
I1209 14:57:31.498975  3198 sgd_solver.cpp:106] Iteration 8080, lr = 0.01
I1209 14:57:39.184916  3198 solver.cpp:237] Iteration 8100, loss = 4.47438
I1209 14:57:39.185089  3198 solver.cpp:253]     Train net output #0: loss = 4.47438 (* 1 = 4.47438 loss)
I1209 14:57:39.185097  3198 sgd_solver.cpp:106] Iteration 8100, lr = 0.01
I1209 14:57:46.920076  3198 solver.cpp:237] Iteration 8120, loss = 4.42888
I1209 14:57:46.920114  3198 solver.cpp:253]     Train net output #0: loss = 4.42888 (* 1 = 4.42888 loss)
I1209 14:57:46.920120  3198 sgd_solver.cpp:106] Iteration 8120, lr = 0.01
I1209 14:57:54.606871  3198 solver.cpp:237] Iteration 8140, loss = 4.40483
I1209 14:57:54.606905  3198 solver.cpp:253]     Train net output #0: loss = 4.40483 (* 1 = 4.40483 loss)
I1209 14:57:54.606911  3198 sgd_solver.cpp:106] Iteration 8140, lr = 0.01
I1209 14:58:02.257827  3198 solver.cpp:237] Iteration 8160, loss = 4.24414
I1209 14:58:02.257863  3198 solver.cpp:253]     Train net output #0: loss = 4.24414 (* 1 = 4.24414 loss)
I1209 14:58:02.257868  3198 sgd_solver.cpp:106] Iteration 8160, lr = 0.01
I1209 14:58:09.922093  3198 solver.cpp:237] Iteration 8180, loss = 4.20806
I1209 14:58:09.922265  3198 solver.cpp:253]     Train net output #0: loss = 4.20806 (* 1 = 4.20806 loss)
I1209 14:58:09.922273  3198 sgd_solver.cpp:106] Iteration 8180, lr = 0.01
I1209 14:58:17.569317  3198 solver.cpp:237] Iteration 8200, loss = 4.30538
I1209 14:58:17.569351  3198 solver.cpp:253]     Train net output #0: loss = 4.30538 (* 1 = 4.30538 loss)
I1209 14:58:17.569357  3198 sgd_solver.cpp:106] Iteration 8200, lr = 0.01
I1209 14:58:25.216320  3198 solver.cpp:237] Iteration 8220, loss = 4.44707
I1209 14:58:25.216356  3198 solver.cpp:253]     Train net output #0: loss = 4.44707 (* 1 = 4.44707 loss)
I1209 14:58:25.216362  3198 sgd_solver.cpp:106] Iteration 8220, lr = 0.01
I1209 14:58:32.849128  3198 solver.cpp:237] Iteration 8240, loss = 4.35994
I1209 14:58:32.849164  3198 solver.cpp:253]     Train net output #0: loss = 4.35994 (* 1 = 4.35994 loss)
I1209 14:58:32.849170  3198 sgd_solver.cpp:106] Iteration 8240, lr = 0.01
I1209 14:58:40.519309  3198 solver.cpp:237] Iteration 8260, loss = 4.34922
I1209 14:58:40.519443  3198 solver.cpp:253]     Train net output #0: loss = 4.34922 (* 1 = 4.34922 loss)
I1209 14:58:40.519459  3198 sgd_solver.cpp:106] Iteration 8260, lr = 0.01
I1209 14:58:48.170260  3198 solver.cpp:237] Iteration 8280, loss = 4.26054
I1209 14:58:48.170295  3198 solver.cpp:253]     Train net output #0: loss = 4.26054 (* 1 = 4.26054 loss)
I1209 14:58:48.170301  3198 sgd_solver.cpp:106] Iteration 8280, lr = 0.01
I1209 14:58:55.970130  3198 solver.cpp:237] Iteration 8300, loss = 4.48286
I1209 14:58:55.970166  3198 solver.cpp:253]     Train net output #0: loss = 4.48286 (* 1 = 4.48286 loss)
I1209 14:58:55.970172  3198 sgd_solver.cpp:106] Iteration 8300, lr = 0.01
I1209 14:59:04.255818  3198 solver.cpp:237] Iteration 8320, loss = 4.3566
I1209 14:59:04.255853  3198 solver.cpp:253]     Train net output #0: loss = 4.3566 (* 1 = 4.3566 loss)
I1209 14:59:04.255859  3198 sgd_solver.cpp:106] Iteration 8320, lr = 0.01
I1209 14:59:12.226447  3198 solver.cpp:237] Iteration 8340, loss = 4.2498
I1209 14:59:12.226708  3198 solver.cpp:253]     Train net output #0: loss = 4.2498 (* 1 = 4.2498 loss)
I1209 14:59:12.226716  3198 sgd_solver.cpp:106] Iteration 8340, lr = 0.01
I1209 14:59:19.937602  3198 solver.cpp:237] Iteration 8360, loss = 4.31397
I1209 14:59:19.937638  3198 solver.cpp:253]     Train net output #0: loss = 4.31397 (* 1 = 4.31397 loss)
I1209 14:59:19.937644  3198 sgd_solver.cpp:106] Iteration 8360, lr = 0.01
I1209 14:59:27.590437  3198 solver.cpp:237] Iteration 8380, loss = 4.19997
I1209 14:59:27.590474  3198 solver.cpp:253]     Train net output #0: loss = 4.19997 (* 1 = 4.19997 loss)
I1209 14:59:27.590481  3198 sgd_solver.cpp:106] Iteration 8380, lr = 0.01
I1209 14:59:35.259095  3198 solver.cpp:237] Iteration 8400, loss = 4.46734
I1209 14:59:35.259132  3198 solver.cpp:253]     Train net output #0: loss = 4.46734 (* 1 = 4.46734 loss)
I1209 14:59:35.259138  3198 sgd_solver.cpp:106] Iteration 8400, lr = 0.01
I1209 14:59:42.917831  3198 solver.cpp:237] Iteration 8420, loss = 4.0409
I1209 14:59:42.917953  3198 solver.cpp:253]     Train net output #0: loss = 4.0409 (* 1 = 4.0409 loss)
I1209 14:59:42.917959  3198 sgd_solver.cpp:106] Iteration 8420, lr = 0.01
I1209 14:59:50.576591  3198 solver.cpp:237] Iteration 8440, loss = 4.30213
I1209 14:59:50.576625  3198 solver.cpp:253]     Train net output #0: loss = 4.30213 (* 1 = 4.30213 loss)
I1209 14:59:50.576632  3198 sgd_solver.cpp:106] Iteration 8440, lr = 0.01
I1209 14:59:58.259467  3198 solver.cpp:237] Iteration 8460, loss = 4.39626
I1209 14:59:58.259503  3198 solver.cpp:253]     Train net output #0: loss = 4.39626 (* 1 = 4.39626 loss)
I1209 14:59:58.259510  3198 sgd_solver.cpp:106] Iteration 8460, lr = 0.01
I1209 15:00:05.904170  3198 solver.cpp:237] Iteration 8480, loss = 4.29156
I1209 15:00:05.904206  3198 solver.cpp:253]     Train net output #0: loss = 4.29156 (* 1 = 4.29156 loss)
I1209 15:00:05.904211  3198 sgd_solver.cpp:106] Iteration 8480, lr = 0.01
I1209 15:00:13.598659  3198 solver.cpp:237] Iteration 8500, loss = 4.38468
I1209 15:00:13.598786  3198 solver.cpp:253]     Train net output #0: loss = 4.38468 (* 1 = 4.38468 loss)
I1209 15:00:13.598803  3198 sgd_solver.cpp:106] Iteration 8500, lr = 0.01
I1209 15:00:21.230698  3198 solver.cpp:237] Iteration 8520, loss = 4.46132
I1209 15:00:21.230734  3198 solver.cpp:253]     Train net output #0: loss = 4.46132 (* 1 = 4.46132 loss)
I1209 15:00:21.230741  3198 sgd_solver.cpp:106] Iteration 8520, lr = 0.01
I1209 15:00:28.895004  3198 solver.cpp:237] Iteration 8540, loss = 4.25927
I1209 15:00:28.895040  3198 solver.cpp:253]     Train net output #0: loss = 4.25927 (* 1 = 4.25927 loss)
I1209 15:00:28.895045  3198 sgd_solver.cpp:106] Iteration 8540, lr = 0.01
I1209 15:00:36.544322  3198 solver.cpp:237] Iteration 8560, loss = 4.28479
I1209 15:00:36.544356  3198 solver.cpp:253]     Train net output #0: loss = 4.28479 (* 1 = 4.28479 loss)
I1209 15:00:36.544363  3198 sgd_solver.cpp:106] Iteration 8560, lr = 0.01
I1209 15:00:44.210024  3198 solver.cpp:237] Iteration 8580, loss = 4.42848
I1209 15:00:44.210199  3198 solver.cpp:253]     Train net output #0: loss = 4.42848 (* 1 = 4.42848 loss)
I1209 15:00:44.210216  3198 sgd_solver.cpp:106] Iteration 8580, lr = 0.01
I1209 15:00:51.840095  3198 solver.cpp:237] Iteration 8600, loss = 4.40684
I1209 15:00:51.840129  3198 solver.cpp:253]     Train net output #0: loss = 4.40684 (* 1 = 4.40684 loss)
I1209 15:00:51.840137  3198 sgd_solver.cpp:106] Iteration 8600, lr = 0.01
I1209 15:00:59.549548  3198 solver.cpp:237] Iteration 8620, loss = 4.04175
I1209 15:00:59.549585  3198 solver.cpp:253]     Train net output #0: loss = 4.04175 (* 1 = 4.04175 loss)
I1209 15:00:59.549592  3198 sgd_solver.cpp:106] Iteration 8620, lr = 0.01
I1209 15:01:07.209842  3198 solver.cpp:237] Iteration 8640, loss = 4.32693
I1209 15:01:07.209877  3198 solver.cpp:253]     Train net output #0: loss = 4.32693 (* 1 = 4.32693 loss)
I1209 15:01:07.209883  3198 sgd_solver.cpp:106] Iteration 8640, lr = 0.01
I1209 15:01:14.825105  3198 solver.cpp:237] Iteration 8660, loss = 4.26398
I1209 15:01:14.825287  3198 solver.cpp:253]     Train net output #0: loss = 4.26398 (* 1 = 4.26398 loss)
I1209 15:01:14.825295  3198 sgd_solver.cpp:106] Iteration 8660, lr = 0.01
I1209 15:01:22.476773  3198 solver.cpp:237] Iteration 8680, loss = 4.29377
I1209 15:01:22.476809  3198 solver.cpp:253]     Train net output #0: loss = 4.29377 (* 1 = 4.29377 loss)
I1209 15:01:22.476814  3198 sgd_solver.cpp:106] Iteration 8680, lr = 0.01
I1209 15:01:30.137256  3198 solver.cpp:237] Iteration 8700, loss = 4.36982
I1209 15:01:30.137295  3198 solver.cpp:253]     Train net output #0: loss = 4.36982 (* 1 = 4.36982 loss)
I1209 15:01:30.137301  3198 sgd_solver.cpp:106] Iteration 8700, lr = 0.01
I1209 15:01:37.839712  3198 solver.cpp:237] Iteration 8720, loss = 4.45516
I1209 15:01:37.839747  3198 solver.cpp:253]     Train net output #0: loss = 4.45516 (* 1 = 4.45516 loss)
I1209 15:01:37.839756  3198 sgd_solver.cpp:106] Iteration 8720, lr = 0.01
I1209 15:01:45.514863  3198 solver.cpp:237] Iteration 8740, loss = 4.08122
I1209 15:01:45.514988  3198 solver.cpp:253]     Train net output #0: loss = 4.08122 (* 1 = 4.08122 loss)
I1209 15:01:45.515005  3198 sgd_solver.cpp:106] Iteration 8740, lr = 0.01
I1209 15:01:53.147902  3198 solver.cpp:237] Iteration 8760, loss = 4.14496
I1209 15:01:53.147936  3198 solver.cpp:253]     Train net output #0: loss = 4.14496 (* 1 = 4.14496 loss)
I1209 15:01:53.147943  3198 sgd_solver.cpp:106] Iteration 8760, lr = 0.01
I1209 15:02:00.796092  3198 solver.cpp:237] Iteration 8780, loss = 4.06428
I1209 15:02:00.796129  3198 solver.cpp:253]     Train net output #0: loss = 4.06428 (* 1 = 4.06428 loss)
I1209 15:02:00.796135  3198 sgd_solver.cpp:106] Iteration 8780, lr = 0.01
I1209 15:02:08.417676  3198 solver.cpp:237] Iteration 8800, loss = 4.29074
I1209 15:02:08.417716  3198 solver.cpp:253]     Train net output #0: loss = 4.29074 (* 1 = 4.29074 loss)
I1209 15:02:08.417731  3198 sgd_solver.cpp:106] Iteration 8800, lr = 0.01
I1209 15:02:16.047402  3198 solver.cpp:237] Iteration 8820, loss = 4.34814
I1209 15:02:16.047571  3198 solver.cpp:253]     Train net output #0: loss = 4.34814 (* 1 = 4.34814 loss)
I1209 15:02:16.047580  3198 sgd_solver.cpp:106] Iteration 8820, lr = 0.01
I1209 15:02:23.672274  3198 solver.cpp:237] Iteration 8840, loss = 4.28967
I1209 15:02:23.672308  3198 solver.cpp:253]     Train net output #0: loss = 4.28967 (* 1 = 4.28967 loss)
I1209 15:02:23.672314  3198 sgd_solver.cpp:106] Iteration 8840, lr = 0.01
I1209 15:02:31.333673  3198 solver.cpp:237] Iteration 8860, loss = 4.2555
I1209 15:02:31.333708  3198 solver.cpp:253]     Train net output #0: loss = 4.2555 (* 1 = 4.2555 loss)
I1209 15:02:31.333714  3198 sgd_solver.cpp:106] Iteration 8860, lr = 0.01
I1209 15:02:39.007254  3198 solver.cpp:237] Iteration 8880, loss = 4.3581
I1209 15:02:39.007290  3198 solver.cpp:253]     Train net output #0: loss = 4.3581 (* 1 = 4.3581 loss)
I1209 15:02:39.007297  3198 sgd_solver.cpp:106] Iteration 8880, lr = 0.01
I1209 15:02:46.639194  3198 solver.cpp:237] Iteration 8900, loss = 4.15717
I1209 15:02:46.639370  3198 solver.cpp:253]     Train net output #0: loss = 4.15717 (* 1 = 4.15717 loss)
I1209 15:02:46.639379  3198 sgd_solver.cpp:106] Iteration 8900, lr = 0.01
I1209 15:02:54.298135  3198 solver.cpp:237] Iteration 8920, loss = 4.31425
I1209 15:02:54.298171  3198 solver.cpp:253]     Train net output #0: loss = 4.31425 (* 1 = 4.31425 loss)
I1209 15:02:54.298177  3198 sgd_solver.cpp:106] Iteration 8920, lr = 0.01
I1209 15:03:01.903015  3198 solver.cpp:237] Iteration 8940, loss = 4.05947
I1209 15:03:01.903051  3198 solver.cpp:253]     Train net output #0: loss = 4.05947 (* 1 = 4.05947 loss)
I1209 15:03:01.903058  3198 sgd_solver.cpp:106] Iteration 8940, lr = 0.01
I1209 15:03:09.559576  3198 solver.cpp:237] Iteration 8960, loss = 4.3782
I1209 15:03:09.559613  3198 solver.cpp:253]     Train net output #0: loss = 4.3782 (* 1 = 4.3782 loss)
I1209 15:03:09.559619  3198 sgd_solver.cpp:106] Iteration 8960, lr = 0.01
I1209 15:03:17.205355  3198 solver.cpp:237] Iteration 8980, loss = 4.21868
I1209 15:03:17.205509  3198 solver.cpp:253]     Train net output #0: loss = 4.21868 (* 1 = 4.21868 loss)
I1209 15:03:17.205526  3198 sgd_solver.cpp:106] Iteration 8980, lr = 0.01
I1209 15:03:24.465019  3198 solver.cpp:341] Iteration 9000, Testing net (#0)
I1209 15:03:30.047199  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:04:42.448379  3198 solver.cpp:409]     Test net output #0: accuracy = 0.18452
I1209 15:04:42.448498  3198 solver.cpp:409]     Test net output #1: loss = 4.20439 (* 1 = 4.20439 loss)
I1209 15:04:42.645725  3198 solver.cpp:237] Iteration 9000, loss = 4.03105
I1209 15:04:42.645790  3198 solver.cpp:253]     Train net output #0: loss = 4.03105 (* 1 = 4.03105 loss)
I1209 15:04:42.645809  3198 sgd_solver.cpp:106] Iteration 9000, lr = 0.01
I1209 15:04:49.637261  3198 solver.cpp:237] Iteration 9020, loss = 4.09694
I1209 15:04:49.637298  3198 solver.cpp:253]     Train net output #0: loss = 4.09694 (* 1 = 4.09694 loss)
I1209 15:04:49.637303  3198 sgd_solver.cpp:106] Iteration 9020, lr = 0.01
I1209 15:04:57.321574  3198 solver.cpp:237] Iteration 9040, loss = 4.2076
I1209 15:04:57.321612  3198 solver.cpp:253]     Train net output #0: loss = 4.2076 (* 1 = 4.2076 loss)
I1209 15:04:57.321619  3198 sgd_solver.cpp:106] Iteration 9040, lr = 0.01
I1209 15:05:04.998281  3198 solver.cpp:237] Iteration 9060, loss = 4.20908
I1209 15:05:04.998317  3198 solver.cpp:253]     Train net output #0: loss = 4.20908 (* 1 = 4.20908 loss)
I1209 15:05:04.998323  3198 sgd_solver.cpp:106] Iteration 9060, lr = 0.01
I1209 15:05:12.263545  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:05:12.621250  3198 solver.cpp:237] Iteration 9080, loss = 4.22014
I1209 15:05:12.621382  3198 solver.cpp:253]     Train net output #0: loss = 4.22014 (* 1 = 4.22014 loss)
I1209 15:05:12.621397  3198 sgd_solver.cpp:106] Iteration 9080, lr = 0.01
I1209 15:05:20.295828  3198 solver.cpp:237] Iteration 9100, loss = 4.35623
I1209 15:05:20.295863  3198 solver.cpp:253]     Train net output #0: loss = 4.35623 (* 1 = 4.35623 loss)
I1209 15:05:20.295869  3198 sgd_solver.cpp:106] Iteration 9100, lr = 0.01
I1209 15:05:27.968032  3198 solver.cpp:237] Iteration 9120, loss = 4.17128
I1209 15:05:27.968068  3198 solver.cpp:253]     Train net output #0: loss = 4.17128 (* 1 = 4.17128 loss)
I1209 15:05:27.968075  3198 sgd_solver.cpp:106] Iteration 9120, lr = 0.01
I1209 15:05:35.614835  3198 solver.cpp:237] Iteration 9140, loss = 4.12046
I1209 15:05:35.614869  3198 solver.cpp:253]     Train net output #0: loss = 4.12046 (* 1 = 4.12046 loss)
I1209 15:05:35.614876  3198 sgd_solver.cpp:106] Iteration 9140, lr = 0.01
I1209 15:05:43.254221  3198 solver.cpp:237] Iteration 9160, loss = 4.35009
I1209 15:05:43.254361  3198 solver.cpp:253]     Train net output #0: loss = 4.35009 (* 1 = 4.35009 loss)
I1209 15:05:43.254369  3198 sgd_solver.cpp:106] Iteration 9160, lr = 0.01
I1209 15:05:50.930773  3198 solver.cpp:237] Iteration 9180, loss = 4.54777
I1209 15:05:50.930809  3198 solver.cpp:253]     Train net output #0: loss = 4.54777 (* 1 = 4.54777 loss)
I1209 15:05:50.930815  3198 sgd_solver.cpp:106] Iteration 9180, lr = 0.01
I1209 15:05:58.612929  3198 solver.cpp:237] Iteration 9200, loss = 3.97646
I1209 15:05:58.612969  3198 solver.cpp:253]     Train net output #0: loss = 3.97646 (* 1 = 3.97646 loss)
I1209 15:05:58.612975  3198 sgd_solver.cpp:106] Iteration 9200, lr = 0.01
I1209 15:06:06.185222  3198 solver.cpp:237] Iteration 9220, loss = 4.22317
I1209 15:06:06.185262  3198 solver.cpp:253]     Train net output #0: loss = 4.22317 (* 1 = 4.22317 loss)
I1209 15:06:06.185269  3198 sgd_solver.cpp:106] Iteration 9220, lr = 0.01
I1209 15:06:13.847856  3198 solver.cpp:237] Iteration 9240, loss = 4.59499
I1209 15:06:13.847975  3198 solver.cpp:253]     Train net output #0: loss = 4.59499 (* 1 = 4.59499 loss)
I1209 15:06:13.847980  3198 sgd_solver.cpp:106] Iteration 9240, lr = 0.01
I1209 15:06:21.519525  3198 solver.cpp:237] Iteration 9260, loss = 4.35446
I1209 15:06:21.519561  3198 solver.cpp:253]     Train net output #0: loss = 4.35446 (* 1 = 4.35446 loss)
I1209 15:06:21.519567  3198 sgd_solver.cpp:106] Iteration 9260, lr = 0.01
I1209 15:06:29.170835  3198 solver.cpp:237] Iteration 9280, loss = 4.19775
I1209 15:06:29.170869  3198 solver.cpp:253]     Train net output #0: loss = 4.19775 (* 1 = 4.19775 loss)
I1209 15:06:29.170876  3198 sgd_solver.cpp:106] Iteration 9280, lr = 0.01
I1209 15:06:36.821555  3198 solver.cpp:237] Iteration 9300, loss = 4.22782
I1209 15:06:36.821593  3198 solver.cpp:253]     Train net output #0: loss = 4.22782 (* 1 = 4.22782 loss)
I1209 15:06:36.821598  3198 sgd_solver.cpp:106] Iteration 9300, lr = 0.01
I1209 15:06:44.492915  3198 solver.cpp:237] Iteration 9320, loss = 4.26201
I1209 15:06:44.493024  3198 solver.cpp:253]     Train net output #0: loss = 4.26201 (* 1 = 4.26201 loss)
I1209 15:06:44.493032  3198 sgd_solver.cpp:106] Iteration 9320, lr = 0.01
I1209 15:06:52.063998  3198 solver.cpp:237] Iteration 9340, loss = 4.2534
I1209 15:06:52.064035  3198 solver.cpp:253]     Train net output #0: loss = 4.2534 (* 1 = 4.2534 loss)
I1209 15:06:52.064043  3198 sgd_solver.cpp:106] Iteration 9340, lr = 0.01
I1209 15:06:59.646683  3198 solver.cpp:237] Iteration 9360, loss = 4.22984
I1209 15:06:59.646718  3198 solver.cpp:253]     Train net output #0: loss = 4.22984 (* 1 = 4.22984 loss)
I1209 15:06:59.646724  3198 sgd_solver.cpp:106] Iteration 9360, lr = 0.01
I1209 15:07:07.283432  3198 solver.cpp:237] Iteration 9380, loss = 4.34629
I1209 15:07:07.283473  3198 solver.cpp:253]     Train net output #0: loss = 4.34629 (* 1 = 4.34629 loss)
I1209 15:07:07.283481  3198 sgd_solver.cpp:106] Iteration 9380, lr = 0.01
I1209 15:07:14.945883  3198 solver.cpp:237] Iteration 9400, loss = 4.47856
I1209 15:07:14.948236  3198 solver.cpp:253]     Train net output #0: loss = 4.47856 (* 1 = 4.47856 loss)
I1209 15:07:14.948243  3198 sgd_solver.cpp:106] Iteration 9400, lr = 0.01
I1209 15:07:22.581352  3198 solver.cpp:237] Iteration 9420, loss = 4.34203
I1209 15:07:22.581389  3198 solver.cpp:253]     Train net output #0: loss = 4.34203 (* 1 = 4.34203 loss)
I1209 15:07:22.581395  3198 sgd_solver.cpp:106] Iteration 9420, lr = 0.01
I1209 15:07:30.227663  3198 solver.cpp:237] Iteration 9440, loss = 4.24572
I1209 15:07:30.227699  3198 solver.cpp:253]     Train net output #0: loss = 4.24572 (* 1 = 4.24572 loss)
I1209 15:07:30.227705  3198 sgd_solver.cpp:106] Iteration 9440, lr = 0.01
I1209 15:07:37.826314  3198 solver.cpp:237] Iteration 9460, loss = 4.19524
I1209 15:07:37.826350  3198 solver.cpp:253]     Train net output #0: loss = 4.19524 (* 1 = 4.19524 loss)
I1209 15:07:37.826356  3198 sgd_solver.cpp:106] Iteration 9460, lr = 0.01
I1209 15:07:45.499554  3198 solver.cpp:237] Iteration 9480, loss = 4.15491
I1209 15:07:45.499725  3198 solver.cpp:253]     Train net output #0: loss = 4.15491 (* 1 = 4.15491 loss)
I1209 15:07:45.499732  3198 sgd_solver.cpp:106] Iteration 9480, lr = 0.01
I1209 15:07:53.119276  3198 solver.cpp:237] Iteration 9500, loss = 4.16911
I1209 15:07:53.119313  3198 solver.cpp:253]     Train net output #0: loss = 4.16911 (* 1 = 4.16911 loss)
I1209 15:07:53.119328  3198 sgd_solver.cpp:106] Iteration 9500, lr = 0.01
I1209 15:08:00.760607  3198 solver.cpp:237] Iteration 9520, loss = 4.33471
I1209 15:08:00.760642  3198 solver.cpp:253]     Train net output #0: loss = 4.33471 (* 1 = 4.33471 loss)
I1209 15:08:00.760648  3198 sgd_solver.cpp:106] Iteration 9520, lr = 0.01
I1209 15:08:08.386399  3198 solver.cpp:237] Iteration 9540, loss = 4.19835
I1209 15:08:08.386436  3198 solver.cpp:253]     Train net output #0: loss = 4.19835 (* 1 = 4.19835 loss)
I1209 15:08:08.386442  3198 sgd_solver.cpp:106] Iteration 9540, lr = 0.01
I1209 15:08:16.011615  3198 solver.cpp:237] Iteration 9560, loss = 4.15003
I1209 15:08:16.011749  3198 solver.cpp:253]     Train net output #0: loss = 4.15003 (* 1 = 4.15003 loss)
I1209 15:08:16.011765  3198 sgd_solver.cpp:106] Iteration 9560, lr = 0.01
I1209 15:08:23.622406  3198 solver.cpp:237] Iteration 9580, loss = 4.26745
I1209 15:08:23.622464  3198 solver.cpp:253]     Train net output #0: loss = 4.26745 (* 1 = 4.26745 loss)
I1209 15:08:23.622473  3198 sgd_solver.cpp:106] Iteration 9580, lr = 0.01
I1209 15:08:31.251926  3198 solver.cpp:237] Iteration 9600, loss = 4.34116
I1209 15:08:31.251963  3198 solver.cpp:253]     Train net output #0: loss = 4.34116 (* 1 = 4.34116 loss)
I1209 15:08:31.251971  3198 sgd_solver.cpp:106] Iteration 9600, lr = 0.01
I1209 15:08:38.932049  3198 solver.cpp:237] Iteration 9620, loss = 4.03732
I1209 15:08:38.932085  3198 solver.cpp:253]     Train net output #0: loss = 4.03732 (* 1 = 4.03732 loss)
I1209 15:08:38.932091  3198 sgd_solver.cpp:106] Iteration 9620, lr = 0.01
I1209 15:08:46.852371  3198 solver.cpp:237] Iteration 9640, loss = 4.44029
I1209 15:08:46.852540  3198 solver.cpp:253]     Train net output #0: loss = 4.44029 (* 1 = 4.44029 loss)
I1209 15:08:46.852547  3198 sgd_solver.cpp:106] Iteration 9640, lr = 0.01
I1209 15:08:54.834189  3198 solver.cpp:237] Iteration 9660, loss = 4.35034
I1209 15:08:54.834226  3198 solver.cpp:253]     Train net output #0: loss = 4.35034 (* 1 = 4.35034 loss)
I1209 15:08:54.834233  3198 sgd_solver.cpp:106] Iteration 9660, lr = 0.01
I1209 15:09:02.724210  3198 solver.cpp:237] Iteration 9680, loss = 4.24585
I1209 15:09:02.724244  3198 solver.cpp:253]     Train net output #0: loss = 4.24585 (* 1 = 4.24585 loss)
I1209 15:09:02.724251  3198 sgd_solver.cpp:106] Iteration 9680, lr = 0.01
I1209 15:09:10.391960  3198 solver.cpp:237] Iteration 9700, loss = 4.15147
I1209 15:09:10.391995  3198 solver.cpp:253]     Train net output #0: loss = 4.15147 (* 1 = 4.15147 loss)
I1209 15:09:10.392002  3198 sgd_solver.cpp:106] Iteration 9700, lr = 0.01
I1209 15:09:18.029495  3198 solver.cpp:237] Iteration 9720, loss = 4.02702
I1209 15:09:18.029659  3198 solver.cpp:253]     Train net output #0: loss = 4.02702 (* 1 = 4.02702 loss)
I1209 15:09:18.029666  3198 sgd_solver.cpp:106] Iteration 9720, lr = 0.01
I1209 15:09:25.690767  3198 solver.cpp:237] Iteration 9740, loss = 4.30249
I1209 15:09:25.690804  3198 solver.cpp:253]     Train net output #0: loss = 4.30249 (* 1 = 4.30249 loss)
I1209 15:09:25.690810  3198 sgd_solver.cpp:106] Iteration 9740, lr = 0.01
I1209 15:09:33.371057  3198 solver.cpp:237] Iteration 9760, loss = 4.2751
I1209 15:09:33.371096  3198 solver.cpp:253]     Train net output #0: loss = 4.2751 (* 1 = 4.2751 loss)
I1209 15:09:33.371103  3198 sgd_solver.cpp:106] Iteration 9760, lr = 0.01
I1209 15:09:40.992605  3198 solver.cpp:237] Iteration 9780, loss = 4.22212
I1209 15:09:40.992642  3198 solver.cpp:253]     Train net output #0: loss = 4.22212 (* 1 = 4.22212 loss)
I1209 15:09:40.992650  3198 sgd_solver.cpp:106] Iteration 9780, lr = 0.01
I1209 15:09:48.656379  3198 solver.cpp:237] Iteration 9800, loss = 4.23162
I1209 15:09:48.656524  3198 solver.cpp:253]     Train net output #0: loss = 4.23162 (* 1 = 4.23162 loss)
I1209 15:09:48.656533  3198 sgd_solver.cpp:106] Iteration 9800, lr = 0.01
I1209 15:09:56.319505  3198 solver.cpp:237] Iteration 9820, loss = 4.13722
I1209 15:09:56.319541  3198 solver.cpp:253]     Train net output #0: loss = 4.13722 (* 1 = 4.13722 loss)
I1209 15:09:56.319547  3198 sgd_solver.cpp:106] Iteration 9820, lr = 0.01
I1209 15:10:03.940359  3198 solver.cpp:237] Iteration 9840, loss = 4.23263
I1209 15:10:03.940395  3198 solver.cpp:253]     Train net output #0: loss = 4.23263 (* 1 = 4.23263 loss)
I1209 15:10:03.940402  3198 sgd_solver.cpp:106] Iteration 9840, lr = 0.01
I1209 15:10:11.632876  3198 solver.cpp:237] Iteration 9860, loss = 4.00323
I1209 15:10:11.632913  3198 solver.cpp:253]     Train net output #0: loss = 4.00323 (* 1 = 4.00323 loss)
I1209 15:10:11.632918  3198 sgd_solver.cpp:106] Iteration 9860, lr = 0.01
I1209 15:10:19.325942  3198 solver.cpp:237] Iteration 9880, loss = 4.43451
I1209 15:10:19.326098  3198 solver.cpp:253]     Train net output #0: loss = 4.43451 (* 1 = 4.43451 loss)
I1209 15:10:19.326105  3198 sgd_solver.cpp:106] Iteration 9880, lr = 0.01
I1209 15:10:26.980495  3198 solver.cpp:237] Iteration 9900, loss = 4.01884
I1209 15:10:26.980533  3198 solver.cpp:253]     Train net output #0: loss = 4.01884 (* 1 = 4.01884 loss)
I1209 15:10:26.980538  3198 sgd_solver.cpp:106] Iteration 9900, lr = 0.01
I1209 15:10:34.664508  3198 solver.cpp:237] Iteration 9920, loss = 4.1847
I1209 15:10:34.664546  3198 solver.cpp:253]     Train net output #0: loss = 4.1847 (* 1 = 4.1847 loss)
I1209 15:10:34.664552  3198 sgd_solver.cpp:106] Iteration 9920, lr = 0.01
I1209 15:10:42.376030  3198 solver.cpp:237] Iteration 9940, loss = 4.06052
I1209 15:10:42.376066  3198 solver.cpp:253]     Train net output #0: loss = 4.06052 (* 1 = 4.06052 loss)
I1209 15:10:42.376072  3198 sgd_solver.cpp:106] Iteration 9940, lr = 0.01
I1209 15:10:50.031769  3198 solver.cpp:237] Iteration 9960, loss = 4.15186
I1209 15:10:50.031942  3198 solver.cpp:253]     Train net output #0: loss = 4.15186 (* 1 = 4.15186 loss)
I1209 15:10:50.031960  3198 sgd_solver.cpp:106] Iteration 9960, lr = 0.01
I1209 15:10:57.726842  3198 solver.cpp:237] Iteration 9980, loss = 3.89492
I1209 15:10:57.726881  3198 solver.cpp:253]     Train net output #0: loss = 3.89492 (* 1 = 3.89492 loss)
I1209 15:10:57.726887  3198 sgd_solver.cpp:106] Iteration 9980, lr = 0.01
I1209 15:11:05.005873  3198 solver.cpp:459] Snapshotting to binary proto file snapshots/caffenet128_no_lrn_lsuv_bn_before_ea_iter_10000.caffemodel
I1209 15:11:05.240311  3198 sgd_solver.cpp:269] Snapshotting solver state to binary proto file snapshots/caffenet128_no_lrn_lsuv_bn_before_ea_iter_10000.solverstate
I1209 15:11:05.301251  3198 solver.cpp:341] Iteration 10000, Testing net (#0)
I1209 15:11:11.817631  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:12:22.349390  3198 solver.cpp:409]     Test net output #0: accuracy = 0.17914
I1209 15:12:22.349555  3198 solver.cpp:409]     Test net output #1: loss = 4.24708 (* 1 = 4.24708 loss)
I1209 15:12:22.545395  3198 solver.cpp:237] Iteration 10000, loss = 4.21049
I1209 15:12:22.545433  3198 solver.cpp:253]     Train net output #0: loss = 4.21049 (* 1 = 4.21049 loss)
I1209 15:12:22.545439  3198 sgd_solver.cpp:106] Iteration 10000, lr = 0.01
I1209 15:12:29.503164  3198 solver.cpp:237] Iteration 10020, loss = 4.2132
I1209 15:12:29.503202  3198 solver.cpp:253]     Train net output #0: loss = 4.2132 (* 1 = 4.2132 loss)
I1209 15:12:29.503209  3198 sgd_solver.cpp:106] Iteration 10020, lr = 0.01
I1209 15:12:37.087959  3198 solver.cpp:237] Iteration 10040, loss = 4.08746
I1209 15:12:37.087993  3198 solver.cpp:253]     Train net output #0: loss = 4.08746 (* 1 = 4.08746 loss)
I1209 15:12:37.087998  3198 sgd_solver.cpp:106] Iteration 10040, lr = 0.01
I1209 15:12:44.723227  3198 solver.cpp:237] Iteration 10060, loss = 4.27287
I1209 15:12:44.723292  3198 solver.cpp:253]     Train net output #0: loss = 4.27287 (* 1 = 4.27287 loss)
I1209 15:12:44.723309  3198 sgd_solver.cpp:106] Iteration 10060, lr = 0.01
I1209 15:12:52.394690  3198 solver.cpp:237] Iteration 10080, loss = 4.27232
I1209 15:12:52.394843  3198 solver.cpp:253]     Train net output #0: loss = 4.27232 (* 1 = 4.27232 loss)
I1209 15:12:52.394850  3198 sgd_solver.cpp:106] Iteration 10080, lr = 0.01
I1209 15:12:55.111531  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:13:00.088343  3198 solver.cpp:237] Iteration 10100, loss = 4.17147
I1209 15:13:00.088381  3198 solver.cpp:253]     Train net output #0: loss = 4.17147 (* 1 = 4.17147 loss)
I1209 15:13:00.088387  3198 sgd_solver.cpp:106] Iteration 10100, lr = 0.01
I1209 15:13:07.721001  3198 solver.cpp:237] Iteration 10120, loss = 4.32579
I1209 15:13:07.721038  3198 solver.cpp:253]     Train net output #0: loss = 4.32579 (* 1 = 4.32579 loss)
I1209 15:13:07.721045  3198 sgd_solver.cpp:106] Iteration 10120, lr = 0.01
I1209 15:13:15.374817  3198 solver.cpp:237] Iteration 10140, loss = 4.17093
I1209 15:13:15.374855  3198 solver.cpp:253]     Train net output #0: loss = 4.17093 (* 1 = 4.17093 loss)
I1209 15:13:15.374862  3198 sgd_solver.cpp:106] Iteration 10140, lr = 0.01
I1209 15:13:23.024197  3198 solver.cpp:237] Iteration 10160, loss = 3.94229
I1209 15:13:23.024348  3198 solver.cpp:253]     Train net output #0: loss = 3.94229 (* 1 = 3.94229 loss)
I1209 15:13:23.024355  3198 sgd_solver.cpp:106] Iteration 10160, lr = 0.01
I1209 15:13:30.669386  3198 solver.cpp:237] Iteration 10180, loss = 4.35794
I1209 15:13:30.669423  3198 solver.cpp:253]     Train net output #0: loss = 4.35794 (* 1 = 4.35794 loss)
I1209 15:13:30.669428  3198 sgd_solver.cpp:106] Iteration 10180, lr = 0.01
I1209 15:13:38.353323  3198 solver.cpp:237] Iteration 10200, loss = 3.94805
I1209 15:13:38.353365  3198 solver.cpp:253]     Train net output #0: loss = 3.94805 (* 1 = 3.94805 loss)
I1209 15:13:38.353373  3198 sgd_solver.cpp:106] Iteration 10200, lr = 0.01
I1209 15:13:46.109261  3198 solver.cpp:237] Iteration 10220, loss = 4.1854
I1209 15:13:46.109298  3198 solver.cpp:253]     Train net output #0: loss = 4.1854 (* 1 = 4.1854 loss)
I1209 15:13:46.109303  3198 sgd_solver.cpp:106] Iteration 10220, lr = 0.01
I1209 15:13:54.005544  3198 solver.cpp:237] Iteration 10240, loss = 4.28595
I1209 15:13:54.005661  3198 solver.cpp:253]     Train net output #0: loss = 4.28595 (* 1 = 4.28595 loss)
I1209 15:13:54.005668  3198 sgd_solver.cpp:106] Iteration 10240, lr = 0.01
I1209 15:14:01.877254  3198 solver.cpp:237] Iteration 10260, loss = 4.30787
I1209 15:14:01.877293  3198 solver.cpp:253]     Train net output #0: loss = 4.30787 (* 1 = 4.30787 loss)
I1209 15:14:01.877300  3198 sgd_solver.cpp:106] Iteration 10260, lr = 0.01
I1209 15:14:09.818497  3198 solver.cpp:237] Iteration 10280, loss = 4.14998
I1209 15:14:09.818533  3198 solver.cpp:253]     Train net output #0: loss = 4.14998 (* 1 = 4.14998 loss)
I1209 15:14:09.818541  3198 sgd_solver.cpp:106] Iteration 10280, lr = 0.01
I1209 15:14:17.710562  3198 solver.cpp:237] Iteration 10300, loss = 4.23631
I1209 15:14:17.710604  3198 solver.cpp:253]     Train net output #0: loss = 4.23631 (* 1 = 4.23631 loss)
I1209 15:14:17.710610  3198 sgd_solver.cpp:106] Iteration 10300, lr = 0.01
I1209 15:14:25.344404  3198 solver.cpp:237] Iteration 10320, loss = 3.94974
I1209 15:14:25.344574  3198 solver.cpp:253]     Train net output #0: loss = 3.94974 (* 1 = 3.94974 loss)
I1209 15:14:25.344581  3198 sgd_solver.cpp:106] Iteration 10320, lr = 0.01
I1209 15:14:33.031747  3198 solver.cpp:237] Iteration 10340, loss = 4.09387
I1209 15:14:33.031782  3198 solver.cpp:253]     Train net output #0: loss = 4.09387 (* 1 = 4.09387 loss)
I1209 15:14:33.031790  3198 sgd_solver.cpp:106] Iteration 10340, lr = 0.01
I1209 15:14:40.684806  3198 solver.cpp:237] Iteration 10360, loss = 3.98003
I1209 15:14:40.684842  3198 solver.cpp:253]     Train net output #0: loss = 3.98003 (* 1 = 3.98003 loss)
I1209 15:14:40.684849  3198 sgd_solver.cpp:106] Iteration 10360, lr = 0.01
I1209 15:14:48.376261  3198 solver.cpp:237] Iteration 10380, loss = 4.13005
I1209 15:14:48.376297  3198 solver.cpp:253]     Train net output #0: loss = 4.13005 (* 1 = 4.13005 loss)
I1209 15:14:48.376303  3198 sgd_solver.cpp:106] Iteration 10380, lr = 0.01
I1209 15:14:56.030827  3198 solver.cpp:237] Iteration 10400, loss = 4.13575
I1209 15:14:56.030944  3198 solver.cpp:253]     Train net output #0: loss = 4.13575 (* 1 = 4.13575 loss)
I1209 15:14:56.030951  3198 sgd_solver.cpp:106] Iteration 10400, lr = 0.01
I1209 15:15:03.678653  3198 solver.cpp:237] Iteration 10420, loss = 4.05348
I1209 15:15:03.678688  3198 solver.cpp:253]     Train net output #0: loss = 4.05348 (* 1 = 4.05348 loss)
I1209 15:15:03.678694  3198 sgd_solver.cpp:106] Iteration 10420, lr = 0.01
I1209 15:15:11.331087  3198 solver.cpp:237] Iteration 10440, loss = 4.24265
I1209 15:15:11.331125  3198 solver.cpp:253]     Train net output #0: loss = 4.24265 (* 1 = 4.24265 loss)
I1209 15:15:11.331140  3198 sgd_solver.cpp:106] Iteration 10440, lr = 0.01
I1209 15:15:18.970824  3198 solver.cpp:237] Iteration 10460, loss = 4.12158
I1209 15:15:18.970859  3198 solver.cpp:253]     Train net output #0: loss = 4.12158 (* 1 = 4.12158 loss)
I1209 15:15:18.970863  3198 sgd_solver.cpp:106] Iteration 10460, lr = 0.01
I1209 15:15:26.590423  3198 solver.cpp:237] Iteration 10480, loss = 3.93002
I1209 15:15:26.590544  3198 solver.cpp:253]     Train net output #0: loss = 3.93002 (* 1 = 3.93002 loss)
I1209 15:15:26.590560  3198 sgd_solver.cpp:106] Iteration 10480, lr = 0.01
I1209 15:15:34.238651  3198 solver.cpp:237] Iteration 10500, loss = 4.20497
I1209 15:15:34.238688  3198 solver.cpp:253]     Train net output #0: loss = 4.20497 (* 1 = 4.20497 loss)
I1209 15:15:34.238694  3198 sgd_solver.cpp:106] Iteration 10500, lr = 0.01
I1209 15:15:41.871518  3198 solver.cpp:237] Iteration 10520, loss = 4.20725
I1209 15:15:41.871554  3198 solver.cpp:253]     Train net output #0: loss = 4.20725 (* 1 = 4.20725 loss)
I1209 15:15:41.871561  3198 sgd_solver.cpp:106] Iteration 10520, lr = 0.01
I1209 15:15:49.507031  3198 solver.cpp:237] Iteration 10540, loss = 4.12697
I1209 15:15:49.507066  3198 solver.cpp:253]     Train net output #0: loss = 4.12697 (* 1 = 4.12697 loss)
I1209 15:15:49.507073  3198 sgd_solver.cpp:106] Iteration 10540, lr = 0.01
I1209 15:15:57.157493  3198 solver.cpp:237] Iteration 10560, loss = 4.04761
I1209 15:15:57.157663  3198 solver.cpp:253]     Train net output #0: loss = 4.04761 (* 1 = 4.04761 loss)
I1209 15:15:57.157670  3198 sgd_solver.cpp:106] Iteration 10560, lr = 0.01
I1209 15:16:04.839221  3198 solver.cpp:237] Iteration 10580, loss = 4.06569
I1209 15:16:04.839258  3198 solver.cpp:253]     Train net output #0: loss = 4.06569 (* 1 = 4.06569 loss)
I1209 15:16:04.839265  3198 sgd_solver.cpp:106] Iteration 10580, lr = 0.01
I1209 15:16:12.498142  3198 solver.cpp:237] Iteration 10600, loss = 3.99974
I1209 15:16:12.498179  3198 solver.cpp:253]     Train net output #0: loss = 3.99974 (* 1 = 3.99974 loss)
I1209 15:16:12.498186  3198 sgd_solver.cpp:106] Iteration 10600, lr = 0.01
I1209 15:16:20.192188  3198 solver.cpp:237] Iteration 10620, loss = 4.0825
I1209 15:16:20.192224  3198 solver.cpp:253]     Train net output #0: loss = 4.0825 (* 1 = 4.0825 loss)
I1209 15:16:20.192230  3198 sgd_solver.cpp:106] Iteration 10620, lr = 0.01
I1209 15:16:27.812235  3198 solver.cpp:237] Iteration 10640, loss = 4.07727
I1209 15:16:27.812328  3198 solver.cpp:253]     Train net output #0: loss = 4.07727 (* 1 = 4.07727 loss)
I1209 15:16:27.812345  3198 sgd_solver.cpp:106] Iteration 10640, lr = 0.01
I1209 15:16:35.400367  3198 solver.cpp:237] Iteration 10660, loss = 4.30445
I1209 15:16:35.400403  3198 solver.cpp:253]     Train net output #0: loss = 4.30445 (* 1 = 4.30445 loss)
I1209 15:16:35.400409  3198 sgd_solver.cpp:106] Iteration 10660, lr = 0.01
I1209 15:16:43.044335  3198 solver.cpp:237] Iteration 10680, loss = 4.12595
I1209 15:16:43.044373  3198 solver.cpp:253]     Train net output #0: loss = 4.12595 (* 1 = 4.12595 loss)
I1209 15:16:43.044380  3198 sgd_solver.cpp:106] Iteration 10680, lr = 0.01
I1209 15:16:50.690975  3198 solver.cpp:237] Iteration 10700, loss = 4.26241
I1209 15:16:50.691011  3198 solver.cpp:253]     Train net output #0: loss = 4.26241 (* 1 = 4.26241 loss)
I1209 15:16:50.691017  3198 sgd_solver.cpp:106] Iteration 10700, lr = 0.01
I1209 15:16:58.376229  3198 solver.cpp:237] Iteration 10720, loss = 4.18766
I1209 15:16:58.376350  3198 solver.cpp:253]     Train net output #0: loss = 4.18766 (* 1 = 4.18766 loss)
I1209 15:16:58.376358  3198 sgd_solver.cpp:106] Iteration 10720, lr = 0.01
I1209 15:17:06.055279  3198 solver.cpp:237] Iteration 10740, loss = 4.18096
I1209 15:17:06.055317  3198 solver.cpp:253]     Train net output #0: loss = 4.18096 (* 1 = 4.18096 loss)
I1209 15:17:06.055325  3198 sgd_solver.cpp:106] Iteration 10740, lr = 0.01
I1209 15:17:13.742950  3198 solver.cpp:237] Iteration 10760, loss = 4.26503
I1209 15:17:13.742987  3198 solver.cpp:253]     Train net output #0: loss = 4.26503 (* 1 = 4.26503 loss)
I1209 15:17:13.742995  3198 sgd_solver.cpp:106] Iteration 10760, lr = 0.01
I1209 15:17:21.402029  3198 solver.cpp:237] Iteration 10780, loss = 4.36803
I1209 15:17:21.402066  3198 solver.cpp:253]     Train net output #0: loss = 4.36803 (* 1 = 4.36803 loss)
I1209 15:17:21.402072  3198 sgd_solver.cpp:106] Iteration 10780, lr = 0.01
I1209 15:17:29.012377  3198 solver.cpp:237] Iteration 10800, loss = 4.15527
I1209 15:17:29.012526  3198 solver.cpp:253]     Train net output #0: loss = 4.15527 (* 1 = 4.15527 loss)
I1209 15:17:29.012534  3198 sgd_solver.cpp:106] Iteration 10800, lr = 0.01
I1209 15:17:36.682289  3198 solver.cpp:237] Iteration 10820, loss = 3.9695
I1209 15:17:36.682322  3198 solver.cpp:253]     Train net output #0: loss = 3.9695 (* 1 = 3.9695 loss)
I1209 15:17:36.682338  3198 sgd_solver.cpp:106] Iteration 10820, lr = 0.01
I1209 15:17:44.380246  3198 solver.cpp:237] Iteration 10840, loss = 4.16565
I1209 15:17:44.380285  3198 solver.cpp:253]     Train net output #0: loss = 4.16565 (* 1 = 4.16565 loss)
I1209 15:17:44.380290  3198 sgd_solver.cpp:106] Iteration 10840, lr = 0.01
I1209 15:17:52.061264  3198 solver.cpp:237] Iteration 10860, loss = 4.13408
I1209 15:17:52.061302  3198 solver.cpp:253]     Train net output #0: loss = 4.13408 (* 1 = 4.13408 loss)
I1209 15:17:52.061307  3198 sgd_solver.cpp:106] Iteration 10860, lr = 0.01
I1209 15:17:59.720057  3198 solver.cpp:237] Iteration 10880, loss = 3.93052
I1209 15:17:59.720147  3198 solver.cpp:253]     Train net output #0: loss = 3.93052 (* 1 = 3.93052 loss)
I1209 15:17:59.720154  3198 sgd_solver.cpp:106] Iteration 10880, lr = 0.01
I1209 15:18:07.380209  3198 solver.cpp:237] Iteration 10900, loss = 4.0804
I1209 15:18:07.380244  3198 solver.cpp:253]     Train net output #0: loss = 4.0804 (* 1 = 4.0804 loss)
I1209 15:18:07.380249  3198 sgd_solver.cpp:106] Iteration 10900, lr = 0.01
I1209 15:18:15.042749  3198 solver.cpp:237] Iteration 10920, loss = 4.16443
I1209 15:18:15.042785  3198 solver.cpp:253]     Train net output #0: loss = 4.16443 (* 1 = 4.16443 loss)
I1209 15:18:15.042791  3198 sgd_solver.cpp:106] Iteration 10920, lr = 0.01
I1209 15:18:22.703256  3198 solver.cpp:237] Iteration 10940, loss = 3.95307
I1209 15:18:22.703294  3198 solver.cpp:253]     Train net output #0: loss = 3.95307 (* 1 = 3.95307 loss)
I1209 15:18:22.703301  3198 sgd_solver.cpp:106] Iteration 10940, lr = 0.01
I1209 15:18:30.428925  3198 solver.cpp:237] Iteration 10960, loss = 3.9517
I1209 15:18:30.429091  3198 solver.cpp:253]     Train net output #0: loss = 3.9517 (* 1 = 3.9517 loss)
I1209 15:18:30.429100  3198 sgd_solver.cpp:106] Iteration 10960, lr = 0.01
I1209 15:18:38.299155  3198 solver.cpp:237] Iteration 10980, loss = 4.17332
I1209 15:18:38.299190  3198 solver.cpp:253]     Train net output #0: loss = 4.17332 (* 1 = 4.17332 loss)
I1209 15:18:38.299196  3198 sgd_solver.cpp:106] Iteration 10980, lr = 0.01
I1209 15:18:45.855216  3198 solver.cpp:341] Iteration 11000, Testing net (#0)
I1209 15:18:52.816406  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:20:03.168433  3198 solver.cpp:409]     Test net output #0: accuracy = 0.18416
I1209 15:20:03.168575  3198 solver.cpp:409]     Test net output #1: loss = 4.22316 (* 1 = 4.22316 loss)
I1209 15:20:03.367359  3198 solver.cpp:237] Iteration 11000, loss = 4.15491
I1209 15:20:03.367395  3198 solver.cpp:253]     Train net output #0: loss = 4.15491 (* 1 = 4.15491 loss)
I1209 15:20:03.367403  3198 sgd_solver.cpp:106] Iteration 11000, lr = 0.01
I1209 15:20:10.375366  3198 solver.cpp:237] Iteration 11020, loss = 3.82961
I1209 15:20:10.375402  3198 solver.cpp:253]     Train net output #0: loss = 3.82961 (* 1 = 3.82961 loss)
I1209 15:20:10.375408  3198 sgd_solver.cpp:106] Iteration 11020, lr = 0.01
I1209 15:20:17.991647  3198 solver.cpp:237] Iteration 11040, loss = 4.44182
I1209 15:20:17.991690  3198 solver.cpp:253]     Train net output #0: loss = 4.44182 (* 1 = 4.44182 loss)
I1209 15:20:17.991699  3198 sgd_solver.cpp:106] Iteration 11040, lr = 0.01
I1209 15:20:25.589161  3198 solver.cpp:237] Iteration 11060, loss = 4.1246
I1209 15:20:25.589197  3198 solver.cpp:253]     Train net output #0: loss = 4.1246 (* 1 = 4.1246 loss)
I1209 15:20:25.589205  3198 sgd_solver.cpp:106] Iteration 11060, lr = 0.01
I1209 15:20:33.238788  3198 solver.cpp:237] Iteration 11080, loss = 4.13528
I1209 15:20:33.238950  3198 solver.cpp:253]     Train net output #0: loss = 4.13528 (* 1 = 4.13528 loss)
I1209 15:20:33.238977  3198 sgd_solver.cpp:106] Iteration 11080, lr = 0.01
I1209 15:20:39.008749  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:20:40.898568  3198 solver.cpp:237] Iteration 11100, loss = 4.07813
I1209 15:20:40.898604  3198 solver.cpp:253]     Train net output #0: loss = 4.07813 (* 1 = 4.07813 loss)
I1209 15:20:40.898610  3198 sgd_solver.cpp:106] Iteration 11100, lr = 0.01
I1209 15:20:48.575573  3198 solver.cpp:237] Iteration 11120, loss = 4.31353
I1209 15:20:48.575610  3198 solver.cpp:253]     Train net output #0: loss = 4.31353 (* 1 = 4.31353 loss)
I1209 15:20:48.575615  3198 sgd_solver.cpp:106] Iteration 11120, lr = 0.01
I1209 15:20:56.272889  3198 solver.cpp:237] Iteration 11140, loss = 4.07526
I1209 15:20:56.272948  3198 solver.cpp:253]     Train net output #0: loss = 4.07526 (* 1 = 4.07526 loss)
I1209 15:20:56.272953  3198 sgd_solver.cpp:106] Iteration 11140, lr = 0.01
I1209 15:21:03.918411  3198 solver.cpp:237] Iteration 11160, loss = 4.21362
I1209 15:21:03.918578  3198 solver.cpp:253]     Train net output #0: loss = 4.21362 (* 1 = 4.21362 loss)
I1209 15:21:03.918586  3198 sgd_solver.cpp:106] Iteration 11160, lr = 0.01
I1209 15:21:11.585278  3198 solver.cpp:237] Iteration 11180, loss = 4.06875
I1209 15:21:11.585314  3198 solver.cpp:253]     Train net output #0: loss = 4.06875 (* 1 = 4.06875 loss)
I1209 15:21:11.585320  3198 sgd_solver.cpp:106] Iteration 11180, lr = 0.01
I1209 15:21:19.239826  3198 solver.cpp:237] Iteration 11200, loss = 4.17275
I1209 15:21:19.239863  3198 solver.cpp:253]     Train net output #0: loss = 4.17275 (* 1 = 4.17275 loss)
I1209 15:21:19.239869  3198 sgd_solver.cpp:106] Iteration 11200, lr = 0.01
I1209 15:21:26.872719  3198 solver.cpp:237] Iteration 11220, loss = 3.92118
I1209 15:21:26.872755  3198 solver.cpp:253]     Train net output #0: loss = 3.92118 (* 1 = 3.92118 loss)
I1209 15:21:26.872761  3198 sgd_solver.cpp:106] Iteration 11220, lr = 0.01
I1209 15:21:34.546385  3198 solver.cpp:237] Iteration 11240, loss = 3.95107
I1209 15:21:34.546553  3198 solver.cpp:253]     Train net output #0: loss = 3.95107 (* 1 = 3.95107 loss)
I1209 15:21:34.546561  3198 sgd_solver.cpp:106] Iteration 11240, lr = 0.01
I1209 15:21:42.230715  3198 solver.cpp:237] Iteration 11260, loss = 3.94679
I1209 15:21:42.230751  3198 solver.cpp:253]     Train net output #0: loss = 3.94679 (* 1 = 3.94679 loss)
I1209 15:21:42.230757  3198 sgd_solver.cpp:106] Iteration 11260, lr = 0.01
I1209 15:21:49.933771  3198 solver.cpp:237] Iteration 11280, loss = 4.37057
I1209 15:21:49.933809  3198 solver.cpp:253]     Train net output #0: loss = 4.37057 (* 1 = 4.37057 loss)
I1209 15:21:49.933815  3198 sgd_solver.cpp:106] Iteration 11280, lr = 0.01
I1209 15:21:57.606273  3198 solver.cpp:237] Iteration 11300, loss = 3.89419
I1209 15:21:57.606312  3198 solver.cpp:253]     Train net output #0: loss = 3.89419 (* 1 = 3.89419 loss)
I1209 15:21:57.606317  3198 sgd_solver.cpp:106] Iteration 11300, lr = 0.01
I1209 15:22:05.270325  3198 solver.cpp:237] Iteration 11320, loss = 4.00173
I1209 15:22:05.270467  3198 solver.cpp:253]     Train net output #0: loss = 4.00173 (* 1 = 4.00173 loss)
I1209 15:22:05.270475  3198 sgd_solver.cpp:106] Iteration 11320, lr = 0.01
I1209 15:22:12.942960  3198 solver.cpp:237] Iteration 11340, loss = 3.91763
I1209 15:22:12.942994  3198 solver.cpp:253]     Train net output #0: loss = 3.91763 (* 1 = 3.91763 loss)
I1209 15:22:12.942999  3198 sgd_solver.cpp:106] Iteration 11340, lr = 0.01
I1209 15:22:20.636729  3198 solver.cpp:237] Iteration 11360, loss = 3.7983
I1209 15:22:20.636765  3198 solver.cpp:253]     Train net output #0: loss = 3.7983 (* 1 = 3.7983 loss)
I1209 15:22:20.636770  3198 sgd_solver.cpp:106] Iteration 11360, lr = 0.01
I1209 15:22:28.292462  3198 solver.cpp:237] Iteration 11380, loss = 4.04893
I1209 15:22:28.292500  3198 solver.cpp:253]     Train net output #0: loss = 4.04893 (* 1 = 4.04893 loss)
I1209 15:22:28.292505  3198 sgd_solver.cpp:106] Iteration 11380, lr = 0.01
I1209 15:22:35.957275  3198 solver.cpp:237] Iteration 11400, loss = 4.02742
I1209 15:22:35.957454  3198 solver.cpp:253]     Train net output #0: loss = 4.02742 (* 1 = 4.02742 loss)
I1209 15:22:35.957464  3198 sgd_solver.cpp:106] Iteration 11400, lr = 0.01
I1209 15:22:43.653897  3198 solver.cpp:237] Iteration 11420, loss = 4.02522
I1209 15:22:43.653930  3198 solver.cpp:253]     Train net output #0: loss = 4.02522 (* 1 = 4.02522 loss)
I1209 15:22:43.653936  3198 sgd_solver.cpp:106] Iteration 11420, lr = 0.01
I1209 15:22:51.326015  3198 solver.cpp:237] Iteration 11440, loss = 4.07573
I1209 15:22:51.326052  3198 solver.cpp:253]     Train net output #0: loss = 4.07573 (* 1 = 4.07573 loss)
I1209 15:22:51.326058  3198 sgd_solver.cpp:106] Iteration 11440, lr = 0.01
I1209 15:22:59.001602  3198 solver.cpp:237] Iteration 11460, loss = 4.10351
I1209 15:22:59.001639  3198 solver.cpp:253]     Train net output #0: loss = 4.10351 (* 1 = 4.10351 loss)
I1209 15:22:59.001646  3198 sgd_solver.cpp:106] Iteration 11460, lr = 0.01
I1209 15:23:06.593427  3198 solver.cpp:237] Iteration 11480, loss = 3.96675
I1209 15:23:06.593566  3198 solver.cpp:253]     Train net output #0: loss = 3.96675 (* 1 = 3.96675 loss)
I1209 15:23:06.593575  3198 sgd_solver.cpp:106] Iteration 11480, lr = 0.01
I1209 15:23:14.245928  3198 solver.cpp:237] Iteration 11500, loss = 4.07537
I1209 15:23:14.245961  3198 solver.cpp:253]     Train net output #0: loss = 4.07537 (* 1 = 4.07537 loss)
I1209 15:23:14.245967  3198 sgd_solver.cpp:106] Iteration 11500, lr = 0.01
I1209 15:23:21.923125  3198 solver.cpp:237] Iteration 11520, loss = 4.09439
I1209 15:23:21.923162  3198 solver.cpp:253]     Train net output #0: loss = 4.09439 (* 1 = 4.09439 loss)
I1209 15:23:21.923168  3198 sgd_solver.cpp:106] Iteration 11520, lr = 0.01
I1209 15:23:29.610896  3198 solver.cpp:237] Iteration 11540, loss = 4.25466
I1209 15:23:29.610929  3198 solver.cpp:253]     Train net output #0: loss = 4.25466 (* 1 = 4.25466 loss)
I1209 15:23:29.610935  3198 sgd_solver.cpp:106] Iteration 11540, lr = 0.01
I1209 15:23:37.478343  3198 solver.cpp:237] Iteration 11560, loss = 4.10795
I1209 15:23:37.478497  3198 solver.cpp:253]     Train net output #0: loss = 4.10795 (* 1 = 4.10795 loss)
I1209 15:23:37.478503  3198 sgd_solver.cpp:106] Iteration 11560, lr = 0.01
I1209 15:23:45.397094  3198 solver.cpp:237] Iteration 11580, loss = 4.16431
I1209 15:23:45.397131  3198 solver.cpp:253]     Train net output #0: loss = 4.16431 (* 1 = 4.16431 loss)
I1209 15:23:45.397137  3198 sgd_solver.cpp:106] Iteration 11580, lr = 0.01
I1209 15:23:53.287636  3198 solver.cpp:237] Iteration 11600, loss = 4.0818
I1209 15:23:53.287672  3198 solver.cpp:253]     Train net output #0: loss = 4.0818 (* 1 = 4.0818 loss)
I1209 15:23:53.287677  3198 sgd_solver.cpp:106] Iteration 11600, lr = 0.01
I1209 15:24:01.248880  3198 solver.cpp:237] Iteration 11620, loss = 4.03869
I1209 15:24:01.248916  3198 solver.cpp:253]     Train net output #0: loss = 4.03869 (* 1 = 4.03869 loss)
I1209 15:24:01.248922  3198 sgd_solver.cpp:106] Iteration 11620, lr = 0.01
I1209 15:24:08.980705  3198 solver.cpp:237] Iteration 11640, loss = 4.19618
I1209 15:24:08.980859  3198 solver.cpp:253]     Train net output #0: loss = 4.19618 (* 1 = 4.19618 loss)
I1209 15:24:08.980867  3198 sgd_solver.cpp:106] Iteration 11640, lr = 0.01
I1209 15:24:16.638468  3198 solver.cpp:237] Iteration 11660, loss = 4.27863
I1209 15:24:16.638504  3198 solver.cpp:253]     Train net output #0: loss = 4.27863 (* 1 = 4.27863 loss)
I1209 15:24:16.638509  3198 sgd_solver.cpp:106] Iteration 11660, lr = 0.01
I1209 15:24:24.312259  3198 solver.cpp:237] Iteration 11680, loss = 3.90807
I1209 15:24:24.312295  3198 solver.cpp:253]     Train net output #0: loss = 3.90807 (* 1 = 3.90807 loss)
I1209 15:24:24.312301  3198 sgd_solver.cpp:106] Iteration 11680, lr = 0.01
I1209 15:24:31.985141  3198 solver.cpp:237] Iteration 11700, loss = 4.03968
I1209 15:24:31.985177  3198 solver.cpp:253]     Train net output #0: loss = 4.03968 (* 1 = 4.03968 loss)
I1209 15:24:31.985184  3198 sgd_solver.cpp:106] Iteration 11700, lr = 0.01
I1209 15:24:39.648737  3198 solver.cpp:237] Iteration 11720, loss = 3.85412
I1209 15:24:39.648859  3198 solver.cpp:253]     Train net output #0: loss = 3.85412 (* 1 = 3.85412 loss)
I1209 15:24:39.648865  3198 sgd_solver.cpp:106] Iteration 11720, lr = 0.01
I1209 15:24:47.302670  3198 solver.cpp:237] Iteration 11740, loss = 4.11708
I1209 15:24:47.302707  3198 solver.cpp:253]     Train net output #0: loss = 4.11708 (* 1 = 4.11708 loss)
I1209 15:24:47.302712  3198 sgd_solver.cpp:106] Iteration 11740, lr = 0.01
I1209 15:24:54.937059  3198 solver.cpp:237] Iteration 11760, loss = 4.05535
I1209 15:24:54.937095  3198 solver.cpp:253]     Train net output #0: loss = 4.05535 (* 1 = 4.05535 loss)
I1209 15:24:54.937101  3198 sgd_solver.cpp:106] Iteration 11760, lr = 0.01
I1209 15:25:02.573220  3198 solver.cpp:237] Iteration 11780, loss = 4.03011
I1209 15:25:02.573256  3198 solver.cpp:253]     Train net output #0: loss = 4.03011 (* 1 = 4.03011 loss)
I1209 15:25:02.573264  3198 sgd_solver.cpp:106] Iteration 11780, lr = 0.01
I1209 15:25:10.199368  3198 solver.cpp:237] Iteration 11800, loss = 3.9624
I1209 15:25:10.199517  3198 solver.cpp:253]     Train net output #0: loss = 3.9624 (* 1 = 3.9624 loss)
I1209 15:25:10.199535  3198 sgd_solver.cpp:106] Iteration 11800, lr = 0.01
I1209 15:25:17.876018  3198 solver.cpp:237] Iteration 11820, loss = 4.05569
I1209 15:25:17.876054  3198 solver.cpp:253]     Train net output #0: loss = 4.05569 (* 1 = 4.05569 loss)
I1209 15:25:17.876060  3198 sgd_solver.cpp:106] Iteration 11820, lr = 0.01
I1209 15:25:25.562950  3198 solver.cpp:237] Iteration 11840, loss = 4.31328
I1209 15:25:25.562988  3198 solver.cpp:253]     Train net output #0: loss = 4.31328 (* 1 = 4.31328 loss)
I1209 15:25:25.562994  3198 sgd_solver.cpp:106] Iteration 11840, lr = 0.01
I1209 15:25:33.176347  3198 solver.cpp:237] Iteration 11860, loss = 4.02207
I1209 15:25:33.176383  3198 solver.cpp:253]     Train net output #0: loss = 4.02207 (* 1 = 4.02207 loss)
I1209 15:25:33.176389  3198 sgd_solver.cpp:106] Iteration 11860, lr = 0.01
I1209 15:25:40.846591  3198 solver.cpp:237] Iteration 11880, loss = 4.13263
I1209 15:25:40.846709  3198 solver.cpp:253]     Train net output #0: loss = 4.13263 (* 1 = 4.13263 loss)
I1209 15:25:40.846716  3198 sgd_solver.cpp:106] Iteration 11880, lr = 0.01
I1209 15:25:48.462797  3198 solver.cpp:237] Iteration 11900, loss = 4.14124
I1209 15:25:48.462834  3198 solver.cpp:253]     Train net output #0: loss = 4.14124 (* 1 = 4.14124 loss)
I1209 15:25:48.462841  3198 sgd_solver.cpp:106] Iteration 11900, lr = 0.01
I1209 15:25:56.143609  3198 solver.cpp:237] Iteration 11920, loss = 4.04898
I1209 15:25:56.143645  3198 solver.cpp:253]     Train net output #0: loss = 4.04898 (* 1 = 4.04898 loss)
I1209 15:25:56.143651  3198 sgd_solver.cpp:106] Iteration 11920, lr = 0.01
I1209 15:26:03.819720  3198 solver.cpp:237] Iteration 11940, loss = 3.91192
I1209 15:26:03.819756  3198 solver.cpp:253]     Train net output #0: loss = 3.91192 (* 1 = 3.91192 loss)
I1209 15:26:03.819762  3198 sgd_solver.cpp:106] Iteration 11940, lr = 0.01
I1209 15:26:11.479779  3198 solver.cpp:237] Iteration 11960, loss = 3.90828
I1209 15:26:11.479864  3198 solver.cpp:253]     Train net output #0: loss = 3.90828 (* 1 = 3.90828 loss)
I1209 15:26:11.479871  3198 sgd_solver.cpp:106] Iteration 11960, lr = 0.01
I1209 15:26:19.198382  3198 solver.cpp:237] Iteration 11980, loss = 4.01674
I1209 15:26:19.198421  3198 solver.cpp:253]     Train net output #0: loss = 4.01674 (* 1 = 4.01674 loss)
I1209 15:26:19.198426  3198 sgd_solver.cpp:106] Iteration 11980, lr = 0.01
I1209 15:26:26.518334  3198 solver.cpp:341] Iteration 12000, Testing net (#0)
I1209 15:26:33.986304  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:27:43.281440  3198 solver.cpp:409]     Test net output #0: accuracy = 0.20326
I1209 15:27:43.281574  3198 solver.cpp:409]     Test net output #1: loss = 4.03631 (* 1 = 4.03631 loss)
I1209 15:27:43.483398  3198 solver.cpp:237] Iteration 12000, loss = 4.07958
I1209 15:27:43.483433  3198 solver.cpp:253]     Train net output #0: loss = 4.07958 (* 1 = 4.07958 loss)
I1209 15:27:43.483440  3198 sgd_solver.cpp:106] Iteration 12000, lr = 0.01
I1209 15:27:50.478615  3198 solver.cpp:237] Iteration 12020, loss = 3.95287
I1209 15:27:50.478652  3198 solver.cpp:253]     Train net output #0: loss = 3.95287 (* 1 = 3.95287 loss)
I1209 15:27:50.478657  3198 sgd_solver.cpp:106] Iteration 12020, lr = 0.01
I1209 15:27:58.146848  3198 solver.cpp:237] Iteration 12040, loss = 4.03597
I1209 15:27:58.146885  3198 solver.cpp:253]     Train net output #0: loss = 4.03597 (* 1 = 4.03597 loss)
I1209 15:27:58.146891  3198 sgd_solver.cpp:106] Iteration 12040, lr = 0.01
I1209 15:28:05.825358  3198 solver.cpp:237] Iteration 12060, loss = 3.78523
I1209 15:28:05.825394  3198 solver.cpp:253]     Train net output #0: loss = 3.78523 (* 1 = 3.78523 loss)
I1209 15:28:05.825400  3198 sgd_solver.cpp:106] Iteration 12060, lr = 0.01
I1209 15:28:13.368311  3198 solver.cpp:237] Iteration 12080, loss = 3.75901
I1209 15:28:13.368466  3198 solver.cpp:253]     Train net output #0: loss = 3.75901 (* 1 = 3.75901 loss)
I1209 15:28:13.368474  3198 sgd_solver.cpp:106] Iteration 12080, lr = 0.01
I1209 15:28:20.956459  3198 solver.cpp:237] Iteration 12100, loss = 4.08328
I1209 15:28:20.956496  3198 solver.cpp:253]     Train net output #0: loss = 4.08328 (* 1 = 4.08328 loss)
I1209 15:28:20.956502  3198 sgd_solver.cpp:106] Iteration 12100, lr = 0.01
I1209 15:28:22.137919  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:28:28.559975  3198 solver.cpp:237] Iteration 12120, loss = 4.10541
I1209 15:28:28.560011  3198 solver.cpp:253]     Train net output #0: loss = 4.10541 (* 1 = 4.10541 loss)
I1209 15:28:28.560019  3198 sgd_solver.cpp:106] Iteration 12120, lr = 0.01
I1209 15:28:36.418349  3198 solver.cpp:237] Iteration 12140, loss = 4.04978
I1209 15:28:36.418385  3198 solver.cpp:253]     Train net output #0: loss = 4.04978 (* 1 = 4.04978 loss)
I1209 15:28:36.418391  3198 sgd_solver.cpp:106] Iteration 12140, lr = 0.01
I1209 15:28:44.364370  3198 solver.cpp:237] Iteration 12160, loss = 4.01427
I1209 15:28:44.364505  3198 solver.cpp:253]     Train net output #0: loss = 4.01427 (* 1 = 4.01427 loss)
I1209 15:28:44.364511  3198 sgd_solver.cpp:106] Iteration 12160, lr = 0.01
I1209 15:28:52.132968  3198 solver.cpp:237] Iteration 12180, loss = 3.96598
I1209 15:28:52.133005  3198 solver.cpp:253]     Train net output #0: loss = 3.96598 (* 1 = 3.96598 loss)
I1209 15:28:52.133011  3198 sgd_solver.cpp:106] Iteration 12180, lr = 0.01
I1209 15:28:59.970674  3198 solver.cpp:237] Iteration 12200, loss = 4.0466
I1209 15:28:59.970711  3198 solver.cpp:253]     Train net output #0: loss = 4.0466 (* 1 = 4.0466 loss)
I1209 15:28:59.970717  3198 sgd_solver.cpp:106] Iteration 12200, lr = 0.01
I1209 15:29:07.709183  3198 solver.cpp:237] Iteration 12220, loss = 3.79086
I1209 15:29:07.709280  3198 solver.cpp:253]     Train net output #0: loss = 3.79086 (* 1 = 3.79086 loss)
I1209 15:29:07.709302  3198 sgd_solver.cpp:106] Iteration 12220, lr = 0.01
I1209 15:29:15.319208  3198 solver.cpp:237] Iteration 12240, loss = 4.19088
I1209 15:29:15.319362  3198 solver.cpp:253]     Train net output #0: loss = 4.19088 (* 1 = 4.19088 loss)
I1209 15:29:15.319378  3198 sgd_solver.cpp:106] Iteration 12240, lr = 0.01
I1209 15:29:23.032897  3198 solver.cpp:237] Iteration 12260, loss = 3.94837
I1209 15:29:23.032933  3198 solver.cpp:253]     Train net output #0: loss = 3.94837 (* 1 = 3.94837 loss)
I1209 15:29:23.032939  3198 sgd_solver.cpp:106] Iteration 12260, lr = 0.01
I1209 15:29:30.679029  3198 solver.cpp:237] Iteration 12280, loss = 4.11199
I1209 15:29:30.679065  3198 solver.cpp:253]     Train net output #0: loss = 4.11199 (* 1 = 4.11199 loss)
I1209 15:29:30.679071  3198 sgd_solver.cpp:106] Iteration 12280, lr = 0.01
I1209 15:29:38.356909  3198 solver.cpp:237] Iteration 12300, loss = 3.95754
I1209 15:29:38.356946  3198 solver.cpp:253]     Train net output #0: loss = 3.95754 (* 1 = 3.95754 loss)
I1209 15:29:38.356951  3198 sgd_solver.cpp:106] Iteration 12300, lr = 0.01
I1209 15:29:46.038771  3198 solver.cpp:237] Iteration 12320, loss = 3.9208
I1209 15:29:46.038884  3198 solver.cpp:253]     Train net output #0: loss = 3.9208 (* 1 = 3.9208 loss)
I1209 15:29:46.038892  3198 sgd_solver.cpp:106] Iteration 12320, lr = 0.01
I1209 15:29:53.720381  3198 solver.cpp:237] Iteration 12340, loss = 3.87247
I1209 15:29:53.720415  3198 solver.cpp:253]     Train net output #0: loss = 3.87247 (* 1 = 3.87247 loss)
I1209 15:29:53.720422  3198 sgd_solver.cpp:106] Iteration 12340, lr = 0.01
I1209 15:30:01.367979  3198 solver.cpp:237] Iteration 12360, loss = 3.82965
I1209 15:30:01.368017  3198 solver.cpp:253]     Train net output #0: loss = 3.82965 (* 1 = 3.82965 loss)
I1209 15:30:01.368023  3198 sgd_solver.cpp:106] Iteration 12360, lr = 0.01
I1209 15:30:09.011111  3198 solver.cpp:237] Iteration 12380, loss = 4.10994
I1209 15:30:09.011148  3198 solver.cpp:253]     Train net output #0: loss = 4.10994 (* 1 = 4.10994 loss)
I1209 15:30:09.011154  3198 sgd_solver.cpp:106] Iteration 12380, lr = 0.01
I1209 15:30:16.667557  3198 solver.cpp:237] Iteration 12400, loss = 4.00026
I1209 15:30:16.668866  3198 solver.cpp:253]     Train net output #0: loss = 4.00026 (* 1 = 4.00026 loss)
I1209 15:30:16.668874  3198 sgd_solver.cpp:106] Iteration 12400, lr = 0.01
I1209 15:30:24.429047  3198 solver.cpp:237] Iteration 12420, loss = 4.00324
I1209 15:30:24.429083  3198 solver.cpp:253]     Train net output #0: loss = 4.00324 (* 1 = 4.00324 loss)
I1209 15:30:24.429090  3198 sgd_solver.cpp:106] Iteration 12420, lr = 0.01
I1209 15:30:32.393631  3198 solver.cpp:237] Iteration 12440, loss = 3.80405
I1209 15:30:32.393669  3198 solver.cpp:253]     Train net output #0: loss = 3.80405 (* 1 = 3.80405 loss)
I1209 15:30:32.393676  3198 sgd_solver.cpp:106] Iteration 12440, lr = 0.01
I1209 15:30:40.611778  3198 solver.cpp:237] Iteration 12460, loss = 4.13687
I1209 15:30:40.611841  3198 solver.cpp:253]     Train net output #0: loss = 4.13687 (* 1 = 4.13687 loss)
I1209 15:30:40.611934  3198 sgd_solver.cpp:106] Iteration 12460, lr = 0.01
I1209 15:30:48.675437  3198 solver.cpp:237] Iteration 12480, loss = 4.02976
I1209 15:30:48.675506  3198 solver.cpp:253]     Train net output #0: loss = 4.02976 (* 1 = 4.02976 loss)
I1209 15:30:48.675513  3198 sgd_solver.cpp:106] Iteration 12480, lr = 0.01
I1209 15:30:56.708627  3198 solver.cpp:237] Iteration 12500, loss = 3.97562
I1209 15:30:56.708662  3198 solver.cpp:253]     Train net output #0: loss = 3.97562 (* 1 = 3.97562 loss)
I1209 15:30:56.708667  3198 sgd_solver.cpp:106] Iteration 12500, lr = 0.01
I1209 15:31:04.607686  3198 solver.cpp:237] Iteration 12520, loss = 3.90737
I1209 15:31:04.607719  3198 solver.cpp:253]     Train net output #0: loss = 3.90737 (* 1 = 3.90737 loss)
I1209 15:31:04.607725  3198 sgd_solver.cpp:106] Iteration 12520, lr = 0.01
I1209 15:31:12.409023  3198 solver.cpp:237] Iteration 12540, loss = 4.06659
I1209 15:31:12.409101  3198 solver.cpp:253]     Train net output #0: loss = 4.06659 (* 1 = 4.06659 loss)
I1209 15:31:12.409117  3198 sgd_solver.cpp:106] Iteration 12540, lr = 0.01
I1209 15:31:20.050097  3198 solver.cpp:237] Iteration 12560, loss = 4.21833
I1209 15:31:20.050220  3198 solver.cpp:253]     Train net output #0: loss = 4.21833 (* 1 = 4.21833 loss)
I1209 15:31:20.050236  3198 sgd_solver.cpp:106] Iteration 12560, lr = 0.01
I1209 15:31:27.668740  3198 solver.cpp:237] Iteration 12580, loss = 3.77302
I1209 15:31:27.668774  3198 solver.cpp:253]     Train net output #0: loss = 3.77302 (* 1 = 3.77302 loss)
I1209 15:31:27.668781  3198 sgd_solver.cpp:106] Iteration 12580, lr = 0.01
I1209 15:31:35.267241  3198 solver.cpp:237] Iteration 12600, loss = 4.19044
I1209 15:31:35.267289  3198 solver.cpp:253]     Train net output #0: loss = 4.19044 (* 1 = 4.19044 loss)
I1209 15:31:35.267294  3198 sgd_solver.cpp:106] Iteration 12600, lr = 0.01
I1209 15:31:42.892494  3198 solver.cpp:237] Iteration 12620, loss = 3.92006
I1209 15:31:42.892529  3198 solver.cpp:253]     Train net output #0: loss = 3.92006 (* 1 = 3.92006 loss)
I1209 15:31:42.892535  3198 sgd_solver.cpp:106] Iteration 12620, lr = 0.01
I1209 15:31:50.615072  3198 solver.cpp:237] Iteration 12640, loss = 4.10887
I1209 15:31:50.615228  3198 solver.cpp:253]     Train net output #0: loss = 4.10887 (* 1 = 4.10887 loss)
I1209 15:31:50.615237  3198 sgd_solver.cpp:106] Iteration 12640, lr = 0.01
I1209 15:31:58.449748  3198 solver.cpp:237] Iteration 12660, loss = 4.18845
I1209 15:31:58.449784  3198 solver.cpp:253]     Train net output #0: loss = 4.18845 (* 1 = 4.18845 loss)
I1209 15:31:58.449790  3198 sgd_solver.cpp:106] Iteration 12660, lr = 0.01
I1209 15:32:06.251539  3198 solver.cpp:237] Iteration 12680, loss = 4.01205
I1209 15:32:06.251575  3198 solver.cpp:253]     Train net output #0: loss = 4.01205 (* 1 = 4.01205 loss)
I1209 15:32:06.251581  3198 sgd_solver.cpp:106] Iteration 12680, lr = 0.01
I1209 15:32:14.048249  3198 solver.cpp:237] Iteration 12700, loss = 4.31368
I1209 15:32:14.048283  3198 solver.cpp:253]     Train net output #0: loss = 4.31368 (* 1 = 4.31368 loss)
I1209 15:32:14.048290  3198 sgd_solver.cpp:106] Iteration 12700, lr = 0.01
I1209 15:32:21.835953  3198 solver.cpp:237] Iteration 12720, loss = 3.96907
I1209 15:32:21.836104  3198 solver.cpp:253]     Train net output #0: loss = 3.96907 (* 1 = 3.96907 loss)
I1209 15:32:21.836112  3198 sgd_solver.cpp:106] Iteration 12720, lr = 0.01
I1209 15:32:29.638648  3198 solver.cpp:237] Iteration 12740, loss = 4.16295
I1209 15:32:29.638685  3198 solver.cpp:253]     Train net output #0: loss = 4.16295 (* 1 = 4.16295 loss)
I1209 15:32:29.638691  3198 sgd_solver.cpp:106] Iteration 12740, lr = 0.01
I1209 15:32:37.405745  3198 solver.cpp:237] Iteration 12760, loss = 4.08529
I1209 15:32:37.405781  3198 solver.cpp:253]     Train net output #0: loss = 4.08529 (* 1 = 4.08529 loss)
I1209 15:32:37.405786  3198 sgd_solver.cpp:106] Iteration 12760, lr = 0.01
I1209 15:32:45.115491  3198 solver.cpp:237] Iteration 12780, loss = 3.845
I1209 15:32:45.115527  3198 solver.cpp:253]     Train net output #0: loss = 3.845 (* 1 = 3.845 loss)
I1209 15:32:45.115533  3198 sgd_solver.cpp:106] Iteration 12780, lr = 0.01
I1209 15:32:52.814451  3198 solver.cpp:237] Iteration 12800, loss = 4.17974
I1209 15:32:52.814568  3198 solver.cpp:253]     Train net output #0: loss = 4.17974 (* 1 = 4.17974 loss)
I1209 15:32:52.814574  3198 sgd_solver.cpp:106] Iteration 12800, lr = 0.01
I1209 15:33:00.512398  3198 solver.cpp:237] Iteration 12820, loss = 3.89962
I1209 15:33:00.512434  3198 solver.cpp:253]     Train net output #0: loss = 3.89962 (* 1 = 3.89962 loss)
I1209 15:33:00.512439  3198 sgd_solver.cpp:106] Iteration 12820, lr = 0.01
I1209 15:33:08.413328  3198 solver.cpp:237] Iteration 12840, loss = 4.1155
I1209 15:33:08.413357  3198 solver.cpp:253]     Train net output #0: loss = 4.1155 (* 1 = 4.1155 loss)
I1209 15:33:08.413362  3198 sgd_solver.cpp:106] Iteration 12840, lr = 0.01
I1209 15:33:16.351064  3198 solver.cpp:237] Iteration 12860, loss = 3.92365
I1209 15:33:16.351090  3198 solver.cpp:253]     Train net output #0: loss = 3.92365 (* 1 = 3.92365 loss)
I1209 15:33:16.351097  3198 sgd_solver.cpp:106] Iteration 12860, lr = 0.01
I1209 15:33:24.402740  3198 solver.cpp:237] Iteration 12880, loss = 3.90506
I1209 15:33:24.402894  3198 solver.cpp:253]     Train net output #0: loss = 3.90506 (* 1 = 3.90506 loss)
I1209 15:33:24.402902  3198 sgd_solver.cpp:106] Iteration 12880, lr = 0.01
I1209 15:33:32.419777  3198 solver.cpp:237] Iteration 12900, loss = 4.18663
I1209 15:33:32.419813  3198 solver.cpp:253]     Train net output #0: loss = 4.18663 (* 1 = 4.18663 loss)
I1209 15:33:32.419818  3198 sgd_solver.cpp:106] Iteration 12900, lr = 0.01
I1209 15:33:40.480243  3198 solver.cpp:237] Iteration 12920, loss = 4.00321
I1209 15:33:40.480278  3198 solver.cpp:253]     Train net output #0: loss = 4.00321 (* 1 = 4.00321 loss)
I1209 15:33:40.480285  3198 sgd_solver.cpp:106] Iteration 12920, lr = 0.01
I1209 15:33:48.205817  3198 solver.cpp:237] Iteration 12940, loss = 3.94898
I1209 15:33:48.205883  3198 solver.cpp:253]     Train net output #0: loss = 3.94898 (* 1 = 3.94898 loss)
I1209 15:33:48.205899  3198 sgd_solver.cpp:106] Iteration 12940, lr = 0.01
I1209 15:33:55.955484  3198 solver.cpp:237] Iteration 12960, loss = 4.20365
I1209 15:33:55.955574  3198 solver.cpp:253]     Train net output #0: loss = 4.20365 (* 1 = 4.20365 loss)
I1209 15:33:55.955580  3198 sgd_solver.cpp:106] Iteration 12960, lr = 0.01
I1209 15:34:03.746640  3198 solver.cpp:237] Iteration 12980, loss = 3.99677
I1209 15:34:03.746675  3198 solver.cpp:253]     Train net output #0: loss = 3.99677 (* 1 = 3.99677 loss)
I1209 15:34:03.746681  3198 sgd_solver.cpp:106] Iteration 12980, lr = 0.01
I1209 15:34:11.172391  3198 solver.cpp:341] Iteration 13000, Testing net (#0)
I1209 15:34:19.315166  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:35:29.237380  3198 solver.cpp:409]     Test net output #0: accuracy = 0.2091
I1209 15:35:29.237517  3198 solver.cpp:409]     Test net output #1: loss = 3.98591 (* 1 = 3.98591 loss)
I1209 15:35:29.453013  3198 solver.cpp:237] Iteration 13000, loss = 3.7752
I1209 15:35:29.453116  3198 solver.cpp:253]     Train net output #0: loss = 3.7752 (* 1 = 3.7752 loss)
I1209 15:35:29.453133  3198 sgd_solver.cpp:106] Iteration 13000, lr = 0.01
I1209 15:35:36.607477  3198 solver.cpp:237] Iteration 13020, loss = 4.01829
I1209 15:35:36.607513  3198 solver.cpp:253]     Train net output #0: loss = 4.01829 (* 1 = 4.01829 loss)
I1209 15:35:36.607519  3198 sgd_solver.cpp:106] Iteration 13020, lr = 0.01
I1209 15:35:44.433600  3198 solver.cpp:237] Iteration 13040, loss = 3.72105
I1209 15:35:44.433637  3198 solver.cpp:253]     Train net output #0: loss = 3.72105 (* 1 = 3.72105 loss)
I1209 15:35:44.433643  3198 sgd_solver.cpp:106] Iteration 13040, lr = 0.01
I1209 15:35:52.221703  3198 solver.cpp:237] Iteration 13060, loss = 4.04768
I1209 15:35:52.221735  3198 solver.cpp:253]     Train net output #0: loss = 4.04768 (* 1 = 4.04768 loss)
I1209 15:35:52.221741  3198 sgd_solver.cpp:106] Iteration 13060, lr = 0.01
I1209 15:35:59.978565  3198 solver.cpp:237] Iteration 13080, loss = 4.0916
I1209 15:35:59.978684  3198 solver.cpp:253]     Train net output #0: loss = 4.0916 (* 1 = 4.0916 loss)
I1209 15:35:59.978699  3198 sgd_solver.cpp:106] Iteration 13080, lr = 0.01
I1209 15:36:07.793270  3198 solver.cpp:237] Iteration 13100, loss = 4.07154
I1209 15:36:07.793306  3198 solver.cpp:253]     Train net output #0: loss = 4.07154 (* 1 = 4.07154 loss)
I1209 15:36:07.793313  3198 sgd_solver.cpp:106] Iteration 13100, lr = 0.01
I1209 15:36:12.129318  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:36:15.636111  3198 solver.cpp:237] Iteration 13120, loss = 4.07938
I1209 15:36:15.636147  3198 solver.cpp:253]     Train net output #0: loss = 4.07938 (* 1 = 4.07938 loss)
I1209 15:36:15.636152  3198 sgd_solver.cpp:106] Iteration 13120, lr = 0.01
I1209 15:36:23.485393  3198 solver.cpp:237] Iteration 13140, loss = 3.94045
I1209 15:36:23.485440  3198 solver.cpp:253]     Train net output #0: loss = 3.94045 (* 1 = 3.94045 loss)
I1209 15:36:23.485447  3198 sgd_solver.cpp:106] Iteration 13140, lr = 0.01
I1209 15:36:31.308639  3198 solver.cpp:237] Iteration 13160, loss = 3.9195
I1209 15:36:31.308807  3198 solver.cpp:253]     Train net output #0: loss = 3.9195 (* 1 = 3.9195 loss)
I1209 15:36:31.308815  3198 sgd_solver.cpp:106] Iteration 13160, lr = 0.01
I1209 15:36:39.145054  3198 solver.cpp:237] Iteration 13180, loss = 4.03671
I1209 15:36:39.145089  3198 solver.cpp:253]     Train net output #0: loss = 4.03671 (* 1 = 4.03671 loss)
I1209 15:36:39.145095  3198 sgd_solver.cpp:106] Iteration 13180, lr = 0.01
I1209 15:36:47.015082  3198 solver.cpp:237] Iteration 13200, loss = 3.97616
I1209 15:36:47.015116  3198 solver.cpp:253]     Train net output #0: loss = 3.97616 (* 1 = 3.97616 loss)
I1209 15:36:47.015121  3198 sgd_solver.cpp:106] Iteration 13200, lr = 0.01
I1209 15:36:54.902925  3198 solver.cpp:237] Iteration 13220, loss = 3.92514
I1209 15:36:54.902963  3198 solver.cpp:253]     Train net output #0: loss = 3.92514 (* 1 = 3.92514 loss)
I1209 15:36:54.902969  3198 sgd_solver.cpp:106] Iteration 13220, lr = 0.01
I1209 15:37:02.581538  3198 solver.cpp:237] Iteration 13240, loss = 4.01016
I1209 15:37:02.581751  3198 solver.cpp:253]     Train net output #0: loss = 4.01016 (* 1 = 4.01016 loss)
I1209 15:37:02.581795  3198 sgd_solver.cpp:106] Iteration 13240, lr = 0.01
I1209 15:37:10.292920  3198 solver.cpp:237] Iteration 13260, loss = 4.03091
I1209 15:37:10.292956  3198 solver.cpp:253]     Train net output #0: loss = 4.03091 (* 1 = 4.03091 loss)
I1209 15:37:10.292963  3198 sgd_solver.cpp:106] Iteration 13260, lr = 0.01
I1209 15:37:17.968664  3198 solver.cpp:237] Iteration 13280, loss = 3.88892
I1209 15:37:17.968701  3198 solver.cpp:253]     Train net output #0: loss = 3.88892 (* 1 = 3.88892 loss)
I1209 15:37:17.968708  3198 sgd_solver.cpp:106] Iteration 13280, lr = 0.01
I1209 15:37:25.609338  3198 solver.cpp:237] Iteration 13300, loss = 4.07311
I1209 15:37:25.609374  3198 solver.cpp:253]     Train net output #0: loss = 4.07311 (* 1 = 4.07311 loss)
I1209 15:37:25.609380  3198 sgd_solver.cpp:106] Iteration 13300, lr = 0.01
I1209 15:37:33.287976  3198 solver.cpp:237] Iteration 13320, loss = 3.99495
I1209 15:37:33.288100  3198 solver.cpp:253]     Train net output #0: loss = 3.99495 (* 1 = 3.99495 loss)
I1209 15:37:33.288107  3198 sgd_solver.cpp:106] Iteration 13320, lr = 0.01
I1209 15:37:40.999639  3198 solver.cpp:237] Iteration 13340, loss = 4.10881
I1209 15:37:40.999738  3198 solver.cpp:253]     Train net output #0: loss = 4.10881 (* 1 = 4.10881 loss)
I1209 15:37:40.999765  3198 sgd_solver.cpp:106] Iteration 13340, lr = 0.01
I1209 15:37:48.936362  3198 solver.cpp:237] Iteration 13360, loss = 4.12788
I1209 15:37:48.936395  3198 solver.cpp:253]     Train net output #0: loss = 4.12788 (* 1 = 4.12788 loss)
I1209 15:37:48.936403  3198 sgd_solver.cpp:106] Iteration 13360, lr = 0.01
I1209 15:37:56.797417  3198 solver.cpp:237] Iteration 13380, loss = 3.9552
I1209 15:37:56.797541  3198 solver.cpp:253]     Train net output #0: loss = 3.9552 (* 1 = 3.9552 loss)
I1209 15:37:56.797550  3198 sgd_solver.cpp:106] Iteration 13380, lr = 0.01
I1209 15:38:04.754009  3198 solver.cpp:237] Iteration 13400, loss = 4.18129
I1209 15:38:04.754148  3198 solver.cpp:253]     Train net output #0: loss = 4.18129 (* 1 = 4.18129 loss)
I1209 15:38:04.754165  3198 sgd_solver.cpp:106] Iteration 13400, lr = 0.01
I1209 15:38:12.785948  3198 solver.cpp:237] Iteration 13420, loss = 3.906
I1209 15:38:12.785982  3198 solver.cpp:253]     Train net output #0: loss = 3.906 (* 1 = 3.906 loss)
I1209 15:38:12.785989  3198 sgd_solver.cpp:106] Iteration 13420, lr = 0.01
I1209 15:38:20.615316  3198 solver.cpp:237] Iteration 13440, loss = 4.18793
I1209 15:38:20.615351  3198 solver.cpp:253]     Train net output #0: loss = 4.18793 (* 1 = 4.18793 loss)
I1209 15:38:20.615357  3198 sgd_solver.cpp:106] Iteration 13440, lr = 0.01
I1209 15:38:28.733654  3198 solver.cpp:237] Iteration 13460, loss = 4.05634
I1209 15:38:28.733690  3198 solver.cpp:253]     Train net output #0: loss = 4.05634 (* 1 = 4.05634 loss)
I1209 15:38:28.733696  3198 sgd_solver.cpp:106] Iteration 13460, lr = 0.01
I1209 15:38:36.553941  3198 solver.cpp:237] Iteration 13480, loss = 4.05102
I1209 15:38:36.554105  3198 solver.cpp:253]     Train net output #0: loss = 4.05102 (* 1 = 4.05102 loss)
I1209 15:38:36.554111  3198 sgd_solver.cpp:106] Iteration 13480, lr = 0.01
I1209 15:38:44.614260  3198 solver.cpp:237] Iteration 13500, loss = 3.74152
I1209 15:38:44.614306  3198 solver.cpp:253]     Train net output #0: loss = 3.74152 (* 1 = 3.74152 loss)
I1209 15:38:44.614325  3198 sgd_solver.cpp:106] Iteration 13500, lr = 0.01
I1209 15:38:52.379266  3198 solver.cpp:237] Iteration 13520, loss = 3.96823
I1209 15:38:52.379302  3198 solver.cpp:253]     Train net output #0: loss = 3.96823 (* 1 = 3.96823 loss)
I1209 15:38:52.379307  3198 sgd_solver.cpp:106] Iteration 13520, lr = 0.01
I1209 15:39:00.196537  3198 solver.cpp:237] Iteration 13540, loss = 4.02116
I1209 15:39:00.196571  3198 solver.cpp:253]     Train net output #0: loss = 4.02116 (* 1 = 4.02116 loss)
I1209 15:39:00.196578  3198 sgd_solver.cpp:106] Iteration 13540, lr = 0.01
I1209 15:39:08.017366  3198 solver.cpp:237] Iteration 13560, loss = 3.72281
I1209 15:39:08.017472  3198 solver.cpp:253]     Train net output #0: loss = 3.72281 (* 1 = 3.72281 loss)
I1209 15:39:08.017488  3198 sgd_solver.cpp:106] Iteration 13560, lr = 0.01
I1209 15:39:15.772303  3198 solver.cpp:237] Iteration 13580, loss = 3.97571
I1209 15:39:15.772330  3198 solver.cpp:253]     Train net output #0: loss = 3.97571 (* 1 = 3.97571 loss)
I1209 15:39:15.772336  3198 sgd_solver.cpp:106] Iteration 13580, lr = 0.01
I1209 15:39:23.493017  3198 solver.cpp:237] Iteration 13600, loss = 3.89302
I1209 15:39:23.493054  3198 solver.cpp:253]     Train net output #0: loss = 3.89302 (* 1 = 3.89302 loss)
I1209 15:39:23.493060  3198 sgd_solver.cpp:106] Iteration 13600, lr = 0.01
I1209 15:39:31.239784  3198 solver.cpp:237] Iteration 13620, loss = 3.99555
I1209 15:39:31.239820  3198 solver.cpp:253]     Train net output #0: loss = 3.99555 (* 1 = 3.99555 loss)
I1209 15:39:31.239826  3198 sgd_solver.cpp:106] Iteration 13620, lr = 0.01
I1209 15:39:39.025043  3198 solver.cpp:237] Iteration 13640, loss = 4.14463
I1209 15:39:39.025156  3198 solver.cpp:253]     Train net output #0: loss = 4.14463 (* 1 = 4.14463 loss)
I1209 15:39:39.025162  3198 sgd_solver.cpp:106] Iteration 13640, lr = 0.01
I1209 15:39:46.718070  3198 solver.cpp:237] Iteration 13660, loss = 4.04228
I1209 15:39:46.718104  3198 solver.cpp:253]     Train net output #0: loss = 4.04228 (* 1 = 4.04228 loss)
I1209 15:39:46.718111  3198 sgd_solver.cpp:106] Iteration 13660, lr = 0.01
I1209 15:39:54.425047  3198 solver.cpp:237] Iteration 13680, loss = 3.96424
I1209 15:39:54.425084  3198 solver.cpp:253]     Train net output #0: loss = 3.96424 (* 1 = 3.96424 loss)
I1209 15:39:54.425091  3198 sgd_solver.cpp:106] Iteration 13680, lr = 0.01
I1209 15:40:02.182027  3198 solver.cpp:237] Iteration 13700, loss = 4.14507
I1209 15:40:02.182063  3198 solver.cpp:253]     Train net output #0: loss = 4.14507 (* 1 = 4.14507 loss)
I1209 15:40:02.182070  3198 sgd_solver.cpp:106] Iteration 13700, lr = 0.01
I1209 15:40:09.978178  3198 solver.cpp:237] Iteration 13720, loss = 4.16031
I1209 15:40:09.978333  3198 solver.cpp:253]     Train net output #0: loss = 4.16031 (* 1 = 4.16031 loss)
I1209 15:40:09.978343  3198 sgd_solver.cpp:106] Iteration 13720, lr = 0.01
I1209 15:40:17.725247  3198 solver.cpp:237] Iteration 13740, loss = 3.97105
I1209 15:40:17.725374  3198 solver.cpp:253]     Train net output #0: loss = 3.97105 (* 1 = 3.97105 loss)
I1209 15:40:17.725384  3198 sgd_solver.cpp:106] Iteration 13740, lr = 0.01
I1209 15:40:25.455701  3198 solver.cpp:237] Iteration 13760, loss = 3.89986
I1209 15:40:25.455737  3198 solver.cpp:253]     Train net output #0: loss = 3.89986 (* 1 = 3.89986 loss)
I1209 15:40:25.455744  3198 sgd_solver.cpp:106] Iteration 13760, lr = 0.01
I1209 15:40:33.158114  3198 solver.cpp:237] Iteration 13780, loss = 3.87919
I1209 15:40:33.158151  3198 solver.cpp:253]     Train net output #0: loss = 3.87919 (* 1 = 3.87919 loss)
I1209 15:40:33.158157  3198 sgd_solver.cpp:106] Iteration 13780, lr = 0.01
I1209 15:40:40.854913  3198 solver.cpp:237] Iteration 13800, loss = 3.75163
I1209 15:40:40.855017  3198 solver.cpp:253]     Train net output #0: loss = 3.75163 (* 1 = 3.75163 loss)
I1209 15:40:40.855025  3198 sgd_solver.cpp:106] Iteration 13800, lr = 0.01
I1209 15:40:48.625036  3198 solver.cpp:237] Iteration 13820, loss = 4.02331
I1209 15:40:48.625072  3198 solver.cpp:253]     Train net output #0: loss = 4.02331 (* 1 = 4.02331 loss)
I1209 15:40:48.625078  3198 sgd_solver.cpp:106] Iteration 13820, lr = 0.01
I1209 15:40:56.308532  3198 solver.cpp:237] Iteration 13840, loss = 3.96998
I1209 15:40:56.308568  3198 solver.cpp:253]     Train net output #0: loss = 3.96998 (* 1 = 3.96998 loss)
I1209 15:40:56.308574  3198 sgd_solver.cpp:106] Iteration 13840, lr = 0.01
I1209 15:41:04.088068  3198 solver.cpp:237] Iteration 13860, loss = 3.91817
I1209 15:41:04.088104  3198 solver.cpp:253]     Train net output #0: loss = 3.91817 (* 1 = 3.91817 loss)
I1209 15:41:04.088109  3198 sgd_solver.cpp:106] Iteration 13860, lr = 0.01
I1209 15:41:11.864238  3198 solver.cpp:237] Iteration 13880, loss = 3.94307
I1209 15:41:11.864352  3198 solver.cpp:253]     Train net output #0: loss = 3.94307 (* 1 = 3.94307 loss)
I1209 15:41:11.864359  3198 sgd_solver.cpp:106] Iteration 13880, lr = 0.01
I1209 15:41:19.624580  3198 solver.cpp:237] Iteration 13900, loss = 3.81571
I1209 15:41:19.624626  3198 solver.cpp:253]     Train net output #0: loss = 3.81571 (* 1 = 3.81571 loss)
I1209 15:41:19.624634  3198 sgd_solver.cpp:106] Iteration 13900, lr = 0.01
I1209 15:41:27.356055  3198 solver.cpp:237] Iteration 13920, loss = 3.91154
I1209 15:41:27.356089  3198 solver.cpp:253]     Train net output #0: loss = 3.91154 (* 1 = 3.91154 loss)
I1209 15:41:27.356096  3198 sgd_solver.cpp:106] Iteration 13920, lr = 0.01
I1209 15:41:35.066123  3198 solver.cpp:237] Iteration 13940, loss = 4.04931
I1209 15:41:35.066161  3198 solver.cpp:253]     Train net output #0: loss = 4.04931 (* 1 = 4.04931 loss)
I1209 15:41:35.066169  3198 sgd_solver.cpp:106] Iteration 13940, lr = 0.01
I1209 15:41:42.760680  3198 solver.cpp:237] Iteration 13960, loss = 4.08366
I1209 15:41:42.760843  3198 solver.cpp:253]     Train net output #0: loss = 4.08366 (* 1 = 4.08366 loss)
I1209 15:41:42.760854  3198 sgd_solver.cpp:106] Iteration 13960, lr = 0.01
I1209 15:41:50.501360  3198 solver.cpp:237] Iteration 13980, loss = 4.09597
I1209 15:41:50.501397  3198 solver.cpp:253]     Train net output #0: loss = 4.09597 (* 1 = 4.09597 loss)
I1209 15:41:50.501404  3198 sgd_solver.cpp:106] Iteration 13980, lr = 0.01
I1209 15:41:57.809705  3198 solver.cpp:341] Iteration 14000, Testing net (#0)
I1209 15:42:06.611559  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:43:16.542966  3198 solver.cpp:409]     Test net output #0: accuracy = 0.20958
I1209 15:43:16.543072  3198 solver.cpp:409]     Test net output #1: loss = 4.00294 (* 1 = 4.00294 loss)
I1209 15:43:16.745534  3198 solver.cpp:237] Iteration 14000, loss = 3.92028
I1209 15:43:16.745569  3198 solver.cpp:253]     Train net output #0: loss = 3.92028 (* 1 = 3.92028 loss)
I1209 15:43:16.745575  3198 sgd_solver.cpp:106] Iteration 14000, lr = 0.01
I1209 15:43:23.801234  3198 solver.cpp:237] Iteration 14020, loss = 3.95515
I1209 15:43:23.801275  3198 solver.cpp:253]     Train net output #0: loss = 3.95515 (* 1 = 3.95515 loss)
I1209 15:43:23.801292  3198 sgd_solver.cpp:106] Iteration 14020, lr = 0.01
I1209 15:43:31.550449  3198 solver.cpp:237] Iteration 14040, loss = 3.95652
I1209 15:43:31.550496  3198 solver.cpp:253]     Train net output #0: loss = 3.95652 (* 1 = 3.95652 loss)
I1209 15:43:31.550503  3198 sgd_solver.cpp:106] Iteration 14040, lr = 0.01
I1209 15:43:39.317927  3198 solver.cpp:237] Iteration 14060, loss = 3.91474
I1209 15:43:39.317961  3198 solver.cpp:253]     Train net output #0: loss = 3.91474 (* 1 = 3.91474 loss)
I1209 15:43:39.317967  3198 sgd_solver.cpp:106] Iteration 14060, lr = 0.01
I1209 15:43:47.040261  3198 solver.cpp:237] Iteration 14080, loss = 3.85025
I1209 15:43:47.040411  3198 solver.cpp:253]     Train net output #0: loss = 3.85025 (* 1 = 3.85025 loss)
I1209 15:43:47.040429  3198 sgd_solver.cpp:106] Iteration 14080, lr = 0.01
I1209 15:43:54.775898  3198 solver.cpp:237] Iteration 14100, loss = 3.98992
I1209 15:43:54.775935  3198 solver.cpp:253]     Train net output #0: loss = 3.98992 (* 1 = 3.98992 loss)
I1209 15:43:54.775941  3198 sgd_solver.cpp:106] Iteration 14100, lr = 0.01
I1209 15:44:02.171355  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:44:02.532820  3198 solver.cpp:237] Iteration 14120, loss = 4.2903
I1209 15:44:02.532857  3198 solver.cpp:253]     Train net output #0: loss = 4.2903 (* 1 = 4.2903 loss)
I1209 15:44:02.532863  3198 sgd_solver.cpp:106] Iteration 14120, lr = 0.01
I1209 15:44:10.295079  3198 solver.cpp:237] Iteration 14140, loss = 3.86224
I1209 15:44:10.295115  3198 solver.cpp:253]     Train net output #0: loss = 3.86224 (* 1 = 3.86224 loss)
I1209 15:44:10.295121  3198 sgd_solver.cpp:106] Iteration 14140, lr = 0.01
I1209 15:44:18.153691  3198 solver.cpp:237] Iteration 14160, loss = 3.89484
I1209 15:44:18.153852  3198 solver.cpp:253]     Train net output #0: loss = 3.89484 (* 1 = 3.89484 loss)
I1209 15:44:18.153859  3198 sgd_solver.cpp:106] Iteration 14160, lr = 0.01
I1209 15:44:25.951442  3198 solver.cpp:237] Iteration 14180, loss = 4.03413
I1209 15:44:25.951478  3198 solver.cpp:253]     Train net output #0: loss = 4.03413 (* 1 = 4.03413 loss)
I1209 15:44:25.951483  3198 sgd_solver.cpp:106] Iteration 14180, lr = 0.01
I1209 15:44:33.832842  3198 solver.cpp:237] Iteration 14200, loss = 4.02223
I1209 15:44:33.832878  3198 solver.cpp:253]     Train net output #0: loss = 4.02223 (* 1 = 4.02223 loss)
I1209 15:44:33.832885  3198 sgd_solver.cpp:106] Iteration 14200, lr = 0.01
I1209 15:44:41.482077  3198 solver.cpp:237] Iteration 14220, loss = 3.92001
I1209 15:44:41.482115  3198 solver.cpp:253]     Train net output #0: loss = 3.92001 (* 1 = 3.92001 loss)
I1209 15:44:41.482121  3198 sgd_solver.cpp:106] Iteration 14220, lr = 0.01
I1209 15:44:49.226425  3198 solver.cpp:237] Iteration 14240, loss = 4.02046
I1209 15:44:49.226603  3198 solver.cpp:253]     Train net output #0: loss = 4.02046 (* 1 = 4.02046 loss)
I1209 15:44:49.226610  3198 sgd_solver.cpp:106] Iteration 14240, lr = 0.01
I1209 15:44:57.078095  3198 solver.cpp:237] Iteration 14260, loss = 3.91063
I1209 15:44:57.078130  3198 solver.cpp:253]     Train net output #0: loss = 3.91063 (* 1 = 3.91063 loss)
I1209 15:44:57.078135  3198 sgd_solver.cpp:106] Iteration 14260, lr = 0.01
I1209 15:45:05.089558  3198 solver.cpp:237] Iteration 14280, loss = 3.83702
I1209 15:45:05.089583  3198 solver.cpp:253]     Train net output #0: loss = 3.83702 (* 1 = 3.83702 loss)
I1209 15:45:05.089589  3198 sgd_solver.cpp:106] Iteration 14280, lr = 0.01
I1209 15:45:13.144299  3198 solver.cpp:237] Iteration 14300, loss = 3.82614
I1209 15:45:13.144333  3198 solver.cpp:253]     Train net output #0: loss = 3.82614 (* 1 = 3.82614 loss)
I1209 15:45:13.144340  3198 sgd_solver.cpp:106] Iteration 14300, lr = 0.01
I1209 15:45:21.035356  3198 solver.cpp:237] Iteration 14320, loss = 4.13084
I1209 15:45:21.035503  3198 solver.cpp:253]     Train net output #0: loss = 4.13084 (* 1 = 4.13084 loss)
I1209 15:45:21.035511  3198 sgd_solver.cpp:106] Iteration 14320, lr = 0.01
I1209 15:45:28.912760  3198 solver.cpp:237] Iteration 14340, loss = 3.8907
I1209 15:45:28.912796  3198 solver.cpp:253]     Train net output #0: loss = 3.8907 (* 1 = 3.8907 loss)
I1209 15:45:28.912801  3198 sgd_solver.cpp:106] Iteration 14340, lr = 0.01
I1209 15:45:36.552397  3198 solver.cpp:237] Iteration 14360, loss = 3.90186
I1209 15:45:36.552430  3198 solver.cpp:253]     Train net output #0: loss = 3.90186 (* 1 = 3.90186 loss)
I1209 15:45:36.552436  3198 sgd_solver.cpp:106] Iteration 14360, lr = 0.01
I1209 15:45:44.387523  3198 solver.cpp:237] Iteration 14380, loss = 3.89252
I1209 15:45:44.387558  3198 solver.cpp:253]     Train net output #0: loss = 3.89252 (* 1 = 3.89252 loss)
I1209 15:45:44.387563  3198 sgd_solver.cpp:106] Iteration 14380, lr = 0.01
I1209 15:45:52.353633  3198 solver.cpp:237] Iteration 14400, loss = 3.87538
I1209 15:45:52.353796  3198 solver.cpp:253]     Train net output #0: loss = 3.87538 (* 1 = 3.87538 loss)
I1209 15:45:52.353803  3198 sgd_solver.cpp:106] Iteration 14400, lr = 0.01
I1209 15:46:00.246145  3198 solver.cpp:237] Iteration 14420, loss = 4.11041
I1209 15:46:00.246178  3198 solver.cpp:253]     Train net output #0: loss = 4.11041 (* 1 = 4.11041 loss)
I1209 15:46:00.246184  3198 sgd_solver.cpp:106] Iteration 14420, lr = 0.01
I1209 15:46:08.099334  3198 solver.cpp:237] Iteration 14440, loss = 3.90295
I1209 15:46:08.099377  3198 solver.cpp:253]     Train net output #0: loss = 3.90295 (* 1 = 3.90295 loss)
I1209 15:46:08.099383  3198 sgd_solver.cpp:106] Iteration 14440, lr = 0.01
I1209 15:46:15.813196  3198 solver.cpp:237] Iteration 14460, loss = 4.0802
I1209 15:46:15.813231  3198 solver.cpp:253]     Train net output #0: loss = 4.0802 (* 1 = 4.0802 loss)
I1209 15:46:15.813236  3198 sgd_solver.cpp:106] Iteration 14460, lr = 0.01
I1209 15:46:23.608131  3198 solver.cpp:237] Iteration 14480, loss = 3.88749
I1209 15:46:23.608285  3198 solver.cpp:253]     Train net output #0: loss = 3.88749 (* 1 = 3.88749 loss)
I1209 15:46:23.608292  3198 sgd_solver.cpp:106] Iteration 14480, lr = 0.01
I1209 15:46:31.399245  3198 solver.cpp:237] Iteration 14500, loss = 3.93587
I1209 15:46:31.399281  3198 solver.cpp:253]     Train net output #0: loss = 3.93587 (* 1 = 3.93587 loss)
I1209 15:46:31.399286  3198 sgd_solver.cpp:106] Iteration 14500, lr = 0.01
I1209 15:46:39.131705  3198 solver.cpp:237] Iteration 14520, loss = 4.08765
I1209 15:46:39.131830  3198 solver.cpp:253]     Train net output #0: loss = 4.08765 (* 1 = 4.08765 loss)
I1209 15:46:39.131866  3198 sgd_solver.cpp:106] Iteration 14520, lr = 0.01
I1209 15:46:46.786751  3198 solver.cpp:237] Iteration 14540, loss = 3.87461
I1209 15:46:46.786793  3198 solver.cpp:253]     Train net output #0: loss = 3.87461 (* 1 = 3.87461 loss)
I1209 15:46:46.786800  3198 sgd_solver.cpp:106] Iteration 14540, lr = 0.01
I1209 15:46:54.524426  3198 solver.cpp:237] Iteration 14560, loss = 3.95494
I1209 15:46:54.524528  3198 solver.cpp:253]     Train net output #0: loss = 3.95494 (* 1 = 3.95494 loss)
I1209 15:46:54.524535  3198 sgd_solver.cpp:106] Iteration 14560, lr = 0.01
I1209 15:47:02.254604  3198 solver.cpp:237] Iteration 14580, loss = 3.97383
I1209 15:47:02.254639  3198 solver.cpp:253]     Train net output #0: loss = 3.97383 (* 1 = 3.97383 loss)
I1209 15:47:02.254645  3198 sgd_solver.cpp:106] Iteration 14580, lr = 0.01
I1209 15:47:10.029724  3198 solver.cpp:237] Iteration 14600, loss = 3.70953
I1209 15:47:10.029780  3198 solver.cpp:253]     Train net output #0: loss = 3.70953 (* 1 = 3.70953 loss)
I1209 15:47:10.029798  3198 sgd_solver.cpp:106] Iteration 14600, lr = 0.01
I1209 15:47:17.906622  3198 solver.cpp:237] Iteration 14620, loss = 4.00364
I1209 15:47:17.906659  3198 solver.cpp:253]     Train net output #0: loss = 4.00364 (* 1 = 4.00364 loss)
I1209 15:47:17.906666  3198 sgd_solver.cpp:106] Iteration 14620, lr = 0.01
I1209 15:47:25.655835  3198 solver.cpp:237] Iteration 14640, loss = 3.97718
I1209 15:47:25.655993  3198 solver.cpp:253]     Train net output #0: loss = 3.97718 (* 1 = 3.97718 loss)
I1209 15:47:25.656000  3198 sgd_solver.cpp:106] Iteration 14640, lr = 0.01
I1209 15:47:33.459983  3198 solver.cpp:237] Iteration 14660, loss = 3.96394
I1209 15:47:33.460018  3198 solver.cpp:253]     Train net output #0: loss = 3.96394 (* 1 = 3.96394 loss)
I1209 15:47:33.460026  3198 sgd_solver.cpp:106] Iteration 14660, lr = 0.01
I1209 15:47:41.229852  3198 solver.cpp:237] Iteration 14680, loss = 3.83452
I1209 15:47:41.229887  3198 solver.cpp:253]     Train net output #0: loss = 3.83452 (* 1 = 3.83452 loss)
I1209 15:47:41.229892  3198 sgd_solver.cpp:106] Iteration 14680, lr = 0.01
I1209 15:47:48.939525  3198 solver.cpp:237] Iteration 14700, loss = 3.77527
I1209 15:47:48.939564  3198 solver.cpp:253]     Train net output #0: loss = 3.77527 (* 1 = 3.77527 loss)
I1209 15:47:48.939570  3198 sgd_solver.cpp:106] Iteration 14700, lr = 0.01
I1209 15:47:56.564417  3198 solver.cpp:237] Iteration 14720, loss = 3.86954
I1209 15:47:56.564544  3198 solver.cpp:253]     Train net output #0: loss = 3.86954 (* 1 = 3.86954 loss)
I1209 15:47:56.564558  3198 sgd_solver.cpp:106] Iteration 14720, lr = 0.01
I1209 15:48:04.224624  3198 solver.cpp:237] Iteration 14740, loss = 3.88949
I1209 15:48:04.224663  3198 solver.cpp:253]     Train net output #0: loss = 3.88949 (* 1 = 3.88949 loss)
I1209 15:48:04.224668  3198 sgd_solver.cpp:106] Iteration 14740, lr = 0.01
I1209 15:48:11.889183  3198 solver.cpp:237] Iteration 14760, loss = 3.91519
I1209 15:48:11.889221  3198 solver.cpp:253]     Train net output #0: loss = 3.91519 (* 1 = 3.91519 loss)
I1209 15:48:11.889227  3198 sgd_solver.cpp:106] Iteration 14760, lr = 0.01
I1209 15:48:19.509532  3198 solver.cpp:237] Iteration 14780, loss = 3.99015
I1209 15:48:19.509568  3198 solver.cpp:253]     Train net output #0: loss = 3.99015 (* 1 = 3.99015 loss)
I1209 15:48:19.509574  3198 sgd_solver.cpp:106] Iteration 14780, lr = 0.01
I1209 15:48:27.200083  3198 solver.cpp:237] Iteration 14800, loss = 4.04856
I1209 15:48:27.200260  3198 solver.cpp:253]     Train net output #0: loss = 4.04856 (* 1 = 4.04856 loss)
I1209 15:48:27.200278  3198 sgd_solver.cpp:106] Iteration 14800, lr = 0.01
I1209 15:48:34.847049  3198 solver.cpp:237] Iteration 14820, loss = 3.71671
I1209 15:48:34.847086  3198 solver.cpp:253]     Train net output #0: loss = 3.71671 (* 1 = 3.71671 loss)
I1209 15:48:34.847091  3198 sgd_solver.cpp:106] Iteration 14820, lr = 0.01
I1209 15:48:42.472223  3198 solver.cpp:237] Iteration 14840, loss = 4.09503
I1209 15:48:42.472256  3198 solver.cpp:253]     Train net output #0: loss = 4.09503 (* 1 = 4.09503 loss)
I1209 15:48:42.472262  3198 sgd_solver.cpp:106] Iteration 14840, lr = 0.01
I1209 15:48:50.163995  3198 solver.cpp:237] Iteration 14860, loss = 3.88881
I1209 15:48:50.164028  3198 solver.cpp:253]     Train net output #0: loss = 3.88881 (* 1 = 3.88881 loss)
I1209 15:48:50.164034  3198 sgd_solver.cpp:106] Iteration 14860, lr = 0.01
I1209 15:48:57.834635  3198 solver.cpp:237] Iteration 14880, loss = 3.87902
I1209 15:48:57.834790  3198 solver.cpp:253]     Train net output #0: loss = 3.87902 (* 1 = 3.87902 loss)
I1209 15:48:57.834797  3198 sgd_solver.cpp:106] Iteration 14880, lr = 0.01
I1209 15:49:05.482194  3198 solver.cpp:237] Iteration 14900, loss = 3.829
I1209 15:49:05.482230  3198 solver.cpp:253]     Train net output #0: loss = 3.829 (* 1 = 3.829 loss)
I1209 15:49:05.482236  3198 sgd_solver.cpp:106] Iteration 14900, lr = 0.01
I1209 15:49:13.165758  3198 solver.cpp:237] Iteration 14920, loss = 4.04481
I1209 15:49:13.165796  3198 solver.cpp:253]     Train net output #0: loss = 4.04481 (* 1 = 4.04481 loss)
I1209 15:49:13.165801  3198 sgd_solver.cpp:106] Iteration 14920, lr = 0.01
I1209 15:49:20.925709  3198 solver.cpp:237] Iteration 14940, loss = 3.97581
I1209 15:49:20.925746  3198 solver.cpp:253]     Train net output #0: loss = 3.97581 (* 1 = 3.97581 loss)
I1209 15:49:20.925753  3198 sgd_solver.cpp:106] Iteration 14940, lr = 0.01
I1209 15:49:28.568069  3198 solver.cpp:237] Iteration 14960, loss = 3.90524
I1209 15:49:28.568231  3198 solver.cpp:253]     Train net output #0: loss = 3.90524 (* 1 = 3.90524 loss)
I1209 15:49:28.568239  3198 sgd_solver.cpp:106] Iteration 14960, lr = 0.01
I1209 15:49:36.270524  3198 solver.cpp:237] Iteration 14980, loss = 3.96803
I1209 15:49:36.270561  3198 solver.cpp:253]     Train net output #0: loss = 3.96803 (* 1 = 3.96803 loss)
I1209 15:49:36.270567  3198 sgd_solver.cpp:106] Iteration 14980, lr = 0.01
I1209 15:49:43.587141  3198 solver.cpp:341] Iteration 15000, Testing net (#0)
I1209 15:49:52.823765  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:51:01.148003  3198 solver.cpp:409]     Test net output #0: accuracy = 0.20316
I1209 15:51:01.148128  3198 solver.cpp:409]     Test net output #1: loss = 4.03823 (* 1 = 4.03823 loss)
I1209 15:51:01.345201  3198 solver.cpp:237] Iteration 15000, loss = 4.05352
I1209 15:51:01.345235  3198 solver.cpp:253]     Train net output #0: loss = 4.05352 (* 1 = 4.05352 loss)
I1209 15:51:01.345250  3198 sgd_solver.cpp:106] Iteration 15000, lr = 0.01
I1209 15:51:08.337119  3198 solver.cpp:237] Iteration 15020, loss = 4.06331
I1209 15:51:08.337157  3198 solver.cpp:253]     Train net output #0: loss = 4.06331 (* 1 = 4.06331 loss)
I1209 15:51:08.337162  3198 sgd_solver.cpp:106] Iteration 15020, lr = 0.01
I1209 15:51:15.911478  3198 solver.cpp:237] Iteration 15040, loss = 3.70641
I1209 15:51:15.911514  3198 solver.cpp:253]     Train net output #0: loss = 3.70641 (* 1 = 3.70641 loss)
I1209 15:51:15.911520  3198 sgd_solver.cpp:106] Iteration 15040, lr = 0.01
I1209 15:51:23.557724  3198 solver.cpp:237] Iteration 15060, loss = 3.85498
I1209 15:51:23.557761  3198 solver.cpp:253]     Train net output #0: loss = 3.85498 (* 1 = 3.85498 loss)
I1209 15:51:23.557767  3198 sgd_solver.cpp:106] Iteration 15060, lr = 0.01
I1209 15:51:31.216500  3198 solver.cpp:237] Iteration 15080, loss = 3.6863
I1209 15:51:31.216616  3198 solver.cpp:253]     Train net output #0: loss = 3.6863 (* 1 = 3.6863 loss)
I1209 15:51:31.216624  3198 sgd_solver.cpp:106] Iteration 15080, lr = 0.01
I1209 15:51:38.911885  3198 solver.cpp:237] Iteration 15100, loss = 3.92901
I1209 15:51:38.911921  3198 solver.cpp:253]     Train net output #0: loss = 3.92901 (* 1 = 3.92901 loss)
I1209 15:51:38.911926  3198 sgd_solver.cpp:106] Iteration 15100, lr = 0.01
I1209 15:51:46.574771  3198 solver.cpp:237] Iteration 15120, loss = 4.18442
I1209 15:51:46.574806  3198 solver.cpp:253]     Train net output #0: loss = 4.18442 (* 1 = 4.18442 loss)
I1209 15:51:46.574812  3198 sgd_solver.cpp:106] Iteration 15120, lr = 0.01
I1209 15:51:49.253700  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:51:54.206202  3198 solver.cpp:237] Iteration 15140, loss = 4.08667
I1209 15:51:54.206236  3198 solver.cpp:253]     Train net output #0: loss = 4.08667 (* 1 = 4.08667 loss)
I1209 15:51:54.206243  3198 sgd_solver.cpp:106] Iteration 15140, lr = 0.01
I1209 15:52:01.982748  3198 solver.cpp:237] Iteration 15160, loss = 3.80484
I1209 15:52:01.982869  3198 solver.cpp:253]     Train net output #0: loss = 3.80484 (* 1 = 3.80484 loss)
I1209 15:52:01.982884  3198 sgd_solver.cpp:106] Iteration 15160, lr = 0.01
I1209 15:52:09.728736  3198 solver.cpp:237] Iteration 15180, loss = 4.01594
I1209 15:52:09.728775  3198 solver.cpp:253]     Train net output #0: loss = 4.01594 (* 1 = 4.01594 loss)
I1209 15:52:09.728780  3198 sgd_solver.cpp:106] Iteration 15180, lr = 0.01
I1209 15:52:17.643385  3198 solver.cpp:237] Iteration 15200, loss = 3.8879
I1209 15:52:17.643420  3198 solver.cpp:253]     Train net output #0: loss = 3.8879 (* 1 = 3.8879 loss)
I1209 15:52:17.643429  3198 sgd_solver.cpp:106] Iteration 15200, lr = 0.01
I1209 15:52:25.547329  3198 solver.cpp:237] Iteration 15220, loss = 4.0239
I1209 15:52:25.547368  3198 solver.cpp:253]     Train net output #0: loss = 4.0239 (* 1 = 4.0239 loss)
I1209 15:52:25.547374  3198 sgd_solver.cpp:106] Iteration 15220, lr = 0.01
I1209 15:52:33.371961  3198 solver.cpp:237] Iteration 15240, loss = 3.68762
I1209 15:52:33.372077  3198 solver.cpp:253]     Train net output #0: loss = 3.68762 (* 1 = 3.68762 loss)
I1209 15:52:33.372084  3198 sgd_solver.cpp:106] Iteration 15240, lr = 0.01
I1209 15:52:41.158833  3198 solver.cpp:237] Iteration 15260, loss = 3.86426
I1209 15:52:41.158870  3198 solver.cpp:253]     Train net output #0: loss = 3.86426 (* 1 = 3.86426 loss)
I1209 15:52:41.158876  3198 sgd_solver.cpp:106] Iteration 15260, lr = 0.01
I1209 15:52:48.766672  3198 solver.cpp:237] Iteration 15280, loss = 3.81386
I1209 15:52:48.766705  3198 solver.cpp:253]     Train net output #0: loss = 3.81386 (* 1 = 3.81386 loss)
I1209 15:52:48.766712  3198 sgd_solver.cpp:106] Iteration 15280, lr = 0.01
I1209 15:52:56.414155  3198 solver.cpp:237] Iteration 15300, loss = 3.87421
I1209 15:52:56.414192  3198 solver.cpp:253]     Train net output #0: loss = 3.87421 (* 1 = 3.87421 loss)
I1209 15:52:56.414198  3198 sgd_solver.cpp:106] Iteration 15300, lr = 0.01
I1209 15:53:04.058899  3198 solver.cpp:237] Iteration 15320, loss = 4.11477
I1209 15:53:04.059052  3198 solver.cpp:253]     Train net output #0: loss = 4.11477 (* 1 = 4.11477 loss)
I1209 15:53:04.059061  3198 sgd_solver.cpp:106] Iteration 15320, lr = 0.01
I1209 15:53:11.750407  3198 solver.cpp:237] Iteration 15340, loss = 4.10316
I1209 15:53:11.750442  3198 solver.cpp:253]     Train net output #0: loss = 4.10316 (* 1 = 4.10316 loss)
I1209 15:53:11.750448  3198 sgd_solver.cpp:106] Iteration 15340, lr = 0.01
I1209 15:53:19.429471  3198 solver.cpp:237] Iteration 15360, loss = 3.93842
I1209 15:53:19.429508  3198 solver.cpp:253]     Train net output #0: loss = 3.93842 (* 1 = 3.93842 loss)
I1209 15:53:19.429515  3198 sgd_solver.cpp:106] Iteration 15360, lr = 0.01
I1209 15:53:27.085640  3198 solver.cpp:237] Iteration 15380, loss = 3.94752
I1209 15:53:27.085687  3198 solver.cpp:253]     Train net output #0: loss = 3.94752 (* 1 = 3.94752 loss)
I1209 15:53:27.085695  3198 sgd_solver.cpp:106] Iteration 15380, lr = 0.01
I1209 15:53:34.724802  3198 solver.cpp:237] Iteration 15400, loss = 3.85325
I1209 15:53:34.724922  3198 solver.cpp:253]     Train net output #0: loss = 3.85325 (* 1 = 3.85325 loss)
I1209 15:53:34.724930  3198 sgd_solver.cpp:106] Iteration 15400, lr = 0.01
I1209 15:53:42.633154  3198 solver.cpp:237] Iteration 15420, loss = 3.7914
I1209 15:53:42.633189  3198 solver.cpp:253]     Train net output #0: loss = 3.7914 (* 1 = 3.7914 loss)
I1209 15:53:42.633195  3198 sgd_solver.cpp:106] Iteration 15420, lr = 0.01
I1209 15:53:50.702087  3198 solver.cpp:237] Iteration 15440, loss = 3.73965
I1209 15:53:50.702180  3198 solver.cpp:253]     Train net output #0: loss = 3.73965 (* 1 = 3.73965 loss)
I1209 15:53:50.702198  3198 sgd_solver.cpp:106] Iteration 15440, lr = 0.01
I1209 15:53:58.520118  3198 solver.cpp:237] Iteration 15460, loss = 4.09657
I1209 15:53:58.520201  3198 solver.cpp:253]     Train net output #0: loss = 4.09657 (* 1 = 4.09657 loss)
I1209 15:53:58.520220  3198 sgd_solver.cpp:106] Iteration 15460, lr = 0.01
I1209 15:54:06.395432  3198 solver.cpp:237] Iteration 15480, loss = 3.9389
I1209 15:54:06.398464  3198 solver.cpp:253]     Train net output #0: loss = 3.9389 (* 1 = 3.9389 loss)
I1209 15:54:06.398484  3198 sgd_solver.cpp:106] Iteration 15480, lr = 0.01
I1209 15:54:14.330214  3198 solver.cpp:237] Iteration 15500, loss = 3.94167
I1209 15:54:14.330251  3198 solver.cpp:253]     Train net output #0: loss = 3.94167 (* 1 = 3.94167 loss)
I1209 15:54:14.330257  3198 sgd_solver.cpp:106] Iteration 15500, lr = 0.01
I1209 15:54:22.195238  3198 solver.cpp:237] Iteration 15520, loss = 3.8267
I1209 15:54:22.195276  3198 solver.cpp:253]     Train net output #0: loss = 3.8267 (* 1 = 3.8267 loss)
I1209 15:54:22.195281  3198 sgd_solver.cpp:106] Iteration 15520, lr = 0.01
I1209 15:54:29.872808  3198 solver.cpp:237] Iteration 15540, loss = 3.9186
I1209 15:54:29.872864  3198 solver.cpp:253]     Train net output #0: loss = 3.9186 (* 1 = 3.9186 loss)
I1209 15:54:29.872875  3198 sgd_solver.cpp:106] Iteration 15540, lr = 0.01
I1209 15:54:37.616118  3198 solver.cpp:237] Iteration 15560, loss = 4.01467
I1209 15:54:37.616224  3198 solver.cpp:253]     Train net output #0: loss = 4.01467 (* 1 = 4.01467 loss)
I1209 15:54:37.616230  3198 sgd_solver.cpp:106] Iteration 15560, lr = 0.01
I1209 15:54:45.443879  3198 solver.cpp:237] Iteration 15580, loss = 3.85635
I1209 15:54:45.443905  3198 solver.cpp:253]     Train net output #0: loss = 3.85635 (* 1 = 3.85635 loss)
I1209 15:54:45.443912  3198 sgd_solver.cpp:106] Iteration 15580, lr = 0.01
I1209 15:54:53.137197  3198 solver.cpp:237] Iteration 15600, loss = 3.98248
I1209 15:54:53.137234  3198 solver.cpp:253]     Train net output #0: loss = 3.98248 (* 1 = 3.98248 loss)
I1209 15:54:53.137240  3198 sgd_solver.cpp:106] Iteration 15600, lr = 0.01
I1209 15:55:00.840283  3198 solver.cpp:237] Iteration 15620, loss = 4.09784
I1209 15:55:00.840395  3198 solver.cpp:253]     Train net output #0: loss = 4.09784 (* 1 = 4.09784 loss)
I1209 15:55:00.840435  3198 sgd_solver.cpp:106] Iteration 15620, lr = 0.01
I1209 15:55:08.669564  3198 solver.cpp:237] Iteration 15640, loss = 3.72817
I1209 15:55:08.669718  3198 solver.cpp:253]     Train net output #0: loss = 3.72817 (* 1 = 3.72817 loss)
I1209 15:55:08.669725  3198 sgd_solver.cpp:106] Iteration 15640, lr = 0.01
I1209 15:55:16.241021  3198 solver.cpp:237] Iteration 15660, loss = 3.84567
I1209 15:55:16.241075  3198 solver.cpp:253]     Train net output #0: loss = 3.84567 (* 1 = 3.84567 loss)
I1209 15:55:16.241088  3198 sgd_solver.cpp:106] Iteration 15660, lr = 0.01
I1209 15:55:23.895969  3198 solver.cpp:237] Iteration 15680, loss = 3.73283
I1209 15:55:23.896005  3198 solver.cpp:253]     Train net output #0: loss = 3.73283 (* 1 = 3.73283 loss)
I1209 15:55:23.896011  3198 sgd_solver.cpp:106] Iteration 15680, lr = 0.01
I1209 15:55:31.611106  3198 solver.cpp:237] Iteration 15700, loss = 3.93973
I1209 15:55:31.611147  3198 solver.cpp:253]     Train net output #0: loss = 3.93973 (* 1 = 3.93973 loss)
I1209 15:55:31.611155  3198 sgd_solver.cpp:106] Iteration 15700, lr = 0.01
I1209 15:55:39.528987  3198 solver.cpp:237] Iteration 15720, loss = 3.93631
I1209 15:55:39.530818  3198 solver.cpp:253]     Train net output #0: loss = 3.93631 (* 1 = 3.93631 loss)
I1209 15:55:39.530838  3198 sgd_solver.cpp:106] Iteration 15720, lr = 0.01
I1209 15:55:47.479851  3198 solver.cpp:237] Iteration 15740, loss = 3.94634
I1209 15:55:47.479889  3198 solver.cpp:253]     Train net output #0: loss = 3.94634 (* 1 = 3.94634 loss)
I1209 15:55:47.479897  3198 sgd_solver.cpp:106] Iteration 15740, lr = 0.01
I1209 15:55:55.412657  3198 solver.cpp:237] Iteration 15760, loss = 3.83467
I1209 15:55:55.412693  3198 solver.cpp:253]     Train net output #0: loss = 3.83467 (* 1 = 3.83467 loss)
I1209 15:55:55.412699  3198 sgd_solver.cpp:106] Iteration 15760, lr = 0.01
I1209 15:56:03.208379  3198 solver.cpp:237] Iteration 15780, loss = 4.15873
I1209 15:56:03.208415  3198 solver.cpp:253]     Train net output #0: loss = 4.15873 (* 1 = 4.15873 loss)
I1209 15:56:03.208421  3198 sgd_solver.cpp:106] Iteration 15780, lr = 0.01
I1209 15:56:10.977929  3198 solver.cpp:237] Iteration 15800, loss = 3.76463
I1209 15:56:10.978086  3198 solver.cpp:253]     Train net output #0: loss = 3.76463 (* 1 = 3.76463 loss)
I1209 15:56:10.978104  3198 sgd_solver.cpp:106] Iteration 15800, lr = 0.01
I1209 15:56:18.733892  3198 solver.cpp:237] Iteration 15820, loss = 3.84639
I1209 15:56:18.733943  3198 solver.cpp:253]     Train net output #0: loss = 3.84639 (* 1 = 3.84639 loss)
I1209 15:56:18.733957  3198 sgd_solver.cpp:106] Iteration 15820, lr = 0.01
I1209 15:56:26.549165  3198 solver.cpp:237] Iteration 15840, loss = 3.85533
I1209 15:56:26.549202  3198 solver.cpp:253]     Train net output #0: loss = 3.85533 (* 1 = 3.85533 loss)
I1209 15:56:26.549208  3198 sgd_solver.cpp:106] Iteration 15840, lr = 0.01
I1209 15:56:34.327615  3198 solver.cpp:237] Iteration 15860, loss = 3.85361
I1209 15:56:34.327649  3198 solver.cpp:253]     Train net output #0: loss = 3.85361 (* 1 = 3.85361 loss)
I1209 15:56:34.327654  3198 sgd_solver.cpp:106] Iteration 15860, lr = 0.01
I1209 15:56:42.197671  3198 solver.cpp:237] Iteration 15880, loss = 3.92568
I1209 15:56:42.197837  3198 solver.cpp:253]     Train net output #0: loss = 3.92568 (* 1 = 3.92568 loss)
I1209 15:56:42.197844  3198 sgd_solver.cpp:106] Iteration 15880, lr = 0.01
I1209 15:56:50.147425  3198 solver.cpp:237] Iteration 15900, loss = 3.80722
I1209 15:56:50.147459  3198 solver.cpp:253]     Train net output #0: loss = 3.80722 (* 1 = 3.80722 loss)
I1209 15:56:50.147465  3198 sgd_solver.cpp:106] Iteration 15900, lr = 0.01
I1209 15:56:58.040460  3198 solver.cpp:237] Iteration 15920, loss = 3.83482
I1209 15:56:58.040498  3198 solver.cpp:253]     Train net output #0: loss = 3.83482 (* 1 = 3.83482 loss)
I1209 15:56:58.040505  3198 sgd_solver.cpp:106] Iteration 15920, lr = 0.01
I1209 15:57:05.823804  3198 solver.cpp:237] Iteration 15940, loss = 3.94045
I1209 15:57:05.823842  3198 solver.cpp:253]     Train net output #0: loss = 3.94045 (* 1 = 3.94045 loss)
I1209 15:57:05.823848  3198 sgd_solver.cpp:106] Iteration 15940, lr = 0.01
I1209 15:57:13.679805  3198 solver.cpp:237] Iteration 15960, loss = 4.00704
I1209 15:57:13.680071  3198 solver.cpp:253]     Train net output #0: loss = 4.00704 (* 1 = 4.00704 loss)
I1209 15:57:13.680093  3198 sgd_solver.cpp:106] Iteration 15960, lr = 0.01
I1209 15:57:21.364316  3198 solver.cpp:237] Iteration 15980, loss = 4.05261
I1209 15:57:21.364354  3198 solver.cpp:253]     Train net output #0: loss = 4.05261 (* 1 = 4.05261 loss)
I1209 15:57:21.364361  3198 sgd_solver.cpp:106] Iteration 15980, lr = 0.01
I1209 15:57:28.704715  3198 solver.cpp:341] Iteration 16000, Testing net (#0)
I1209 15:57:38.749373  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:58:46.864114  3198 solver.cpp:409]     Test net output #0: accuracy = 0.19232
I1209 15:58:46.864245  3198 solver.cpp:409]     Test net output #1: loss = 4.1367 (* 1 = 4.1367 loss)
I1209 15:58:47.061128  3198 solver.cpp:237] Iteration 16000, loss = 3.76697
I1209 15:58:47.061164  3198 solver.cpp:253]     Train net output #0: loss = 3.76697 (* 1 = 3.76697 loss)
I1209 15:58:47.061172  3198 sgd_solver.cpp:106] Iteration 16000, lr = 0.01
I1209 15:58:54.213601  3198 solver.cpp:237] Iteration 16020, loss = 3.94839
I1209 15:58:54.213639  3198 solver.cpp:253]     Train net output #0: loss = 3.94839 (* 1 = 3.94839 loss)
I1209 15:58:54.213644  3198 sgd_solver.cpp:106] Iteration 16020, lr = 0.01
I1209 15:59:01.849242  3198 solver.cpp:237] Iteration 16040, loss = 3.69707
I1209 15:59:01.849278  3198 solver.cpp:253]     Train net output #0: loss = 3.69707 (* 1 = 3.69707 loss)
I1209 15:59:01.849294  3198 sgd_solver.cpp:106] Iteration 16040, lr = 0.01
I1209 15:59:09.623829  3198 solver.cpp:237] Iteration 16060, loss = 3.96489
I1209 15:59:09.623865  3198 solver.cpp:253]     Train net output #0: loss = 3.96489 (* 1 = 3.96489 loss)
I1209 15:59:09.623872  3198 sgd_solver.cpp:106] Iteration 16060, lr = 0.01
I1209 15:59:17.462962  3198 solver.cpp:237] Iteration 16080, loss = 3.91751
I1209 15:59:17.463110  3198 solver.cpp:253]     Train net output #0: loss = 3.91751 (* 1 = 3.91751 loss)
I1209 15:59:17.463119  3198 sgd_solver.cpp:106] Iteration 16080, lr = 0.01
I1209 15:59:25.253190  3198 solver.cpp:237] Iteration 16100, loss = 3.93447
I1209 15:59:25.253226  3198 solver.cpp:253]     Train net output #0: loss = 3.93447 (* 1 = 3.93447 loss)
I1209 15:59:25.253232  3198 sgd_solver.cpp:106] Iteration 16100, lr = 0.01
I1209 15:59:33.030074  3198 solver.cpp:237] Iteration 16120, loss = 3.66227
I1209 15:59:33.030109  3198 solver.cpp:253]     Train net output #0: loss = 3.66227 (* 1 = 3.66227 loss)
I1209 15:59:33.030115  3198 sgd_solver.cpp:106] Iteration 16120, lr = 0.01
I1209 15:59:38.881062  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 15:59:40.805331  3198 solver.cpp:237] Iteration 16140, loss = 3.83791
I1209 15:59:40.805367  3198 solver.cpp:253]     Train net output #0: loss = 3.83791 (* 1 = 3.83791 loss)
I1209 15:59:40.805374  3198 sgd_solver.cpp:106] Iteration 16140, lr = 0.01
I1209 15:59:48.548909  3198 solver.cpp:237] Iteration 16160, loss = 3.6537
I1209 15:59:48.549059  3198 solver.cpp:253]     Train net output #0: loss = 3.6537 (* 1 = 3.6537 loss)
I1209 15:59:48.549067  3198 sgd_solver.cpp:106] Iteration 16160, lr = 0.01
I1209 15:59:56.272680  3198 solver.cpp:237] Iteration 16180, loss = 4.03067
I1209 15:59:56.272714  3198 solver.cpp:253]     Train net output #0: loss = 4.03067 (* 1 = 4.03067 loss)
I1209 15:59:56.272722  3198 sgd_solver.cpp:106] Iteration 16180, lr = 0.01
I1209 16:00:04.033903  3198 solver.cpp:237] Iteration 16200, loss = 3.82792
I1209 16:00:04.033943  3198 solver.cpp:253]     Train net output #0: loss = 3.82792 (* 1 = 3.82792 loss)
I1209 16:00:04.033951  3198 sgd_solver.cpp:106] Iteration 16200, lr = 0.01
I1209 16:00:11.736987  3198 solver.cpp:237] Iteration 16220, loss = 3.95815
I1209 16:00:11.737021  3198 solver.cpp:253]     Train net output #0: loss = 3.95815 (* 1 = 3.95815 loss)
I1209 16:00:11.737028  3198 sgd_solver.cpp:106] Iteration 16220, lr = 0.01
I1209 16:00:19.403877  3198 solver.cpp:237] Iteration 16240, loss = 3.91611
I1209 16:00:19.404058  3198 solver.cpp:253]     Train net output #0: loss = 3.91611 (* 1 = 3.91611 loss)
I1209 16:00:19.404067  3198 sgd_solver.cpp:106] Iteration 16240, lr = 0.01
I1209 16:00:27.145105  3198 solver.cpp:237] Iteration 16260, loss = 3.85732
I1209 16:00:27.145140  3198 solver.cpp:253]     Train net output #0: loss = 3.85732 (* 1 = 3.85732 loss)
I1209 16:00:27.145146  3198 sgd_solver.cpp:106] Iteration 16260, lr = 0.01
I1209 16:00:34.878134  3198 solver.cpp:237] Iteration 16280, loss = 3.89416
I1209 16:00:34.878168  3198 solver.cpp:253]     Train net output #0: loss = 3.89416 (* 1 = 3.89416 loss)
I1209 16:00:34.878175  3198 sgd_solver.cpp:106] Iteration 16280, lr = 0.01
I1209 16:00:42.598338  3198 solver.cpp:237] Iteration 16300, loss = 4.03577
I1209 16:00:42.598372  3198 solver.cpp:253]     Train net output #0: loss = 4.03577 (* 1 = 4.03577 loss)
I1209 16:00:42.598378  3198 sgd_solver.cpp:106] Iteration 16300, lr = 0.01
I1209 16:00:50.285346  3198 solver.cpp:237] Iteration 16320, loss = 4.02608
I1209 16:00:50.285465  3198 solver.cpp:253]     Train net output #0: loss = 4.02608 (* 1 = 4.02608 loss)
I1209 16:00:50.285471  3198 sgd_solver.cpp:106] Iteration 16320, lr = 0.01
I1209 16:00:58.026351  3198 solver.cpp:237] Iteration 16340, loss = 3.90926
I1209 16:00:58.026386  3198 solver.cpp:253]     Train net output #0: loss = 3.90926 (* 1 = 3.90926 loss)
I1209 16:00:58.026393  3198 sgd_solver.cpp:106] Iteration 16340, lr = 0.01
I1209 16:01:05.726205  3198 solver.cpp:237] Iteration 16360, loss = 3.82096
I1209 16:01:05.726243  3198 solver.cpp:253]     Train net output #0: loss = 3.82096 (* 1 = 3.82096 loss)
I1209 16:01:05.726249  3198 sgd_solver.cpp:106] Iteration 16360, lr = 0.01
I1209 16:01:13.608973  3198 solver.cpp:237] Iteration 16380, loss = 4.06487
I1209 16:01:13.609009  3198 solver.cpp:253]     Train net output #0: loss = 4.06487 (* 1 = 4.06487 loss)
I1209 16:01:13.609015  3198 sgd_solver.cpp:106] Iteration 16380, lr = 0.01
I1209 16:01:21.398342  3198 solver.cpp:237] Iteration 16400, loss = 3.8428
I1209 16:01:21.398509  3198 solver.cpp:253]     Train net output #0: loss = 3.8428 (* 1 = 3.8428 loss)
I1209 16:01:21.398517  3198 sgd_solver.cpp:106] Iteration 16400, lr = 0.01
I1209 16:01:29.228821  3198 solver.cpp:237] Iteration 16420, loss = 3.76069
I1209 16:01:29.228849  3198 solver.cpp:253]     Train net output #0: loss = 3.76069 (* 1 = 3.76069 loss)
I1209 16:01:29.228855  3198 sgd_solver.cpp:106] Iteration 16420, lr = 0.01
I1209 16:01:37.126487  3198 solver.cpp:237] Iteration 16440, loss = 3.74815
I1209 16:01:37.126523  3198 solver.cpp:253]     Train net output #0: loss = 3.74815 (* 1 = 3.74815 loss)
I1209 16:01:37.126539  3198 sgd_solver.cpp:106] Iteration 16440, lr = 0.01
I1209 16:01:45.296524  3198 solver.cpp:237] Iteration 16460, loss = 4.04344
I1209 16:01:45.296560  3198 solver.cpp:253]     Train net output #0: loss = 4.04344 (* 1 = 4.04344 loss)
I1209 16:01:45.296564  3198 sgd_solver.cpp:106] Iteration 16460, lr = 0.01
I1209 16:01:53.126760  3198 solver.cpp:237] Iteration 16480, loss = 3.8225
I1209 16:01:53.126910  3198 solver.cpp:253]     Train net output #0: loss = 3.8225 (* 1 = 3.8225 loss)
I1209 16:01:53.126917  3198 sgd_solver.cpp:106] Iteration 16480, lr = 0.01
I1209 16:02:00.905871  3198 solver.cpp:237] Iteration 16500, loss = 3.68232
I1209 16:02:00.905906  3198 solver.cpp:253]     Train net output #0: loss = 3.68232 (* 1 = 3.68232 loss)
I1209 16:02:00.905912  3198 sgd_solver.cpp:106] Iteration 16500, lr = 0.01
I1209 16:02:08.619865  3198 solver.cpp:237] Iteration 16520, loss = 3.91201
I1209 16:02:08.619902  3198 solver.cpp:253]     Train net output #0: loss = 3.91201 (* 1 = 3.91201 loss)
I1209 16:02:08.619909  3198 sgd_solver.cpp:106] Iteration 16520, lr = 0.01
I1209 16:02:16.421536  3198 solver.cpp:237] Iteration 16540, loss = 3.63384
I1209 16:02:16.421574  3198 solver.cpp:253]     Train net output #0: loss = 3.63384 (* 1 = 3.63384 loss)
I1209 16:02:16.421581  3198 sgd_solver.cpp:106] Iteration 16540, lr = 0.01
I1209 16:02:24.353138  3198 solver.cpp:237] Iteration 16560, loss = 3.57696
I1209 16:02:24.353255  3198 solver.cpp:253]     Train net output #0: loss = 3.57696 (* 1 = 3.57696 loss)
I1209 16:02:24.353266  3198 sgd_solver.cpp:106] Iteration 16560, lr = 0.01
I1209 16:02:32.009094  3198 solver.cpp:237] Iteration 16580, loss = 4.01834
I1209 16:02:32.009131  3198 solver.cpp:253]     Train net output #0: loss = 4.01834 (* 1 = 4.01834 loss)
I1209 16:02:32.009136  3198 sgd_solver.cpp:106] Iteration 16580, lr = 0.01
I1209 16:02:39.650534  3198 solver.cpp:237] Iteration 16600, loss = 3.85442
I1209 16:02:39.650571  3198 solver.cpp:253]     Train net output #0: loss = 3.85442 (* 1 = 3.85442 loss)
I1209 16:02:39.650578  3198 sgd_solver.cpp:106] Iteration 16600, lr = 0.01
I1209 16:02:47.390132  3198 solver.cpp:237] Iteration 16620, loss = 3.87692
I1209 16:02:47.390168  3198 solver.cpp:253]     Train net output #0: loss = 3.87692 (* 1 = 3.87692 loss)
I1209 16:02:47.390173  3198 sgd_solver.cpp:106] Iteration 16620, lr = 0.01
I1209 16:02:55.066504  3198 solver.cpp:237] Iteration 16640, loss = 4.07208
I1209 16:02:55.066609  3198 solver.cpp:253]     Train net output #0: loss = 4.07208 (* 1 = 4.07208 loss)
I1209 16:02:55.066617  3198 sgd_solver.cpp:106] Iteration 16640, lr = 0.01
I1209 16:03:02.775671  3198 solver.cpp:237] Iteration 16660, loss = 3.78364
I1209 16:03:02.775708  3198 solver.cpp:253]     Train net output #0: loss = 3.78364 (* 1 = 3.78364 loss)
I1209 16:03:02.775714  3198 sgd_solver.cpp:106] Iteration 16660, lr = 0.01
I1209 16:03:10.498587  3198 solver.cpp:237] Iteration 16680, loss = 3.89196
I1209 16:03:10.498633  3198 solver.cpp:253]     Train net output #0: loss = 3.89196 (* 1 = 3.89196 loss)
I1209 16:03:10.498639  3198 sgd_solver.cpp:106] Iteration 16680, lr = 0.01
I1209 16:03:18.334847  3198 solver.cpp:237] Iteration 16700, loss = 3.95163
I1209 16:03:18.334890  3198 solver.cpp:253]     Train net output #0: loss = 3.95163 (* 1 = 3.95163 loss)
I1209 16:03:18.334903  3198 sgd_solver.cpp:106] Iteration 16700, lr = 0.01
I1209 16:03:26.068086  3198 solver.cpp:237] Iteration 16720, loss = 3.86846
I1209 16:03:26.068243  3198 solver.cpp:253]     Train net output #0: loss = 3.86846 (* 1 = 3.86846 loss)
I1209 16:03:26.068251  3198 sgd_solver.cpp:106] Iteration 16720, lr = 0.01
I1209 16:03:33.807701  3198 solver.cpp:237] Iteration 16740, loss = 3.95728
I1209 16:03:33.807737  3198 solver.cpp:253]     Train net output #0: loss = 3.95728 (* 1 = 3.95728 loss)
I1209 16:03:33.807744  3198 sgd_solver.cpp:106] Iteration 16740, lr = 0.01
I1209 16:03:41.510367  3198 solver.cpp:237] Iteration 16760, loss = 4.2015
I1209 16:03:41.510404  3198 solver.cpp:253]     Train net output #0: loss = 4.2015 (* 1 = 4.2015 loss)
I1209 16:03:41.510409  3198 sgd_solver.cpp:106] Iteration 16760, lr = 0.01
I1209 16:03:49.192965  3198 solver.cpp:237] Iteration 16780, loss = 3.95292
I1209 16:03:49.193001  3198 solver.cpp:253]     Train net output #0: loss = 3.95292 (* 1 = 3.95292 loss)
I1209 16:03:49.193006  3198 sgd_solver.cpp:106] Iteration 16780, lr = 0.01
I1209 16:03:56.797624  3198 solver.cpp:237] Iteration 16800, loss = 4.13747
I1209 16:03:56.797780  3198 solver.cpp:253]     Train net output #0: loss = 4.13747 (* 1 = 4.13747 loss)
I1209 16:03:56.797788  3198 sgd_solver.cpp:106] Iteration 16800, lr = 0.01
I1209 16:04:04.467100  3198 solver.cpp:237] Iteration 16820, loss = 3.97116
I1209 16:04:04.467138  3198 solver.cpp:253]     Train net output #0: loss = 3.97116 (* 1 = 3.97116 loss)
I1209 16:04:04.467144  3198 sgd_solver.cpp:106] Iteration 16820, lr = 0.01
I1209 16:04:12.143986  3198 solver.cpp:237] Iteration 16840, loss = 3.70506
I1209 16:04:12.144023  3198 solver.cpp:253]     Train net output #0: loss = 3.70506 (* 1 = 3.70506 loss)
I1209 16:04:12.144029  3198 sgd_solver.cpp:106] Iteration 16840, lr = 0.01
I1209 16:04:19.802631  3198 solver.cpp:237] Iteration 16860, loss = 3.98795
I1209 16:04:19.802667  3198 solver.cpp:253]     Train net output #0: loss = 3.98795 (* 1 = 3.98795 loss)
I1209 16:04:19.802675  3198 sgd_solver.cpp:106] Iteration 16860, lr = 0.01
I1209 16:04:27.427083  3198 solver.cpp:237] Iteration 16880, loss = 3.81318
I1209 16:04:27.427212  3198 solver.cpp:253]     Train net output #0: loss = 3.81318 (* 1 = 3.81318 loss)
I1209 16:04:27.427227  3198 sgd_solver.cpp:106] Iteration 16880, lr = 0.01
I1209 16:04:35.136379  3198 solver.cpp:237] Iteration 16900, loss = 3.85713
I1209 16:04:35.136415  3198 solver.cpp:253]     Train net output #0: loss = 3.85713 (* 1 = 3.85713 loss)
I1209 16:04:35.136422  3198 sgd_solver.cpp:106] Iteration 16900, lr = 0.01
I1209 16:04:42.840939  3198 solver.cpp:237] Iteration 16920, loss = 3.90498
I1209 16:04:42.840984  3198 solver.cpp:253]     Train net output #0: loss = 3.90498 (* 1 = 3.90498 loss)
I1209 16:04:42.840991  3198 sgd_solver.cpp:106] Iteration 16920, lr = 0.01
I1209 16:04:50.553151  3198 solver.cpp:237] Iteration 16940, loss = 3.86862
I1209 16:04:50.553186  3198 solver.cpp:253]     Train net output #0: loss = 3.86862 (* 1 = 3.86862 loss)
I1209 16:04:50.553191  3198 sgd_solver.cpp:106] Iteration 16940, lr = 0.01
I1209 16:04:58.364624  3198 solver.cpp:237] Iteration 16960, loss = 3.76261
I1209 16:04:58.364780  3198 solver.cpp:253]     Train net output #0: loss = 3.76261 (* 1 = 3.76261 loss)
I1209 16:04:58.364789  3198 sgd_solver.cpp:106] Iteration 16960, lr = 0.01
I1209 16:05:06.144567  3198 solver.cpp:237] Iteration 16980, loss = 3.78071
I1209 16:05:06.144603  3198 solver.cpp:253]     Train net output #0: loss = 3.78071 (* 1 = 3.78071 loss)
I1209 16:05:06.144609  3198 sgd_solver.cpp:106] Iteration 16980, lr = 0.01
I1209 16:05:13.544699  3198 solver.cpp:341] Iteration 17000, Testing net (#0)
I1209 16:05:24.141679  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 16:06:31.768470  3198 solver.cpp:409]     Test net output #0: accuracy = 0.2172
I1209 16:06:31.768626  3198 solver.cpp:409]     Test net output #1: loss = 3.941 (* 1 = 3.941 loss)
I1209 16:06:31.963860  3198 solver.cpp:237] Iteration 17000, loss = 3.95176
I1209 16:06:31.963894  3198 solver.cpp:253]     Train net output #0: loss = 3.95176 (* 1 = 3.95176 loss)
I1209 16:06:31.963901  3198 sgd_solver.cpp:106] Iteration 17000, lr = 0.01
I1209 16:06:39.191030  3198 solver.cpp:237] Iteration 17020, loss = 4.05567
I1209 16:06:39.191066  3198 solver.cpp:253]     Train net output #0: loss = 4.05567 (* 1 = 4.05567 loss)
I1209 16:06:39.191071  3198 sgd_solver.cpp:106] Iteration 17020, lr = 0.01
I1209 16:06:47.018143  3198 solver.cpp:237] Iteration 17040, loss = 3.88753
I1209 16:06:47.018179  3198 solver.cpp:253]     Train net output #0: loss = 3.88753 (* 1 = 3.88753 loss)
I1209 16:06:47.018187  3198 sgd_solver.cpp:106] Iteration 17040, lr = 0.01
I1209 16:06:54.804033  3198 solver.cpp:237] Iteration 17060, loss = 3.72101
I1209 16:06:54.804069  3198 solver.cpp:253]     Train net output #0: loss = 3.72101 (* 1 = 3.72101 loss)
I1209 16:06:54.804075  3198 sgd_solver.cpp:106] Iteration 17060, lr = 0.01
I1209 16:07:02.441450  3198 solver.cpp:237] Iteration 17080, loss = 3.71383
I1209 16:07:02.441565  3198 solver.cpp:253]     Train net output #0: loss = 3.71383 (* 1 = 3.71383 loss)
I1209 16:07:02.441571  3198 sgd_solver.cpp:106] Iteration 17080, lr = 0.01
I1209 16:07:10.148740  3198 solver.cpp:237] Iteration 17100, loss = 3.96248
I1209 16:07:10.148775  3198 solver.cpp:253]     Train net output #0: loss = 3.96248 (* 1 = 3.96248 loss)
I1209 16:07:10.148780  3198 sgd_solver.cpp:106] Iteration 17100, lr = 0.01
I1209 16:07:17.838835  3198 solver.cpp:237] Iteration 17120, loss = 3.84067
I1209 16:07:17.838915  3198 solver.cpp:253]     Train net output #0: loss = 3.84067 (* 1 = 3.84067 loss)
I1209 16:07:17.838930  3198 sgd_solver.cpp:106] Iteration 17120, lr = 0.01
I1209 16:07:25.545613  3198 solver.cpp:237] Iteration 17140, loss = 3.90765
I1209 16:07:25.545649  3198 solver.cpp:253]     Train net output #0: loss = 3.90765 (* 1 = 3.90765 loss)
I1209 16:07:25.545655  3198 sgd_solver.cpp:106] Iteration 17140, lr = 0.01
I1209 16:07:26.724216  3198 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 16:07:33.212324  3198 solver.cpp:237] Iteration 17160, loss = 3.68197
I1209 16:07:33.212505  3198 solver.cpp:253]     Train net output #0: loss = 3.68197 (* 1 = 3.68197 loss)
I1209 16:07:33.212512  3198 sgd_solver.cpp:106] Iteration 17160, lr = 0.01
I1209 16:07:40.824272  3198 solver.cpp:237] Iteration 17180, loss = 3.96372
I1209 16:07:40.824307  3198 solver.cpp:253]     Train net output #0: loss = 3.96372 (* 1 = 3.96372 loss)
I1209 16:07:40.824323  3198 sgd_solver.cpp:106] Iteration 17180, lr = 0.01
I1209 16:07:48.507189  3198 solver.cpp:237] Iteration 17200, loss = 3.71564
I1209 16:07:48.507222  3198 solver.cpp:253]     Train net output #0: loss = 3.71564 (* 1 = 3.71564 loss)
I1209 16:07:48.507228  3198 sgd_solver.cpp:106] Iteration 17200, lr = 0.01
I1209 16:07:56.295406  3198 solver.cpp:237] Iteration 17220, loss = 3.77719
I1209 16:07:56.295460  3198 solver.cpp:253]     Train net output #0: loss = 3.77719 (* 1 = 3.77719 loss)
I1209 16:07:56.295472  3198 sgd_solver.cpp:106] Iteration 17220, lr = 0.01
I1209 16:08:03.978090  3198 solver.cpp:237] Iteration 17240, loss = 4.11519
I1209 16:08:03.978193  3198 solver.cpp:253]     Train net output #0: loss = 4.11519 (* 1 = 4.11519 loss)
I1209 16:08:03.978206  3198 sgd_solver.cpp:106] Iteration 17240, lr = 0.01
I1209 16:08:11.754724  3198 solver.cpp:237] Iteration 17260, loss = 3.81747
I1209 16:08:11.754761  3198 solver.cpp:253]     Train net output #0: loss = 3.81747 (* 1 = 3.81747 loss)
I1209 16:08:11.754768  3198 sgd_solver.cpp:106] Iteration 17260, lr = 0.01
I1209 16:08:19.522011  3198 solver.cpp:237] Iteration 17280, loss = 3.70533
I1209 16:08:19.522050  3198 solver.cpp:253]     Train net output #0: loss = 3.70533 (* 1 = 3.70533 loss)
I1209 16:08:19.522056  3198 sgd_solver.cpp:106] Iteration 17280, lr = 0.01
I1209 16:08:27.355432  3198 solver.cpp:237] Iteration 17300, loss = 3.90142
I1209 16:08:27.355466  3198 solver.cpp:253]     Train net output #0: loss = 3.90142 (* 1 = 3.90142 loss)
I1209 16:08:27.355473  3198 sgd_solver.cpp:106] Iteration 17300, lr = 0.01
I1209 16:08:35.092541  3198 solver.cpp:237] Iteration 17320, loss = 3.7088
I1209 16:08:35.094987  3198 solver.cpp:253]     Train net output #0: loss = 3.7088 (* 1 = 3.7088 loss)
I1209 16:08:35.095010  3198 sgd_solver.cpp:106] Iteration 17320, lr = 0.01
I1209 16:08:42.884243  3198 solver.cpp:237] Iteration 17340, loss = 3.92678
I1209 16:08:42.884294  3198 solver.cpp:253]     Train net output #0: loss = 3.92678 (* 1 = 3.92678 loss)
I1209 16:08:42.884306  3198 sgd_solver.cpp:106] Iteration 17340, lr = 0.01
I1209 16:08:50.624424  3198 solver.cpp:237] Iteration 17360, loss = 3.71995
I1209 16:08:50.624460  3198 solver.cpp:253]     Train net output #0: loss = 3.71995 (* 1 = 3.71995 loss)
I1209 16:08:50.624467  3198 sgd_solver.cpp:106] Iteration 17360, lr = 0.01
I1209 16:08:58.415482  3198 solver.cpp:237] Iteration 17380, loss = 3.71902
I1209 16:08:58.415530  3198 solver.cpp:253]     Train net output #0: loss = 3.71902 (* 1 = 3.71902 loss)
I1209 16:08:58.415544  3198 sgd_solver.cpp:106] Iteration 17380, lr = 0.01
I1209 16:09:06.237474  3198 solver.cpp:237] Iteration 17400, loss = 4.0713
I1209 16:09:06.237632  3198 solver.cpp:253]     Train net output #0: loss = 4.0713 (* 1 = 4.0713 loss)
I1209 16:09:06.237642  3198 sgd_solver.cpp:106] Iteration 17400, lr = 0.01
I1209 16:09:13.897796  3198 solver.cpp:237] Iteration 17420, loss = 3.97211
I1209 16:09:13.897846  3198 solver.cpp:253]     Train net output #0: loss = 3.97211 (* 1 = 3.97211 loss)
I1209 16:09:13.897858  3198 sgd_solver.cpp:106] Iteration 17420, lr = 0.01
I1209 16:09:21.679733  3198 solver.cpp:237] Iteration 17440, loss = 3.80431
I1209 16:09:21.679774  3198 solver.cpp:253]     Train net output #0: loss = 3.80431 (* 1 = 3.80431 loss)
I1209 16:09:21.679780  3198 sgd_solver.cpp:106] Iteration 17440, lr = 0.01
I1209 16:09:29.492600  3198 solver.cpp:237] Iteration 17460, loss = 3.92561
I1209 16:09:29.492645  3198 solver.cpp:253]     Train net output #0: loss = 3.92561 (* 1 = 3.92561 loss)
I1209 16:09:29.492658  3198 sgd_solver.cpp:106] Iteration 17460, lr = 0.01
I1209 16:09:37.325137  3198 solver.cpp:237] Iteration 17480, loss = 3.90743
I1209 16:09:37.325295  3198 solver.cpp:253]     Train net output #0: loss = 3.90743 (* 1 = 3.90743 loss)
I1209 16:09:37.325302  3198 sgd_solver.cpp:106] Iteration 17480, lr = 0.01
I1209 16:09:45.035305  3198 solver.cpp:237] Iteration 17500, loss = 3.79601
I1209 16:09:45.035341  3198 solver.cpp:253]     Train net output #0: loss = 3.79601 (* 1 = 3.79601 loss)
I1209 16:09:45.035347  3198 sgd_solver.cpp:106] Iteration 17500, lr = 0.01
I1209 16:09:52.744595  3198 solver.cpp:237] Iteration 17520, loss = 3.7118
I1209 16:09:52.744631  3198 solver.cpp:253]     Train net output #0: loss = 3.7118 (* 1 = 3.7118 loss)
I1209 16:09:52.744637  3198 sgd_solver.cpp:106] Iteration 17520, lr = 0.01
I1209 16:10:00.561221  3198 solver.cpp:237] Iteration 17540, loss = 3.80858
I1209 16:10:00.561285  3198 solver.cpp:253]     Train net output #0: loss = 3.80858 (* 1 = 3.80858 loss)
I1209 16:10:00.561298  3198 sgd_solver.cpp:106] Iteration 17540, lr = 0.01
I1209 16:10:08.315837  3198 solver.cpp:237] Iteration 17560, loss = 3.98769
I1209 16:10:08.316056  3198 solver.cpp:253]     Train net output #0: loss = 3.98769 (* 1 = 3.98769 loss)
I1209 16:10:08.316066  3198 sgd_solver.cpp:106] Iteration 17560, lr = 0.01
I1209 16:10:15.945865  3198 solver.cpp:237] Iteration 17580, loss = 3.81114
I1209 16:10:15.945902  3198 solver.cpp:253]     Train net output #0: loss = 3.81114 (* 1 = 3.81114 loss)
I1209 16:10:15.945909  3198 sgd_solver.cpp:106] Iteration 17580, lr = 0.01
I1209 16:10:23.567086  3198 solver.cpp:237] Iteration 17600, loss = 3.73042
I1209 16:10:23.567122  3198 solver.cpp:253]     Train net output #0: loss = 3.73042 (* 1 = 3.73042 loss)
I1209 16:10:23.567128  3198 sgd_solver.cpp:106] Iteration 17600, lr = 0.01
I1209 16:10:31.248865  3198 solver.cpp:237] Iteration 17620, loss = 4.05033
I1209 16:10:31.248903  3198 solver.cpp:253]     Train net output #0: loss = 4.05033 (* 1 = 4.05033 loss)
I1209 16:10:31.248909  3198 sgd_solver.cpp:106] Iteration 17620, lr = 0.01
I1209 16:10:39.032832  3198 solver.cpp:237] Iteration 17640, loss = 3.65593
I1209 16:10:39.032976  3198 solver.cpp:253]     Train net output #0: loss = 3.65593 (* 1 = 3.65593 loss)
I1209 16:10:39.032984  3198 sgd_solver.cpp:106] Iteration 17640, lr = 0.01
I1209 16:10:46.913413  3198 solver.cpp:237] Iteration 17660, loss = 3.75301
I1209 16:10:46.913445  3198 solver.cpp:253]     Train net output #0: loss = 3.75301 (* 1 = 3.75301 loss)
I1209 16:10:46.913450  3198 sgd_solver.cpp:106] Iteration 17660, lr = 0.01
I1209 16:10:54.942193  3198 solver.cpp:237] Iteration 17680, loss = 3.96152
I1209 16:10:54.942246  3198 solver.cpp:253]     Train net output #0: loss = 3.96152 (* 1 = 3.96152 loss)
I1209 16:10:54.942260  3198 sgd_solver.cpp:106] Iteration 17680, lr = 0.01
I1209 16:11:02.841393  3198 solver.cpp:237] Iteration 17700, loss = 3.55292
I1209 16:11:02.841428  3198 solver.cpp:253]     Train net output #0: loss = 3.55292 (* 1 = 3.55292 loss)
I1209 16:11:02.841434  3198 sgd_solver.cpp:106] Iteration 17700, lr = 0.01
I1209 16:11:10.715035  3198 solver.cpp:237] Iteration 17720, loss = 3.99858
I1209 16:11:10.715183  3198 solver.cpp:253]     Train net output #0: loss = 3.99858 (* 1 = 3.99858 loss)
I1209 16:11:10.715189  3198 sgd_solver.cpp:106] Iteration 17720, lr = 0.01
