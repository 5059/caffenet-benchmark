I1209 13:06:30.545509  4871 upgrade_proto.cpp:990] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': ./caffenet128_lsuv_no_lrn_BatchNormBeforeReLU_EltwiseAffine.prototxt
I1209 13:06:30.545655  4871 upgrade_proto.cpp:997] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1209 13:06:30.545657  4871 upgrade_proto.cpp:999] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1209 13:06:30.545718  4871 caffe.cpp:184] Using GPUs 0
I1209 13:06:30.673408  4871 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.01
display: 20
max_iter: 320000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 10000
snapshot_prefix: "snapshots/caffenet128_no_lrn_lsuv_bn_before_ea"
solver_mode: GPU
device_id: 0
net_param {
  name: "CaffeNet"
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TRAIN
    }
    transform_param {
      scale: 0.04
      mirror: true
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 124
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: CUBIC
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/home/share/storage/datasets/imagenet/lmdb/ilsvrc12_train_lmdb"
      batch_size: 256
      backend: LMDB
    }
  }
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TEST
    }
    transform_param {
      scale: 0.04
      mirror: false
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 123
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: CUBIC
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/home/share/storage/datasets/imagenet/lmdb/ilsvrc12_val_lmdb"
      batch_size: 50
      backend: LMDB
    }
  }
  layer {
    name: "conv1"
    type: "Convolution"
    bottom: "data"
    top: "conv1"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 96
      kernel_size: 11
      stride: 4
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "conv1_BN"
    type: "BatchNorm"
    bottom: "conv1"
    top: "conv1_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "conv1_BN"
    type: "BatchNorm"
    bottom: "conv1"
    top: "conv1_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN1"
    type: "EltwiseAffine"
    bottom: "conv1_BN"
    top: "conv1_BN_ea"
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu1"
    type: "ReLU"
    bottom: "conv1_BN_ea"
    top: "relu1"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "relu1"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv2"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 2
      kernel_size: 5
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "conv2_BN"
    type: "BatchNorm"
    bottom: "conv2"
    top: "conv2_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "conv2_BN"
    type: "BatchNorm"
    bottom: "conv2"
    top: "conv2_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN2"
    type: "EltwiseAffine"
    bottom: "conv2_BN"
    top: "conv2_BN_ea"
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu2"
    type: "ReLU"
    bottom: "conv2_BN_ea"
    top: "relu2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "relu2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv3"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "conv3_BN"
    type: "BatchNorm"
    bottom: "conv3"
    top: "conv3_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "conv3_BN"
    type: "BatchNorm"
    bottom: "conv3"
    top: "conv3_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN3"
    type: "EltwiseAffine"
    bottom: "conv3_BN"
    top: "conv3_BN_ea"
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu3"
    type: "ReLU"
    bottom: "conv3_BN_ea"
    top: "relu3"
  }
  layer {
    name: "conv4"
    type: "Convolution"
    bottom: "relu3"
    top: "conv4"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "conv4_BN"
    type: "BatchNorm"
    bottom: "conv4"
    top: "conv4_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "conv4_BN"
    type: "BatchNorm"
    bottom: "conv4"
    top: "conv4_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN4"
    type: "EltwiseAffine"
    bottom: "conv4_BN"
    top: "conv4_BN_ea"
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu4"
    type: "ReLU"
    bottom: "conv4_BN_ea"
    top: "relu4"
  }
  layer {
    name: "conv5"
    type: "Convolution"
    bottom: "relu4"
    top: "conv5"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "conv5_BN"
    type: "BatchNorm"
    bottom: "conv5"
    top: "conv5_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "conv5_BN"
    type: "BatchNorm"
    bottom: "conv5"
    top: "conv5_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN5"
    type: "EltwiseAffine"
    bottom: "conv5_BN"
    top: "conv5_BN_ea"
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu5"
    type: "ReLU"
    bottom: "conv5_BN_ea"
    top: "relu5"
  }
  layer {
    name: "pool5"
    type: "Pooling"
    bottom: "relu5"
    top: "pool5"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "pool5"
    top: "fc6"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "fc6_BN"
    type: "BatchNorm"
    bottom: "fc6"
    top: "fc6_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "fc6_BN"
    type: "BatchNorm"
    bottom: "fc6"
    top: "fc6_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN6"
    type: "EltwiseAffine"
    bottom: "fc6_BN"
    top: "fc6_BN_ea"
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu6"
    type: "ReLU"
    bottom: "fc6_BN_ea"
    top: "relu6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "relu6"
    top: "relu6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "relu6"
    top: "fc7"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "fc7_BN"
    type: "BatchNorm"
    bottom: "fc7"
    top: "fc7_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TRAIN
    }
    batch_norm_param {
      use_global_stats: false
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "fc7_BN"
    type: "BatchNorm"
    bottom: "fc7"
    top: "fc7_BN"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    include {
      phase: TEST
    }
    batch_norm_param {
      use_global_stats: true
      moving_average_fraction: 0.95
    }
  }
  layer {
    name: "ea_BN7"
    type: "EltwiseAffine"
    bottom: "fc7_BN"
    top: "fc7_BN_ea"
    eltwise_affine_param {
      slope_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0.0001
      }
    }
  }
  layer {
    name: "relu7"
    type: "ReLU"
    bottom: "fc7_BN_ea"
    top: "relu7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "relu7"
    top: "relu7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc8"
    type: "InnerProduct"
    bottom: "relu7"
    top: "fc8"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 1000
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "accuracy"
    type: "Accuracy"
    bottom: "fc8"
    bottom: "label"
    top: "accuracy"
    include {
      phase: TEST
    }
  }
  layer {
    name: "loss"
    type: "SoftmaxWithLoss"
    bottom: "fc8"
    bottom: "label"
    top: "loss"
  }
}
iter_size: 1
type: "SGD"
I1209 13:06:30.673611  4871 solver.cpp:86] Creating training net specified in net_param.
I1209 13:06:30.673712  4871 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1209 13:06:30.673717  4871 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer conv1_BN
I1209 13:06:30.673722  4871 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer conv2_BN
I1209 13:06:30.673725  4871 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer conv3_BN
I1209 13:06:30.673728  4871 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer conv4_BN
I1209 13:06:30.673732  4871 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer conv5_BN
I1209 13:06:30.673735  4871 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer fc6_BN
I1209 13:06:30.673738  4871 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer fc7_BN
I1209 13:06:30.673741  4871 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1209 13:06:30.673909  4871 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.04
    mirror: true
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 124
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: CUBIC
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/share/storage/datasets/imagenet/lmdb/ilsvrc12_train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_BN"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN1"
  type: "EltwiseAffine"
  bottom: "conv1_BN"
  top: "conv1_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1_BN_ea"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv2_BN"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN2"
  type: "EltwiseAffine"
  bottom: "conv2_BN"
  top: "conv2_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2_BN_ea"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv3_BN"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN3"
  type: "EltwiseAffine"
  bottom: "conv3_BN"
  top: "conv3_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3_BN_ea"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv4_BN"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN4"
  type: "EltwiseAffine"
  bottom: "conv4_BN"
  top: "conv4_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4_BN_ea"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv5_BN"
  type: "BatchNorm"
  bottom: "conv5"
  top: "conv5_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN5"
  type: "EltwiseAffine"
  bottom: "conv5_BN"
  top: "conv5_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5_BN_ea"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "fc6_BN"
  type: "BatchNorm"
  bottom: "fc6"
  top: "fc6_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN6"
  type: "EltwiseAffine"
  bottom: "fc6_BN"
  top: "fc6_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6_BN_ea"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "relu6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "relu6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "fc7_BN"
  type: "BatchNorm"
  bottom: "fc7"
  top: "fc7_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN7"
  type: "EltwiseAffine"
  bottom: "fc7_BN"
  top: "fc7_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7_BN_ea"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "relu7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "relu7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1209 13:06:30.674028  4871 layer_factory.hpp:76] Creating layer data
I1209 13:06:30.674513  4871 net.cpp:106] Creating Layer data
I1209 13:06:30.674530  4871 net.cpp:411] data -> data
I1209 13:06:30.674552  4871 net.cpp:411] data -> label
I1209 13:06:30.675228  4875 db_lmdb.cpp:38] Opened lmdb /home/share/storage/datasets/imagenet/lmdb/ilsvrc12_train_lmdb
I1209 13:06:30.684806  4871 data_layer.cpp:41] output data size: 256,3,128,128
I1209 13:06:30.761571  4871 net.cpp:150] Setting up data
I1209 13:06:30.761610  4871 net.cpp:157] Top shape: 256 3 128 128 (12582912)
I1209 13:06:30.761613  4871 net.cpp:157] Top shape: 256 (256)
I1209 13:06:30.761615  4871 net.cpp:165] Memory required for data: 50332672
I1209 13:06:30.761632  4871 layer_factory.hpp:76] Creating layer conv1
I1209 13:06:30.761649  4871 net.cpp:106] Creating Layer conv1
I1209 13:06:30.761654  4871 net.cpp:454] conv1 <- data
I1209 13:06:30.761665  4871 net.cpp:411] conv1 -> conv1
I1209 13:06:30.895890  4871 net.cpp:150] Setting up conv1
I1209 13:06:30.895922  4871 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I1209 13:06:30.895925  4871 net.cpp:165] Memory required for data: 138806272
I1209 13:06:30.895941  4871 layer_factory.hpp:76] Creating layer conv1_BN
I1209 13:06:30.895963  4871 net.cpp:106] Creating Layer conv1_BN
I1209 13:06:30.895967  4871 net.cpp:454] conv1_BN <- conv1
I1209 13:06:30.895972  4871 net.cpp:411] conv1_BN -> conv1_BN
I1209 13:06:30.896123  4871 net.cpp:150] Setting up conv1_BN
I1209 13:06:30.896128  4871 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I1209 13:06:30.896141  4871 net.cpp:165] Memory required for data: 227279872
I1209 13:06:30.896147  4871 layer_factory.hpp:76] Creating layer ea_BN1
I1209 13:06:30.896153  4871 net.cpp:106] Creating Layer ea_BN1
I1209 13:06:30.896165  4871 net.cpp:454] ea_BN1 <- conv1_BN
I1209 13:06:30.896168  4871 net.cpp:411] ea_BN1 -> conv1_BN_ea
I1209 13:06:30.896694  4871 net.cpp:150] Setting up ea_BN1
I1209 13:06:30.896703  4871 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I1209 13:06:30.896714  4871 net.cpp:165] Memory required for data: 315753472
I1209 13:06:30.896718  4871 layer_factory.hpp:76] Creating layer relu1
I1209 13:06:30.896724  4871 net.cpp:106] Creating Layer relu1
I1209 13:06:30.896726  4871 net.cpp:454] relu1 <- conv1_BN_ea
I1209 13:06:30.896739  4871 net.cpp:411] relu1 -> relu1
I1209 13:06:30.896890  4871 net.cpp:150] Setting up relu1
I1209 13:06:30.896896  4871 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I1209 13:06:30.896908  4871 net.cpp:165] Memory required for data: 404227072
I1209 13:06:30.896910  4871 layer_factory.hpp:76] Creating layer pool1
I1209 13:06:30.896914  4871 net.cpp:106] Creating Layer pool1
I1209 13:06:30.896916  4871 net.cpp:454] pool1 <- relu1
I1209 13:06:30.896919  4871 net.cpp:411] pool1 -> pool1
I1209 13:06:30.897186  4871 net.cpp:150] Setting up pool1
I1209 13:06:30.897192  4871 net.cpp:157] Top shape: 256 96 15 15 (5529600)
I1209 13:06:30.897204  4871 net.cpp:165] Memory required for data: 426345472
I1209 13:06:30.897207  4871 layer_factory.hpp:76] Creating layer conv2
I1209 13:06:30.897213  4871 net.cpp:106] Creating Layer conv2
I1209 13:06:30.897215  4871 net.cpp:454] conv2 <- pool1
I1209 13:06:30.897228  4871 net.cpp:411] conv2 -> conv2
I1209 13:06:30.905407  4871 net.cpp:150] Setting up conv2
I1209 13:06:30.905431  4871 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I1209 13:06:30.905433  4871 net.cpp:165] Memory required for data: 485327872
I1209 13:06:30.905441  4871 layer_factory.hpp:76] Creating layer conv2_BN
I1209 13:06:30.905447  4871 net.cpp:106] Creating Layer conv2_BN
I1209 13:06:30.905478  4871 net.cpp:454] conv2_BN <- conv2
I1209 13:06:30.905483  4871 net.cpp:411] conv2_BN -> conv2_BN
I1209 13:06:30.905627  4871 net.cpp:150] Setting up conv2_BN
I1209 13:06:30.905642  4871 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I1209 13:06:30.905645  4871 net.cpp:165] Memory required for data: 544310272
I1209 13:06:30.905650  4871 layer_factory.hpp:76] Creating layer ea_BN2
I1209 13:06:30.905657  4871 net.cpp:106] Creating Layer ea_BN2
I1209 13:06:30.905668  4871 net.cpp:454] ea_BN2 <- conv2_BN
I1209 13:06:30.905671  4871 net.cpp:411] ea_BN2 -> conv2_BN_ea
I1209 13:06:30.905812  4871 net.cpp:150] Setting up ea_BN2
I1209 13:06:30.905817  4871 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I1209 13:06:30.905818  4871 net.cpp:165] Memory required for data: 603292672
I1209 13:06:30.905832  4871 layer_factory.hpp:76] Creating layer relu2
I1209 13:06:30.905835  4871 net.cpp:106] Creating Layer relu2
I1209 13:06:30.905838  4871 net.cpp:454] relu2 <- conv2_BN_ea
I1209 13:06:30.905840  4871 net.cpp:411] relu2 -> relu2
I1209 13:06:30.906100  4871 net.cpp:150] Setting up relu2
I1209 13:06:30.906107  4871 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I1209 13:06:30.906118  4871 net.cpp:165] Memory required for data: 662275072
I1209 13:06:30.906121  4871 layer_factory.hpp:76] Creating layer pool2
I1209 13:06:30.906126  4871 net.cpp:106] Creating Layer pool2
I1209 13:06:30.906127  4871 net.cpp:454] pool2 <- relu2
I1209 13:06:30.906131  4871 net.cpp:411] pool2 -> pool2
I1209 13:06:30.906378  4871 net.cpp:150] Setting up pool2
I1209 13:06:30.906385  4871 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I1209 13:06:30.906396  4871 net.cpp:165] Memory required for data: 675120128
I1209 13:06:30.906399  4871 layer_factory.hpp:76] Creating layer conv3
I1209 13:06:30.906405  4871 net.cpp:106] Creating Layer conv3
I1209 13:06:30.906407  4871 net.cpp:454] conv3 <- pool2
I1209 13:06:30.906412  4871 net.cpp:411] conv3 -> conv3
I1209 13:06:30.928617  4871 net.cpp:150] Setting up conv3
I1209 13:06:30.928643  4871 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:06:30.928645  4871 net.cpp:165] Memory required for data: 694387712
I1209 13:06:30.928653  4871 layer_factory.hpp:76] Creating layer conv3_BN
I1209 13:06:30.928671  4871 net.cpp:106] Creating Layer conv3_BN
I1209 13:06:30.928674  4871 net.cpp:454] conv3_BN <- conv3
I1209 13:06:30.928679  4871 net.cpp:411] conv3_BN -> conv3_BN
I1209 13:06:30.928827  4871 net.cpp:150] Setting up conv3_BN
I1209 13:06:30.928831  4871 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:06:30.928833  4871 net.cpp:165] Memory required for data: 713655296
I1209 13:06:30.928849  4871 layer_factory.hpp:76] Creating layer ea_BN3
I1209 13:06:30.928855  4871 net.cpp:106] Creating Layer ea_BN3
I1209 13:06:30.928867  4871 net.cpp:454] ea_BN3 <- conv3_BN
I1209 13:06:30.928870  4871 net.cpp:411] ea_BN3 -> conv3_BN_ea
I1209 13:06:30.928982  4871 net.cpp:150] Setting up ea_BN3
I1209 13:06:30.928987  4871 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:06:30.928987  4871 net.cpp:165] Memory required for data: 732922880
I1209 13:06:30.929002  4871 layer_factory.hpp:76] Creating layer relu3
I1209 13:06:30.929007  4871 net.cpp:106] Creating Layer relu3
I1209 13:06:30.929008  4871 net.cpp:454] relu3 <- conv3_BN_ea
I1209 13:06:30.929011  4871 net.cpp:411] relu3 -> relu3
I1209 13:06:30.929268  4871 net.cpp:150] Setting up relu3
I1209 13:06:30.929275  4871 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:06:30.929287  4871 net.cpp:165] Memory required for data: 752190464
I1209 13:06:30.929289  4871 layer_factory.hpp:76] Creating layer conv4
I1209 13:06:30.929296  4871 net.cpp:106] Creating Layer conv4
I1209 13:06:30.929299  4871 net.cpp:454] conv4 <- relu3
I1209 13:06:30.929313  4871 net.cpp:411] conv4 -> conv4
I1209 13:06:30.946149  4871 net.cpp:150] Setting up conv4
I1209 13:06:30.946176  4871 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:06:30.946177  4871 net.cpp:165] Memory required for data: 771458048
I1209 13:06:30.946184  4871 layer_factory.hpp:76] Creating layer conv4_BN
I1209 13:06:30.946226  4871 net.cpp:106] Creating Layer conv4_BN
I1209 13:06:30.946230  4871 net.cpp:454] conv4_BN <- conv4
I1209 13:06:30.946236  4871 net.cpp:411] conv4_BN -> conv4_BN
I1209 13:06:30.946388  4871 net.cpp:150] Setting up conv4_BN
I1209 13:06:30.946393  4871 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:06:30.946395  4871 net.cpp:165] Memory required for data: 790725632
I1209 13:06:30.946409  4871 layer_factory.hpp:76] Creating layer ea_BN4
I1209 13:06:30.946415  4871 net.cpp:106] Creating Layer ea_BN4
I1209 13:06:30.946418  4871 net.cpp:454] ea_BN4 <- conv4_BN
I1209 13:06:30.946429  4871 net.cpp:411] ea_BN4 -> conv4_BN_ea
I1209 13:06:30.946544  4871 net.cpp:150] Setting up ea_BN4
I1209 13:06:30.946548  4871 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:06:30.946550  4871 net.cpp:165] Memory required for data: 809993216
I1209 13:06:30.946563  4871 layer_factory.hpp:76] Creating layer relu4
I1209 13:06:30.946568  4871 net.cpp:106] Creating Layer relu4
I1209 13:06:30.946569  4871 net.cpp:454] relu4 <- conv4_BN_ea
I1209 13:06:30.946573  4871 net.cpp:411] relu4 -> relu4
I1209 13:06:30.946825  4871 net.cpp:150] Setting up relu4
I1209 13:06:30.946832  4871 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I1209 13:06:30.946844  4871 net.cpp:165] Memory required for data: 829260800
I1209 13:06:30.946846  4871 layer_factory.hpp:76] Creating layer conv5
I1209 13:06:30.946853  4871 net.cpp:106] Creating Layer conv5
I1209 13:06:30.946856  4871 net.cpp:454] conv5 <- relu4
I1209 13:06:30.946869  4871 net.cpp:411] conv5 -> conv5
I1209 13:06:30.958930  4871 net.cpp:150] Setting up conv5
I1209 13:06:30.958957  4871 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I1209 13:06:30.958959  4871 net.cpp:165] Memory required for data: 842105856
I1209 13:06:30.958966  4871 layer_factory.hpp:76] Creating layer conv5_BN
I1209 13:06:30.958976  4871 net.cpp:106] Creating Layer conv5_BN
I1209 13:06:30.958979  4871 net.cpp:454] conv5_BN <- conv5
I1209 13:06:30.958986  4871 net.cpp:411] conv5_BN -> conv5_BN
I1209 13:06:30.959123  4871 net.cpp:150] Setting up conv5_BN
I1209 13:06:30.959126  4871 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I1209 13:06:30.959128  4871 net.cpp:165] Memory required for data: 854950912
I1209 13:06:30.959136  4871 layer_factory.hpp:76] Creating layer ea_BN5
I1209 13:06:30.959141  4871 net.cpp:106] Creating Layer ea_BN5
I1209 13:06:30.959142  4871 net.cpp:454] ea_BN5 <- conv5_BN
I1209 13:06:30.959146  4871 net.cpp:411] ea_BN5 -> conv5_BN_ea
I1209 13:06:30.959656  4871 net.cpp:150] Setting up ea_BN5
I1209 13:06:30.959676  4871 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I1209 13:06:30.959677  4871 net.cpp:165] Memory required for data: 867795968
I1209 13:06:30.959682  4871 layer_factory.hpp:76] Creating layer relu5
I1209 13:06:30.959686  4871 net.cpp:106] Creating Layer relu5
I1209 13:06:30.959689  4871 net.cpp:454] relu5 <- conv5_BN_ea
I1209 13:06:30.959692  4871 net.cpp:411] relu5 -> relu5
I1209 13:06:30.959841  4871 net.cpp:150] Setting up relu5
I1209 13:06:30.959848  4871 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I1209 13:06:30.959859  4871 net.cpp:165] Memory required for data: 880641024
I1209 13:06:30.959861  4871 layer_factory.hpp:76] Creating layer pool5
I1209 13:06:30.959867  4871 net.cpp:106] Creating Layer pool5
I1209 13:06:30.959867  4871 net.cpp:454] pool5 <- relu5
I1209 13:06:30.959872  4871 net.cpp:411] pool5 -> pool5
I1209 13:06:30.960129  4871 net.cpp:150] Setting up pool5
I1209 13:06:30.960136  4871 net.cpp:157] Top shape: 256 256 3 3 (589824)
I1209 13:06:30.960149  4871 net.cpp:165] Memory required for data: 883000320
I1209 13:06:30.960150  4871 layer_factory.hpp:76] Creating layer fc6
I1209 13:06:30.960160  4871 net.cpp:106] Creating Layer fc6
I1209 13:06:30.960161  4871 net.cpp:454] fc6 <- pool5
I1209 13:06:30.960165  4871 net.cpp:411] fc6 -> fc6
I1209 13:06:31.072522  4871 net.cpp:150] Setting up fc6
I1209 13:06:31.072549  4871 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:06:31.072552  4871 net.cpp:165] Memory required for data: 885097472
I1209 13:06:31.072569  4871 layer_factory.hpp:76] Creating layer fc6_BN
I1209 13:06:31.072607  4871 net.cpp:106] Creating Layer fc6_BN
I1209 13:06:31.072613  4871 net.cpp:454] fc6_BN <- fc6
I1209 13:06:31.072618  4871 net.cpp:411] fc6_BN -> fc6_BN
I1209 13:06:31.072757  4871 net.cpp:150] Setting up fc6_BN
I1209 13:06:31.072762  4871 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:06:31.072762  4871 net.cpp:165] Memory required for data: 887194624
I1209 13:06:31.072767  4871 layer_factory.hpp:76] Creating layer ea_BN6
I1209 13:06:31.072775  4871 net.cpp:106] Creating Layer ea_BN6
I1209 13:06:31.072777  4871 net.cpp:454] ea_BN6 <- fc6_BN
I1209 13:06:31.072780  4871 net.cpp:411] ea_BN6 -> fc6_BN_ea
I1209 13:06:31.072865  4871 net.cpp:150] Setting up ea_BN6
I1209 13:06:31.072870  4871 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:06:31.072870  4871 net.cpp:165] Memory required for data: 889291776
I1209 13:06:31.072875  4871 layer_factory.hpp:76] Creating layer relu6
I1209 13:06:31.072878  4871 net.cpp:106] Creating Layer relu6
I1209 13:06:31.072880  4871 net.cpp:454] relu6 <- fc6_BN_ea
I1209 13:06:31.072882  4871 net.cpp:411] relu6 -> relu6
I1209 13:06:31.073062  4871 net.cpp:150] Setting up relu6
I1209 13:06:31.073066  4871 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:06:31.073068  4871 net.cpp:165] Memory required for data: 891388928
I1209 13:06:31.073071  4871 layer_factory.hpp:76] Creating layer drop6
I1209 13:06:31.073078  4871 net.cpp:106] Creating Layer drop6
I1209 13:06:31.073081  4871 net.cpp:454] drop6 <- relu6
I1209 13:06:31.073086  4871 net.cpp:397] drop6 -> relu6 (in-place)
I1209 13:06:31.073107  4871 net.cpp:150] Setting up drop6
I1209 13:06:31.073109  4871 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:06:31.073113  4871 net.cpp:165] Memory required for data: 893486080
I1209 13:06:31.073115  4871 layer_factory.hpp:76] Creating layer fc7
I1209 13:06:31.073120  4871 net.cpp:106] Creating Layer fc7
I1209 13:06:31.073122  4871 net.cpp:454] fc7 <- relu6
I1209 13:06:31.073125  4871 net.cpp:411] fc7 -> fc7
I1209 13:06:31.173365  4871 net.cpp:150] Setting up fc7
I1209 13:06:31.173393  4871 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:06:31.173394  4871 net.cpp:165] Memory required for data: 895583232
I1209 13:06:31.173401  4871 layer_factory.hpp:76] Creating layer fc7_BN
I1209 13:06:31.173421  4871 net.cpp:106] Creating Layer fc7_BN
I1209 13:06:31.173425  4871 net.cpp:454] fc7_BN <- fc7
I1209 13:06:31.173430  4871 net.cpp:411] fc7_BN -> fc7_BN
I1209 13:06:31.173580  4871 net.cpp:150] Setting up fc7_BN
I1209 13:06:31.173584  4871 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:06:31.173586  4871 net.cpp:165] Memory required for data: 897680384
I1209 13:06:31.173599  4871 layer_factory.hpp:76] Creating layer ea_BN7
I1209 13:06:31.173604  4871 net.cpp:106] Creating Layer ea_BN7
I1209 13:06:31.173616  4871 net.cpp:454] ea_BN7 <- fc7_BN
I1209 13:06:31.173619  4871 net.cpp:411] ea_BN7 -> fc7_BN_ea
I1209 13:06:31.173719  4871 net.cpp:150] Setting up ea_BN7
I1209 13:06:31.173722  4871 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:06:31.173724  4871 net.cpp:165] Memory required for data: 899777536
I1209 13:06:31.173737  4871 layer_factory.hpp:76] Creating layer relu7
I1209 13:06:31.173743  4871 net.cpp:106] Creating Layer relu7
I1209 13:06:31.173754  4871 net.cpp:454] relu7 <- fc7_BN_ea
I1209 13:06:31.173758  4871 net.cpp:411] relu7 -> relu7
I1209 13:06:31.174270  4871 net.cpp:150] Setting up relu7
I1209 13:06:31.174278  4871 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:06:31.174288  4871 net.cpp:165] Memory required for data: 901874688
I1209 13:06:31.174291  4871 layer_factory.hpp:76] Creating layer drop7
I1209 13:06:31.174295  4871 net.cpp:106] Creating Layer drop7
I1209 13:06:31.174307  4871 net.cpp:454] drop7 <- relu7
I1209 13:06:31.174311  4871 net.cpp:397] drop7 -> relu7 (in-place)
I1209 13:06:31.174329  4871 net.cpp:150] Setting up drop7
I1209 13:06:31.174334  4871 net.cpp:157] Top shape: 256 2048 (524288)
I1209 13:06:31.174335  4871 net.cpp:165] Memory required for data: 903971840
I1209 13:06:31.174337  4871 layer_factory.hpp:76] Creating layer fc8
I1209 13:06:31.174360  4871 net.cpp:106] Creating Layer fc8
I1209 13:06:31.174361  4871 net.cpp:454] fc8 <- relu7
I1209 13:06:31.174365  4871 net.cpp:411] fc8 -> fc8
I1209 13:06:31.222849  4871 net.cpp:150] Setting up fc8
I1209 13:06:31.222875  4871 net.cpp:157] Top shape: 256 1000 (256000)
I1209 13:06:31.222877  4871 net.cpp:165] Memory required for data: 904995840
I1209 13:06:31.222884  4871 layer_factory.hpp:76] Creating layer loss
I1209 13:06:31.222894  4871 net.cpp:106] Creating Layer loss
I1209 13:06:31.222908  4871 net.cpp:454] loss <- fc8
I1209 13:06:31.222911  4871 net.cpp:454] loss <- label
I1209 13:06:31.222918  4871 net.cpp:411] loss -> loss
I1209 13:06:31.222929  4871 layer_factory.hpp:76] Creating layer loss
I1209 13:06:31.223675  4871 net.cpp:150] Setting up loss
I1209 13:06:31.223683  4871 net.cpp:157] Top shape: (1)
I1209 13:06:31.223695  4871 net.cpp:160]     with loss weight 1
I1209 13:06:31.223722  4871 net.cpp:165] Memory required for data: 904995844
I1209 13:06:31.223724  4871 net.cpp:226] loss needs backward computation.
I1209 13:06:31.223727  4871 net.cpp:226] fc8 needs backward computation.
I1209 13:06:31.223731  4871 net.cpp:226] drop7 needs backward computation.
I1209 13:06:31.223732  4871 net.cpp:226] relu7 needs backward computation.
I1209 13:06:31.223736  4871 net.cpp:226] ea_BN7 needs backward computation.
I1209 13:06:31.223737  4871 net.cpp:226] fc7_BN needs backward computation.
I1209 13:06:31.223738  4871 net.cpp:226] fc7 needs backward computation.
I1209 13:06:31.223740  4871 net.cpp:226] drop6 needs backward computation.
I1209 13:06:31.223742  4871 net.cpp:226] relu6 needs backward computation.
I1209 13:06:31.223744  4871 net.cpp:226] ea_BN6 needs backward computation.
I1209 13:06:31.223747  4871 net.cpp:226] fc6_BN needs backward computation.
I1209 13:06:31.223748  4871 net.cpp:226] fc6 needs backward computation.
I1209 13:06:31.223750  4871 net.cpp:226] pool5 needs backward computation.
I1209 13:06:31.223753  4871 net.cpp:226] relu5 needs backward computation.
I1209 13:06:31.223755  4871 net.cpp:226] ea_BN5 needs backward computation.
I1209 13:06:31.223757  4871 net.cpp:226] conv5_BN needs backward computation.
I1209 13:06:31.223759  4871 net.cpp:226] conv5 needs backward computation.
I1209 13:06:31.223762  4871 net.cpp:226] relu4 needs backward computation.
I1209 13:06:31.223763  4871 net.cpp:226] ea_BN4 needs backward computation.
I1209 13:06:31.223765  4871 net.cpp:226] conv4_BN needs backward computation.
I1209 13:06:31.223767  4871 net.cpp:226] conv4 needs backward computation.
I1209 13:06:31.223770  4871 net.cpp:226] relu3 needs backward computation.
I1209 13:06:31.223773  4871 net.cpp:226] ea_BN3 needs backward computation.
I1209 13:06:31.223774  4871 net.cpp:226] conv3_BN needs backward computation.
I1209 13:06:31.223776  4871 net.cpp:226] conv3 needs backward computation.
I1209 13:06:31.223778  4871 net.cpp:226] pool2 needs backward computation.
I1209 13:06:31.223780  4871 net.cpp:226] relu2 needs backward computation.
I1209 13:06:31.223783  4871 net.cpp:226] ea_BN2 needs backward computation.
I1209 13:06:31.223784  4871 net.cpp:226] conv2_BN needs backward computation.
I1209 13:06:31.223788  4871 net.cpp:226] conv2 needs backward computation.
I1209 13:06:31.223788  4871 net.cpp:226] pool1 needs backward computation.
I1209 13:06:31.223790  4871 net.cpp:226] relu1 needs backward computation.
I1209 13:06:31.223793  4871 net.cpp:226] ea_BN1 needs backward computation.
I1209 13:06:31.223794  4871 net.cpp:226] conv1_BN needs backward computation.
I1209 13:06:31.223796  4871 net.cpp:226] conv1 needs backward computation.
I1209 13:06:31.223799  4871 net.cpp:228] data does not need backward computation.
I1209 13:06:31.223800  4871 net.cpp:270] This network produces output loss
I1209 13:06:31.223819  4871 net.cpp:283] Network initialization done.
I1209 13:06:31.223968  4871 solver.cpp:181] Creating test net (#0) specified by net_param
I1209 13:06:31.224038  4871 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1209 13:06:31.224066  4871 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer conv1_BN
I1209 13:06:31.224071  4871 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer conv2_BN
I1209 13:06:31.224086  4871 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer conv3_BN
I1209 13:06:31.224089  4871 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer conv4_BN
I1209 13:06:31.224092  4871 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer conv5_BN
I1209 13:06:31.224097  4871 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer fc6_BN
I1209 13:06:31.224100  4871 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer fc7_BN
I1209 13:06:31.224304  4871 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.04
    mirror: false
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 123
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: CUBIC
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/share/storage/datasets/imagenet/lmdb/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_BN"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN1"
  type: "EltwiseAffine"
  bottom: "conv1_BN"
  top: "conv1_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1_BN_ea"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv2_BN"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN2"
  type: "EltwiseAffine"
  bottom: "conv2_BN"
  top: "conv2_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2_BN_ea"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv3_BN"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN3"
  type: "EltwiseAffine"
  bottom: "conv3_BN"
  top: "conv3_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3_BN_ea"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv4_BN"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN4"
  type: "EltwiseAffine"
  bottom: "conv4_BN"
  top: "conv4_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4_BN_ea"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv5_BN"
  type: "BatchNorm"
  bottom: "conv5"
  top: "conv5_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN5"
  type: "EltwiseAffine"
  bottom: "conv5_BN"
  top: "conv5_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5_BN_ea"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "fc6_BN"
  type: "BatchNorm"
  bottom: "fc6"
  top: "fc6_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN6"
  type: "EltwiseAffine"
  bottom: "fc6_BN"
  top: "fc6_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6_BN_ea"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "relu6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "relu6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "fc7_BN"
  type: "BatchNorm"
  bottom: "fc7"
  top: "fc7_BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "ea_BN7"
  type: "EltwiseAffine"
  bottom: "fc7_BN"
  top: "fc7_BN_ea"
  eltwise_affine_param {
    slope_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0.0001
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7_BN_ea"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "relu7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "relu7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1209 13:06:31.224423  4871 layer_factory.hpp:76] Creating layer data
I1209 13:06:31.224480  4871 net.cpp:106] Creating Layer data
I1209 13:06:31.224495  4871 net.cpp:411] data -> data
I1209 13:06:31.224501  4871 net.cpp:411] data -> label
I1209 13:06:31.225168  4884 db_lmdb.cpp:38] Opened lmdb /home/share/storage/datasets/imagenet/lmdb/ilsvrc12_val_lmdb
I1209 13:06:31.226336  4871 data_layer.cpp:41] output data size: 50,3,128,128
I1209 13:06:31.244025  4871 net.cpp:150] Setting up data
I1209 13:06:31.244050  4871 net.cpp:157] Top shape: 50 3 128 128 (2457600)
I1209 13:06:31.244055  4871 net.cpp:157] Top shape: 50 (50)
I1209 13:06:31.244058  4871 net.cpp:165] Memory required for data: 9830600
I1209 13:06:31.244065  4871 layer_factory.hpp:76] Creating layer label_data_1_split
I1209 13:06:31.244079  4871 net.cpp:106] Creating Layer label_data_1_split
I1209 13:06:31.244084  4871 net.cpp:454] label_data_1_split <- label
I1209 13:06:31.244091  4871 net.cpp:411] label_data_1_split -> label_data_1_split_0
I1209 13:06:31.244102  4871 net.cpp:411] label_data_1_split -> label_data_1_split_1
I1209 13:06:31.251693  4871 net.cpp:150] Setting up label_data_1_split
I1209 13:06:31.251705  4871 net.cpp:157] Top shape: 50 (50)
I1209 13:06:31.251709  4871 net.cpp:157] Top shape: 50 (50)
I1209 13:06:31.251711  4871 net.cpp:165] Memory required for data: 9831000
I1209 13:06:31.251715  4871 layer_factory.hpp:76] Creating layer conv1
I1209 13:06:31.251725  4871 net.cpp:106] Creating Layer conv1
I1209 13:06:31.251729  4871 net.cpp:454] conv1 <- data
I1209 13:06:31.251734  4871 net.cpp:411] conv1 -> conv1
I1209 13:06:31.254431  4871 net.cpp:150] Setting up conv1
I1209 13:06:31.254447  4871 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I1209 13:06:31.254451  4871 net.cpp:165] Memory required for data: 27111000
I1209 13:06:31.254459  4871 layer_factory.hpp:76] Creating layer conv1_BN
I1209 13:06:31.254468  4871 net.cpp:106] Creating Layer conv1_BN
I1209 13:06:31.254470  4871 net.cpp:454] conv1_BN <- conv1
I1209 13:06:31.254475  4871 net.cpp:411] conv1_BN -> conv1_BN
I1209 13:06:31.254640  4871 net.cpp:150] Setting up conv1_BN
I1209 13:06:31.254662  4871 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I1209 13:06:31.254663  4871 net.cpp:165] Memory required for data: 44391000
I1209 13:06:31.254672  4871 layer_factory.hpp:76] Creating layer ea_BN1
I1209 13:06:31.254678  4871 net.cpp:106] Creating Layer ea_BN1
I1209 13:06:31.254680  4871 net.cpp:454] ea_BN1 <- conv1_BN
I1209 13:06:31.254684  4871 net.cpp:411] ea_BN1 -> conv1_BN_ea
I1209 13:06:31.254858  4871 net.cpp:150] Setting up ea_BN1
I1209 13:06:31.254863  4871 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I1209 13:06:31.254864  4871 net.cpp:165] Memory required for data: 61671000
I1209 13:06:31.254868  4871 layer_factory.hpp:76] Creating layer relu1
I1209 13:06:31.254873  4871 net.cpp:106] Creating Layer relu1
I1209 13:06:31.254875  4871 net.cpp:454] relu1 <- conv1_BN_ea
I1209 13:06:31.254878  4871 net.cpp:411] relu1 -> relu1
I1209 13:06:31.256664  4871 net.cpp:150] Setting up relu1
I1209 13:06:31.256672  4871 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I1209 13:06:31.256685  4871 net.cpp:165] Memory required for data: 78951000
I1209 13:06:31.256687  4871 layer_factory.hpp:76] Creating layer pool1
I1209 13:06:31.256692  4871 net.cpp:106] Creating Layer pool1
I1209 13:06:31.256695  4871 net.cpp:454] pool1 <- relu1
I1209 13:06:31.256697  4871 net.cpp:411] pool1 -> pool1
I1209 13:06:31.256847  4871 net.cpp:150] Setting up pool1
I1209 13:06:31.256855  4871 net.cpp:157] Top shape: 50 96 15 15 (1080000)
I1209 13:06:31.256865  4871 net.cpp:165] Memory required for data: 83271000
I1209 13:06:31.256867  4871 layer_factory.hpp:76] Creating layer conv2
I1209 13:06:31.256875  4871 net.cpp:106] Creating Layer conv2
I1209 13:06:31.256877  4871 net.cpp:454] conv2 <- pool1
I1209 13:06:31.256880  4871 net.cpp:411] conv2 -> conv2
I1209 13:06:31.265785  4871 net.cpp:150] Setting up conv2
I1209 13:06:31.265811  4871 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I1209 13:06:31.265815  4871 net.cpp:165] Memory required for data: 94791000
I1209 13:06:31.265825  4871 layer_factory.hpp:76] Creating layer conv2_BN
I1209 13:06:31.265836  4871 net.cpp:106] Creating Layer conv2_BN
I1209 13:06:31.265838  4871 net.cpp:454] conv2_BN <- conv2
I1209 13:06:31.265844  4871 net.cpp:411] conv2_BN -> conv2_BN
I1209 13:06:31.266000  4871 net.cpp:150] Setting up conv2_BN
I1209 13:06:31.266002  4871 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I1209 13:06:31.266005  4871 net.cpp:165] Memory required for data: 106311000
I1209 13:06:31.266010  4871 layer_factory.hpp:76] Creating layer ea_BN2
I1209 13:06:31.266015  4871 net.cpp:106] Creating Layer ea_BN2
I1209 13:06:31.266016  4871 net.cpp:454] ea_BN2 <- conv2_BN
I1209 13:06:31.266019  4871 net.cpp:411] ea_BN2 -> conv2_BN_ea
I1209 13:06:31.266163  4871 net.cpp:150] Setting up ea_BN2
I1209 13:06:31.266167  4871 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I1209 13:06:31.266170  4871 net.cpp:165] Memory required for data: 117831000
I1209 13:06:31.266172  4871 layer_factory.hpp:76] Creating layer relu2
I1209 13:06:31.266177  4871 net.cpp:106] Creating Layer relu2
I1209 13:06:31.266180  4871 net.cpp:454] relu2 <- conv2_BN_ea
I1209 13:06:31.266182  4871 net.cpp:411] relu2 -> relu2
I1209 13:06:31.266326  4871 net.cpp:150] Setting up relu2
I1209 13:06:31.266331  4871 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I1209 13:06:31.266332  4871 net.cpp:165] Memory required for data: 129351000
I1209 13:06:31.266335  4871 layer_factory.hpp:76] Creating layer pool2
I1209 13:06:31.266340  4871 net.cpp:106] Creating Layer pool2
I1209 13:06:31.266342  4871 net.cpp:454] pool2 <- relu2
I1209 13:06:31.266345  4871 net.cpp:411] pool2 -> pool2
I1209 13:06:31.266597  4871 net.cpp:150] Setting up pool2
I1209 13:06:31.266604  4871 net.cpp:157] Top shape: 50 256 7 7 (627200)
I1209 13:06:31.266607  4871 net.cpp:165] Memory required for data: 131859800
I1209 13:06:31.266608  4871 layer_factory.hpp:76] Creating layer conv3
I1209 13:06:31.266615  4871 net.cpp:106] Creating Layer conv3
I1209 13:06:31.266618  4871 net.cpp:454] conv3 <- pool2
I1209 13:06:31.266623  4871 net.cpp:411] conv3 -> conv3
I1209 13:06:31.289521  4871 net.cpp:150] Setting up conv3
I1209 13:06:31.289541  4871 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:06:31.289543  4871 net.cpp:165] Memory required for data: 135623000
I1209 13:06:31.289551  4871 layer_factory.hpp:76] Creating layer conv3_BN
I1209 13:06:31.289561  4871 net.cpp:106] Creating Layer conv3_BN
I1209 13:06:31.289564  4871 net.cpp:454] conv3_BN <- conv3
I1209 13:06:31.289572  4871 net.cpp:411] conv3_BN -> conv3_BN
I1209 13:06:31.289744  4871 net.cpp:150] Setting up conv3_BN
I1209 13:06:31.289749  4871 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:06:31.289752  4871 net.cpp:165] Memory required for data: 139386200
I1209 13:06:31.289759  4871 layer_factory.hpp:76] Creating layer ea_BN3
I1209 13:06:31.289765  4871 net.cpp:106] Creating Layer ea_BN3
I1209 13:06:31.289767  4871 net.cpp:454] ea_BN3 <- conv3_BN
I1209 13:06:31.289772  4871 net.cpp:411] ea_BN3 -> conv3_BN_ea
I1209 13:06:31.289890  4871 net.cpp:150] Setting up ea_BN3
I1209 13:06:31.289893  4871 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:06:31.289896  4871 net.cpp:165] Memory required for data: 143149400
I1209 13:06:31.289899  4871 layer_factory.hpp:76] Creating layer relu3
I1209 13:06:31.289904  4871 net.cpp:106] Creating Layer relu3
I1209 13:06:31.289906  4871 net.cpp:454] relu3 <- conv3_BN_ea
I1209 13:06:31.289908  4871 net.cpp:411] relu3 -> relu3
I1209 13:06:31.290058  4871 net.cpp:150] Setting up relu3
I1209 13:06:31.290065  4871 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:06:31.290066  4871 net.cpp:165] Memory required for data: 146912600
I1209 13:06:31.290068  4871 layer_factory.hpp:76] Creating layer conv4
I1209 13:06:31.290077  4871 net.cpp:106] Creating Layer conv4
I1209 13:06:31.290081  4871 net.cpp:454] conv4 <- relu3
I1209 13:06:31.290083  4871 net.cpp:411] conv4 -> conv4
I1209 13:06:31.308217  4871 net.cpp:150] Setting up conv4
I1209 13:06:31.308235  4871 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:06:31.308238  4871 net.cpp:165] Memory required for data: 150675800
I1209 13:06:31.308245  4871 layer_factory.hpp:76] Creating layer conv4_BN
I1209 13:06:31.308254  4871 net.cpp:106] Creating Layer conv4_BN
I1209 13:06:31.308257  4871 net.cpp:454] conv4_BN <- conv4
I1209 13:06:31.308264  4871 net.cpp:411] conv4_BN -> conv4_BN
I1209 13:06:31.308444  4871 net.cpp:150] Setting up conv4_BN
I1209 13:06:31.308449  4871 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:06:31.308450  4871 net.cpp:165] Memory required for data: 154439000
I1209 13:06:31.308455  4871 layer_factory.hpp:76] Creating layer ea_BN4
I1209 13:06:31.308462  4871 net.cpp:106] Creating Layer ea_BN4
I1209 13:06:31.308465  4871 net.cpp:454] ea_BN4 <- conv4_BN
I1209 13:06:31.308468  4871 net.cpp:411] ea_BN4 -> conv4_BN_ea
I1209 13:06:31.308610  4871 net.cpp:150] Setting up ea_BN4
I1209 13:06:31.308617  4871 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:06:31.308619  4871 net.cpp:165] Memory required for data: 158202200
I1209 13:06:31.308624  4871 layer_factory.hpp:76] Creating layer relu4
I1209 13:06:31.308627  4871 net.cpp:106] Creating Layer relu4
I1209 13:06:31.308629  4871 net.cpp:454] relu4 <- conv4_BN_ea
I1209 13:06:31.308632  4871 net.cpp:411] relu4 -> relu4
I1209 13:06:31.308789  4871 net.cpp:150] Setting up relu4
I1209 13:06:31.308794  4871 net.cpp:157] Top shape: 50 384 7 7 (940800)
I1209 13:06:31.308795  4871 net.cpp:165] Memory required for data: 161965400
I1209 13:06:31.308799  4871 layer_factory.hpp:76] Creating layer conv5
I1209 13:06:31.308804  4871 net.cpp:106] Creating Layer conv5
I1209 13:06:31.308807  4871 net.cpp:454] conv5 <- relu4
I1209 13:06:31.308811  4871 net.cpp:411] conv5 -> conv5
I1209 13:06:31.321969  4871 net.cpp:150] Setting up conv5
I1209 13:06:31.321995  4871 net.cpp:157] Top shape: 50 256 7 7 (627200)
I1209 13:06:31.322000  4871 net.cpp:165] Memory required for data: 164474200
I1209 13:06:31.322007  4871 layer_factory.hpp:76] Creating layer conv5_BN
I1209 13:06:31.322017  4871 net.cpp:106] Creating Layer conv5_BN
I1209 13:06:31.322021  4871 net.cpp:454] conv5_BN <- conv5
I1209 13:06:31.322043  4871 net.cpp:411] conv5_BN -> conv5_BN
I1209 13:06:31.322216  4871 net.cpp:150] Setting up conv5_BN
I1209 13:06:31.322219  4871 net.cpp:157] Top shape: 50 256 7 7 (627200)
I1209 13:06:31.322221  4871 net.cpp:165] Memory required for data: 166983000
I1209 13:06:31.322229  4871 layer_factory.hpp:76] Creating layer ea_BN5
I1209 13:06:31.322235  4871 net.cpp:106] Creating Layer ea_BN5
I1209 13:06:31.322237  4871 net.cpp:454] ea_BN5 <- conv5_BN
I1209 13:06:31.322242  4871 net.cpp:411] ea_BN5 -> conv5_BN_ea
I1209 13:06:31.322356  4871 net.cpp:150] Setting up ea_BN5
I1209 13:06:31.322360  4871 net.cpp:157] Top shape: 50 256 7 7 (627200)
I1209 13:06:31.322361  4871 net.cpp:165] Memory required for data: 169491800
I1209 13:06:31.322365  4871 layer_factory.hpp:76] Creating layer relu5
I1209 13:06:31.322370  4871 net.cpp:106] Creating Layer relu5
I1209 13:06:31.322371  4871 net.cpp:454] relu5 <- conv5_BN_ea
I1209 13:06:31.322374  4871 net.cpp:411] relu5 -> relu5
I1209 13:06:31.322523  4871 net.cpp:150] Setting up relu5
I1209 13:06:31.322528  4871 net.cpp:157] Top shape: 50 256 7 7 (627200)
I1209 13:06:31.322530  4871 net.cpp:165] Memory required for data: 172000600
I1209 13:06:31.322532  4871 layer_factory.hpp:76] Creating layer pool5
I1209 13:06:31.322537  4871 net.cpp:106] Creating Layer pool5
I1209 13:06:31.322540  4871 net.cpp:454] pool5 <- relu5
I1209 13:06:31.322543  4871 net.cpp:411] pool5 -> pool5
I1209 13:06:31.322827  4871 net.cpp:150] Setting up pool5
I1209 13:06:31.322835  4871 net.cpp:157] Top shape: 50 256 3 3 (115200)
I1209 13:06:31.322847  4871 net.cpp:165] Memory required for data: 172461400
I1209 13:06:31.322849  4871 layer_factory.hpp:76] Creating layer fc6
I1209 13:06:31.322855  4871 net.cpp:106] Creating Layer fc6
I1209 13:06:31.322859  4871 net.cpp:454] fc6 <- pool5
I1209 13:06:31.322862  4871 net.cpp:411] fc6 -> fc6
I1209 13:06:31.437264  4871 net.cpp:150] Setting up fc6
I1209 13:06:31.437293  4871 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:06:31.437296  4871 net.cpp:165] Memory required for data: 172871000
I1209 13:06:31.437304  4871 layer_factory.hpp:76] Creating layer fc6_BN
I1209 13:06:31.437312  4871 net.cpp:106] Creating Layer fc6_BN
I1209 13:06:31.437316  4871 net.cpp:454] fc6_BN <- fc6
I1209 13:06:31.437325  4871 net.cpp:411] fc6_BN -> fc6_BN
I1209 13:06:31.437481  4871 net.cpp:150] Setting up fc6_BN
I1209 13:06:31.437485  4871 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:06:31.437486  4871 net.cpp:165] Memory required for data: 173280600
I1209 13:06:31.437491  4871 layer_factory.hpp:76] Creating layer ea_BN6
I1209 13:06:31.437497  4871 net.cpp:106] Creating Layer ea_BN6
I1209 13:06:31.437500  4871 net.cpp:454] ea_BN6 <- fc6_BN
I1209 13:06:31.437502  4871 net.cpp:411] ea_BN6 -> fc6_BN_ea
I1209 13:06:31.437598  4871 net.cpp:150] Setting up ea_BN6
I1209 13:06:31.437602  4871 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:06:31.437603  4871 net.cpp:165] Memory required for data: 173690200
I1209 13:06:31.437608  4871 layer_factory.hpp:76] Creating layer relu6
I1209 13:06:31.437610  4871 net.cpp:106] Creating Layer relu6
I1209 13:06:31.437613  4871 net.cpp:454] relu6 <- fc6_BN_ea
I1209 13:06:31.437616  4871 net.cpp:411] relu6 -> relu6
I1209 13:06:31.437993  4871 net.cpp:150] Setting up relu6
I1209 13:06:31.437999  4871 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:06:31.438011  4871 net.cpp:165] Memory required for data: 174099800
I1209 13:06:31.438014  4871 layer_factory.hpp:76] Creating layer drop6
I1209 13:06:31.438017  4871 net.cpp:106] Creating Layer drop6
I1209 13:06:31.438020  4871 net.cpp:454] drop6 <- relu6
I1209 13:06:31.438024  4871 net.cpp:397] drop6 -> relu6 (in-place)
I1209 13:06:31.438045  4871 net.cpp:150] Setting up drop6
I1209 13:06:31.438047  4871 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:06:31.438048  4871 net.cpp:165] Memory required for data: 174509400
I1209 13:06:31.438050  4871 layer_factory.hpp:76] Creating layer fc7
I1209 13:06:31.438055  4871 net.cpp:106] Creating Layer fc7
I1209 13:06:31.438057  4871 net.cpp:454] fc7 <- relu6
I1209 13:06:31.438079  4871 net.cpp:411] fc7 -> fc7
I1209 13:06:31.538687  4871 net.cpp:150] Setting up fc7
I1209 13:06:31.538714  4871 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:06:31.538717  4871 net.cpp:165] Memory required for data: 174919000
I1209 13:06:31.538723  4871 layer_factory.hpp:76] Creating layer fc7_BN
I1209 13:06:31.538743  4871 net.cpp:106] Creating Layer fc7_BN
I1209 13:06:31.538748  4871 net.cpp:454] fc7_BN <- fc7
I1209 13:06:31.538753  4871 net.cpp:411] fc7_BN -> fc7_BN
I1209 13:06:31.538936  4871 net.cpp:150] Setting up fc7_BN
I1209 13:06:31.538943  4871 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:06:31.538954  4871 net.cpp:165] Memory required for data: 175328600
I1209 13:06:31.538959  4871 layer_factory.hpp:76] Creating layer ea_BN7
I1209 13:06:31.538965  4871 net.cpp:106] Creating Layer ea_BN7
I1209 13:06:31.538967  4871 net.cpp:454] ea_BN7 <- fc7_BN
I1209 13:06:31.538972  4871 net.cpp:411] ea_BN7 -> fc7_BN_ea
I1209 13:06:31.539074  4871 net.cpp:150] Setting up ea_BN7
I1209 13:06:31.539078  4871 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:06:31.539080  4871 net.cpp:165] Memory required for data: 175738200
I1209 13:06:31.539093  4871 layer_factory.hpp:76] Creating layer relu7
I1209 13:06:31.539098  4871 net.cpp:106] Creating Layer relu7
I1209 13:06:31.539100  4871 net.cpp:454] relu7 <- fc7_BN_ea
I1209 13:06:31.539103  4871 net.cpp:411] relu7 -> relu7
I1209 13:06:31.539304  4871 net.cpp:150] Setting up relu7
I1209 13:06:31.539309  4871 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:06:31.539320  4871 net.cpp:165] Memory required for data: 176147800
I1209 13:06:31.539322  4871 layer_factory.hpp:76] Creating layer drop7
I1209 13:06:31.539326  4871 net.cpp:106] Creating Layer drop7
I1209 13:06:31.539329  4871 net.cpp:454] drop7 <- relu7
I1209 13:06:31.539332  4871 net.cpp:397] drop7 -> relu7 (in-place)
I1209 13:06:31.539360  4871 net.cpp:150] Setting up drop7
I1209 13:06:31.539366  4871 net.cpp:157] Top shape: 50 2048 (102400)
I1209 13:06:31.539368  4871 net.cpp:165] Memory required for data: 176557400
I1209 13:06:31.539371  4871 layer_factory.hpp:76] Creating layer fc8
I1209 13:06:31.539376  4871 net.cpp:106] Creating Layer fc8
I1209 13:06:31.539377  4871 net.cpp:454] fc8 <- relu7
I1209 13:06:31.539381  4871 net.cpp:411] fc8 -> fc8
I1209 13:06:31.588387  4871 net.cpp:150] Setting up fc8
I1209 13:06:31.588414  4871 net.cpp:157] Top shape: 50 1000 (50000)
I1209 13:06:31.588417  4871 net.cpp:165] Memory required for data: 176757400
I1209 13:06:31.588423  4871 layer_factory.hpp:76] Creating layer fc8_fc8_0_split
I1209 13:06:31.588430  4871 net.cpp:106] Creating Layer fc8_fc8_0_split
I1209 13:06:31.588443  4871 net.cpp:454] fc8_fc8_0_split <- fc8
I1209 13:06:31.588449  4871 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1209 13:06:31.588455  4871 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1209 13:06:31.588497  4871 net.cpp:150] Setting up fc8_fc8_0_split
I1209 13:06:31.588501  4871 net.cpp:157] Top shape: 50 1000 (50000)
I1209 13:06:31.588505  4871 net.cpp:157] Top shape: 50 1000 (50000)
I1209 13:06:31.588515  4871 net.cpp:165] Memory required for data: 177157400
I1209 13:06:31.588516  4871 layer_factory.hpp:76] Creating layer accuracy
I1209 13:06:31.588534  4871 net.cpp:106] Creating Layer accuracy
I1209 13:06:31.588536  4871 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I1209 13:06:31.588538  4871 net.cpp:454] accuracy <- label_data_1_split_0
I1209 13:06:31.588541  4871 net.cpp:411] accuracy -> accuracy
I1209 13:06:31.588558  4871 net.cpp:150] Setting up accuracy
I1209 13:06:31.588562  4871 net.cpp:157] Top shape: (1)
I1209 13:06:31.588562  4871 net.cpp:165] Memory required for data: 177157404
I1209 13:06:31.588564  4871 layer_factory.hpp:76] Creating layer loss
I1209 13:06:31.588569  4871 net.cpp:106] Creating Layer loss
I1209 13:06:31.588572  4871 net.cpp:454] loss <- fc8_fc8_0_split_1
I1209 13:06:31.588573  4871 net.cpp:454] loss <- label_data_1_split_1
I1209 13:06:31.588577  4871 net.cpp:411] loss -> loss
I1209 13:06:31.588582  4871 layer_factory.hpp:76] Creating layer loss
I1209 13:06:31.589382  4871 net.cpp:150] Setting up loss
I1209 13:06:31.589390  4871 net.cpp:157] Top shape: (1)
I1209 13:06:31.589401  4871 net.cpp:160]     with loss weight 1
I1209 13:06:31.589409  4871 net.cpp:165] Memory required for data: 177157408
I1209 13:06:31.589412  4871 net.cpp:226] loss needs backward computation.
I1209 13:06:31.589414  4871 net.cpp:228] accuracy does not need backward computation.
I1209 13:06:31.589426  4871 net.cpp:226] fc8_fc8_0_split needs backward computation.
I1209 13:06:31.589428  4871 net.cpp:226] fc8 needs backward computation.
I1209 13:06:31.589431  4871 net.cpp:226] drop7 needs backward computation.
I1209 13:06:31.589432  4871 net.cpp:226] relu7 needs backward computation.
I1209 13:06:31.589434  4871 net.cpp:226] ea_BN7 needs backward computation.
I1209 13:06:31.589437  4871 net.cpp:226] fc7_BN needs backward computation.
I1209 13:06:31.589438  4871 net.cpp:226] fc7 needs backward computation.
I1209 13:06:31.589440  4871 net.cpp:226] drop6 needs backward computation.
I1209 13:06:31.589442  4871 net.cpp:226] relu6 needs backward computation.
I1209 13:06:31.589444  4871 net.cpp:226] ea_BN6 needs backward computation.
I1209 13:06:31.589447  4871 net.cpp:226] fc6_BN needs backward computation.
I1209 13:06:31.589448  4871 net.cpp:226] fc6 needs backward computation.
I1209 13:06:31.589452  4871 net.cpp:226] pool5 needs backward computation.
I1209 13:06:31.589454  4871 net.cpp:226] relu5 needs backward computation.
I1209 13:06:31.589457  4871 net.cpp:226] ea_BN5 needs backward computation.
I1209 13:06:31.589458  4871 net.cpp:226] conv5_BN needs backward computation.
I1209 13:06:31.589460  4871 net.cpp:226] conv5 needs backward computation.
I1209 13:06:31.589463  4871 net.cpp:226] relu4 needs backward computation.
I1209 13:06:31.589465  4871 net.cpp:226] ea_BN4 needs backward computation.
I1209 13:06:31.589468  4871 net.cpp:226] conv4_BN needs backward computation.
I1209 13:06:31.589469  4871 net.cpp:226] conv4 needs backward computation.
I1209 13:06:31.589471  4871 net.cpp:226] relu3 needs backward computation.
I1209 13:06:31.589473  4871 net.cpp:226] ea_BN3 needs backward computation.
I1209 13:06:31.589476  4871 net.cpp:226] conv3_BN needs backward computation.
I1209 13:06:31.589478  4871 net.cpp:226] conv3 needs backward computation.
I1209 13:06:31.589480  4871 net.cpp:226] pool2 needs backward computation.
I1209 13:06:31.589483  4871 net.cpp:226] relu2 needs backward computation.
I1209 13:06:31.589484  4871 net.cpp:226] ea_BN2 needs backward computation.
I1209 13:06:31.589486  4871 net.cpp:226] conv2_BN needs backward computation.
I1209 13:06:31.589488  4871 net.cpp:226] conv2 needs backward computation.
I1209 13:06:31.589490  4871 net.cpp:226] pool1 needs backward computation.
I1209 13:06:31.589493  4871 net.cpp:226] relu1 needs backward computation.
I1209 13:06:31.589494  4871 net.cpp:226] ea_BN1 needs backward computation.
I1209 13:06:31.589496  4871 net.cpp:226] conv1_BN needs backward computation.
I1209 13:06:31.589498  4871 net.cpp:226] conv1 needs backward computation.
I1209 13:06:31.589500  4871 net.cpp:228] label_data_1_split does not need backward computation.
I1209 13:06:31.589504  4871 net.cpp:228] data does not need backward computation.
I1209 13:06:31.589504  4871 net.cpp:270] This network produces output accuracy
I1209 13:06:31.589507  4871 net.cpp:270] This network produces output loss
I1209 13:06:31.589536  4871 net.cpp:283] Network initialization done.
I1209 13:06:31.589660  4871 solver.cpp:60] Solver scaffolding done.
I1209 13:06:31.590978  4871 caffe.cpp:128] Finetuning from ./caffenet128_lsuv_no_lrn_BatchNormBeforeReLU_EltwiseAffine.prototxt.caffemodel
I1209 13:06:31.733180  4871 caffe.cpp:212] Starting Optimization
I1209 13:06:31.733211  4871 solver.cpp:288] Solving CaffeNet
I1209 13:06:31.733213  4871 solver.cpp:289] Learning Rate Policy: step
I1209 13:06:31.734712  4871 solver.cpp:341] Iteration 0, Testing net (#0)
I1209 13:06:31.801429  4871 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 13:07:48.847904  4871 solver.cpp:409]     Test net output #0: accuracy = 0.00098
I1209 13:07:48.848134  4871 solver.cpp:409]     Test net output #1: loss = 7.98021 (* 1 = 7.98021 loss)
I1209 13:07:49.060647  4871 solver.cpp:237] Iteration 0, loss = 7.50467
I1209 13:07:49.060684  4871 solver.cpp:253]     Train net output #0: loss = 7.50467 (* 1 = 7.50467 loss)
I1209 13:07:49.060706  4871 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1209 13:07:51.082612  4871 blocking_queue.cpp:50] Data layer prefetch queue empty
I1209 13:07:55.991281  4871 solver.cpp:237] Iteration 20, loss = 7.20654
I1209 13:07:55.991308  4871 solver.cpp:253]     Train net output #0: loss = 7.20654 (* 1 = 7.20654 loss)
I1209 13:07:55.991314  4871 sgd_solver.cpp:106] Iteration 20, lr = 0.01
I1209 13:08:03.587174  4871 solver.cpp:237] Iteration 40, loss = 7.39031
I1209 13:08:03.587213  4871 solver.cpp:253]     Train net output #0: loss = 7.39031 (* 1 = 7.39031 loss)
I1209 13:08:03.587218  4871 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I1209 13:08:11.188032  4871 solver.cpp:237] Iteration 60, loss = 7.18173
I1209 13:08:11.188077  4871 solver.cpp:253]     Train net output #0: loss = 7.18173 (* 1 = 7.18173 loss)
I1209 13:08:11.188083  4871 sgd_solver.cpp:106] Iteration 60, lr = 0.01
I1209 13:08:18.918686  4871 solver.cpp:237] Iteration 80, loss = 7.0615
I1209 13:08:18.918860  4871 solver.cpp:253]     Train net output #0: loss = 7.0615 (* 1 = 7.0615 loss)
I1209 13:08:18.918876  4871 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I1209 13:08:26.632046  4871 solver.cpp:237] Iteration 100, loss = 7.03844
I1209 13:08:26.632083  4871 solver.cpp:253]     Train net output #0: loss = 7.03844 (* 1 = 7.03844 loss)
I1209 13:08:26.632088  4871 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I1209 13:08:34.266355  4871 solver.cpp:237] Iteration 120, loss = 7.07862
I1209 13:08:34.266394  4871 solver.cpp:253]     Train net output #0: loss = 7.07862 (* 1 = 7.07862 loss)
I1209 13:08:34.266399  4871 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I1209 13:08:41.955137  4871 solver.cpp:237] Iteration 140, loss = 6.9329
I1209 13:08:41.955173  4871 solver.cpp:253]     Train net output #0: loss = 6.9329 (* 1 = 6.9329 loss)
I1209 13:08:41.955178  4871 sgd_solver.cpp:106] Iteration 140, lr = 0.01
I1209 13:08:49.643357  4871 solver.cpp:237] Iteration 160, loss = 7.03519
I1209 13:08:49.643496  4871 solver.cpp:253]     Train net output #0: loss = 7.03519 (* 1 = 7.03519 loss)
I1209 13:08:49.643503  4871 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I1209 13:08:57.330659  4871 solver.cpp:237] Iteration 180, loss = 7.01976
I1209 13:08:57.330698  4871 solver.cpp:253]     Train net output #0: loss = 7.01976 (* 1 = 7.01976 loss)
I1209 13:08:57.330704  4871 sgd_solver.cpp:106] Iteration 180, lr = 0.01
I1209 13:09:05.161303  4871 solver.cpp:237] Iteration 200, loss = 6.92948
I1209 13:09:05.161339  4871 solver.cpp:253]     Train net output #0: loss = 6.92948 (* 1 = 6.92948 loss)
I1209 13:09:05.161345  4871 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I1209 13:09:12.865082  4871 solver.cpp:237] Iteration 220, loss = 6.94602
I1209 13:09:12.865116  4871 solver.cpp:253]     Train net output #0: loss = 6.94602 (* 1 = 6.94602 loss)
I1209 13:09:12.865121  4871 sgd_solver.cpp:106] Iteration 220, lr = 0.01
I1209 13:09:20.478245  4871 solver.cpp:237] Iteration 240, loss = 6.92212
I1209 13:09:20.478355  4871 solver.cpp:253]     Train net output #0: loss = 6.92212 (* 1 = 6.92212 loss)
I1209 13:09:20.478363  4871 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I1209 13:09:28.122916  4871 solver.cpp:237] Iteration 260, loss = 6.85819
I1209 13:09:28.122951  4871 solver.cpp:253]     Train net output #0: loss = 6.85819 (* 1 = 6.85819 loss)
I1209 13:09:28.122957  4871 sgd_solver.cpp:106] Iteration 260, lr = 0.01
I1209 13:09:35.710126  4871 solver.cpp:237] Iteration 280, loss = 6.95829
I1209 13:09:35.710165  4871 solver.cpp:253]     Train net output #0: loss = 6.95829 (* 1 = 6.95829 loss)
I1209 13:09:35.710170  4871 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I1209 13:09:43.420521  4871 solver.cpp:237] Iteration 300, loss = 6.86211
I1209 13:09:43.420559  4871 solver.cpp:253]     Train net output #0: loss = 6.86211 (* 1 = 6.86211 loss)
I1209 13:09:43.420565  4871 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I1209 13:09:51.113849  4871 solver.cpp:237] Iteration 320, loss = 6.77838
I1209 13:09:51.113984  4871 solver.cpp:253]     Train net output #0: loss = 6.77838 (* 1 = 6.77838 loss)
I1209 13:09:51.114001  4871 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I1209 13:09:58.891551  4871 solver.cpp:237] Iteration 340, loss = 6.77243
I1209 13:09:58.891587  4871 solver.cpp:253]     Train net output #0: loss = 6.77243 (* 1 = 6.77243 loss)
I1209 13:09:58.891593  4871 sgd_solver.cpp:106] Iteration 340, lr = 0.01
I1209 13:10:06.626910  4871 solver.cpp:237] Iteration 360, loss = 6.81943
I1209 13:10:06.626946  4871 solver.cpp:253]     Train net output #0: loss = 6.81943 (* 1 = 6.81943 loss)
I1209 13:10:06.626951  4871 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I1209 13:10:14.396193  4871 solver.cpp:237] Iteration 380, loss = 6.73837
I1209 13:10:14.396229  4871 solver.cpp:253]     Train net output #0: loss = 6.73837 (* 1 = 6.73837 loss)
I1209 13:10:14.396234  4871 sgd_solver.cpp:106] Iteration 380, lr = 0.01
I1209 13:10:22.128231  4871 solver.cpp:237] Iteration 400, loss = 6.76249
I1209 13:10:22.128387  4871 solver.cpp:253]     Train net output #0: loss = 6.76249 (* 1 = 6.76249 loss)
I1209 13:10:22.128394  4871 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I1209 13:10:29.843463  4871 solver.cpp:237] Iteration 420, loss = 6.70614
I1209 13:10:29.843499  4871 solver.cpp:253]     Train net output #0: loss = 6.70614 (* 1 = 6.70614 loss)
I1209 13:10:29.843505  4871 sgd_solver.cpp:106] Iteration 420, lr = 0.01
I1209 13:10:37.539316  4871 solver.cpp:237] Iteration 440, loss = 6.66044
I1209 13:10:37.539353  4871 solver.cpp:253]     Train net output #0: loss = 6.66044 (* 1 = 6.66044 loss)
I1209 13:10:37.539358  4871 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I1209 13:10:45.551093  4871 solver.cpp:237] Iteration 460, loss = 6.6759
I1209 13:10:45.551156  4871 solver.cpp:253]     Train net output #0: loss = 6.6759 (* 1 = 6.6759 loss)
I1209 13:10:45.551167  4871 sgd_solver.cpp:106] Iteration 460, lr = 0.01
I1209 13:10:53.799127  4871 solver.cpp:237] Iteration 480, loss = 6.68342
I1209 13:10:53.799299  4871 solver.cpp:253]     Train net output #0: loss = 6.68342 (* 1 = 6.68342 loss)
I1209 13:10:53.799307  4871 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I1209 13:11:01.559928  4871 solver.cpp:237] Iteration 500, loss = 6.62109
I1209 13:11:01.559965  4871 solver.cpp:253]     Train net output #0: loss = 6.62109 (* 1 = 6.62109 loss)
I1209 13:11:01.559972  4871 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I1209 13:11:09.311975  4871 solver.cpp:237] Iteration 520, loss = 6.53016
I1209 13:11:09.312011  4871 solver.cpp:253]     Train net output #0: loss = 6.53016 (* 1 = 6.53016 loss)
I1209 13:11:09.312017  4871 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I1209 13:11:17.058051  4871 solver.cpp:237] Iteration 540, loss = 6.53617
I1209 13:11:17.058168  4871 solver.cpp:253]     Train net output #0: loss = 6.53617 (* 1 = 6.53617 loss)
I1209 13:11:17.058212  4871 sgd_solver.cpp:106] Iteration 540, lr = 0.01
I1209 13:11:24.821033  4871 solver.cpp:237] Iteration 560, loss = 6.44833
I1209 13:11:24.821178  4871 solver.cpp:253]     Train net output #0: loss = 6.44833 (* 1 = 6.44833 loss)
I1209 13:11:24.821187  4871 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I1209 13:11:32.593642  4871 solver.cpp:237] Iteration 580, loss = 6.53477
I1209 13:11:32.593677  4871 solver.cpp:253]     Train net output #0: loss = 6.53477 (* 1 = 6.53477 loss)
I1209 13:11:32.593683  4871 sgd_solver.cpp:106] Iteration 580, lr = 0.01
I1209 13:11:40.341567  4871 solver.cpp:237] Iteration 600, loss = 6.36849
I1209 13:11:40.341603  4871 solver.cpp:253]     Train net output #0: loss = 6.36849 (* 1 = 6.36849 loss)
I1209 13:11:40.341608  4871 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I1209 13:11:48.064282  4871 solver.cpp:237] Iteration 620, loss = 6.4543
I1209 13:11:48.064318  4871 solver.cpp:253]     Train net output #0: loss = 6.4543 (* 1 = 6.4543 loss)
I1209 13:11:48.064324  4871 sgd_solver.cpp:106] Iteration 620, lr = 0.01
I1209 13:11:55.706217  4871 solver.cpp:237] Iteration 640, loss = 6.55829
I1209 13:11:55.706372  4871 solver.cpp:253]     Train net output #0: loss = 6.55829 (* 1 = 6.55829 loss)
I1209 13:11:55.706380  4871 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I1209 13:12:03.370756  4871 solver.cpp:237] Iteration 660, loss = 6.48154
I1209 13:12:03.370791  4871 solver.cpp:253]     Train net output #0: loss = 6.48154 (* 1 = 6.48154 loss)
I1209 13:12:03.370797  4871 sgd_solver.cpp:106] Iteration 660, lr = 0.01
I1209 13:12:11.055557  4871 solver.cpp:237] Iteration 680, loss = 6.29532
I1209 13:12:11.055593  4871 solver.cpp:253]     Train net output #0: loss = 6.29532 (* 1 = 6.29532 loss)
I1209 13:12:11.055599  4871 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I1209 13:12:18.778494  4871 solver.cpp:237] Iteration 700, loss = 6.43229
I1209 13:12:18.778530  4871 solver.cpp:253]     Train net output #0: loss = 6.43229 (* 1 = 6.43229 loss)
I1209 13:12:18.778537  4871 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I1209 13:12:26.548877  4871 solver.cpp:237] Iteration 720, loss = 6.20852
I1209 13:12:26.549031  4871 solver.cpp:253]     Train net output #0: loss = 6.20852 (* 1 = 6.20852 loss)
I1209 13:12:26.549047  4871 sgd_solver.cpp:106] Iteration 720, lr = 0.01
I1209 13:12:34.322703  4871 solver.cpp:237] Iteration 740, loss = 6.33215
I1209 13:12:34.322741  4871 solver.cpp:253]     Train net output #0: loss = 6.33215 (* 1 = 6.33215 loss)
I1209 13:12:34.322746  4871 sgd_solver.cpp:106] Iteration 740, lr = 0.01
I1209 13:12:42.056349  4871 solver.cpp:237] Iteration 760, loss = 6.41563
I1209 13:12:42.056385  4871 solver.cpp:253]     Train net output #0: loss = 6.41563 (* 1 = 6.41563 loss)
I1209 13:12:42.056391  4871 sgd_solver.cpp:106] Iteration 760, lr = 0.01
I1209 13:12:49.778080  4871 solver.cpp:237] Iteration 780, loss = 6.33127
I1209 13:12:49.778116  4871 solver.cpp:253]     Train net output #0: loss = 6.33127 (* 1 = 6.33127 loss)
I1209 13:12:49.778122  4871 sgd_solver.cpp:106] Iteration 780, lr = 0.01
I1209 13:12:57.523162  4871 solver.cpp:237] Iteration 800, loss = 6.30734
I1209 13:12:57.523346  4871 solver.cpp:253]     Train net output #0: loss = 6.30734 (* 1 = 6.30734 loss)
I1209 13:12:57.523362  4871 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I1209 13:13:05.601979  4871 solver.cpp:237] Iteration 820, loss = 6.21589
I1209 13:13:05.602013  4871 solver.cpp:253]     Train net output #0: loss = 6.21589 (* 1 = 6.21589 loss)
I1209 13:13:05.602020  4871 sgd_solver.cpp:106] Iteration 820, lr = 0.01
I1209 13:13:13.557060  4871 solver.cpp:237] Iteration 840, loss = 6.36995
I1209 13:13:13.557114  4871 solver.cpp:253]     Train net output #0: loss = 6.36995 (* 1 = 6.36995 loss)
I1209 13:13:13.557129  4871 sgd_solver.cpp:106] Iteration 840, lr = 0.01
I1209 13:13:21.358187  4871 solver.cpp:237] Iteration 860, loss = 6.19641
I1209 13:13:21.358212  4871 solver.cpp:253]     Train net output #0: loss = 6.19641 (* 1 = 6.19641 loss)
I1209 13:13:21.358217  4871 sgd_solver.cpp:106] Iteration 860, lr = 0.01
I1209 13:13:29.160027  4871 solver.cpp:237] Iteration 880, loss = 6.20362
I1209 13:13:29.160109  4871 solver.cpp:253]     Train net output #0: loss = 6.20362 (* 1 = 6.20362 loss)
I1209 13:13:29.160116  4871 sgd_solver.cpp:106] Iteration 880, lr = 0.01
I1209 13:13:36.937497  4871 solver.cpp:237] Iteration 900, loss = 6.19774
I1209 13:13:36.937533  4871 solver.cpp:253]     Train net output #0: loss = 6.19774 (* 1 = 6.19774 loss)
I1209 13:13:36.937539  4871 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I1209 13:13:44.689960  4871 solver.cpp:237] Iteration 920, loss = 6.1203
I1209 13:13:44.689995  4871 solver.cpp:253]     Train net output #0: loss = 6.1203 (* 1 = 6.1203 loss)
I1209 13:13:44.690001  4871 sgd_solver.cpp:106] Iteration 920, lr = 0.01
