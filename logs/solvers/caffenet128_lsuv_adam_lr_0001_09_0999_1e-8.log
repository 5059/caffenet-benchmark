/home.dokt/mishkdmy/irish-coffe/./build/tools/caffe: error while loading shared libraries: libcudart.so.7.5: cannot open shared object file: No such file or directory
I0124 14:24:05.989720 11446 caffe.cpp:184] Using GPUs 0
I0124 14:24:06.613287 11446 solver.cpp:47] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.001
display: 20
max_iter: 320000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "snapshots/caffenet128_lsuv_adam"
solver_mode: GPU
device_id: 0
net_param {
  name: "CaffeNet"
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TRAIN
    }
    transform_param {
      scale: 0.04
      mirror: true
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 124
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/local/temporary/imagenet_clc_orig/ilsvrc12_train_shuffle_lmdb"
      batch_size: 128
      backend: LMDB
    }
  }
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TEST
    }
    transform_param {
      scale: 0.04
      mirror: false
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 123
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/local/temporary/imagenet_clc_orig/ilsvrc12_val_lmdb"
      batch_size: 50
      backend: LMDB
    }
  }
  layer {
    name: "conv1"
    type: "Convolution"
    bottom: "data"
    top: "conv1"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 96
      kernel_size: 11
      stride: 4
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu1"
    type: "ReLU"
    bottom: "conv1"
    top: "relu1"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "relu1"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv2"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 2
      kernel_size: 5
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu2"
    type: "ReLU"
    bottom: "conv2"
    top: "relu2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "relu2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv3"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu3"
    type: "ReLU"
    bottom: "conv3"
    top: "relu3"
  }
  layer {
    name: "conv4"
    type: "Convolution"
    bottom: "relu3"
    top: "conv4"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu4"
    type: "ReLU"
    bottom: "conv4"
    top: "relu4"
  }
  layer {
    name: "conv5"
    type: "Convolution"
    bottom: "relu4"
    top: "conv5"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu5"
    type: "ReLU"
    bottom: "conv5"
    top: "relu5"
  }
  layer {
    name: "pool5"
    type: "Pooling"
    bottom: "relu5"
    top: "pool5"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "pool5"
    top: "fc6"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu6"
    type: "ReLU"
    bottom: "fc6"
    top: "relu6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "relu6"
    top: "drop6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "drop6"
    top: "fc7"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu7"
    type: "ReLU"
    bottom: "fc7"
    top: "relu7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "relu7"
    top: "drop7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc8"
    type: "InnerProduct"
    bottom: "drop7"
    top: "fc8"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 1000
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "accuracy"
    type: "Accuracy"
    bottom: "fc8"
    bottom: "label"
    top: "accuracy"
    include {
      phase: TEST
    }
  }
  layer {
    name: "loss"
    type: "SoftmaxWithLoss"
    bottom: "fc8"
    bottom: "label"
    top: "loss"
  }
}
delta: 1e-08
test_initialization: false
iter_size: 1
momentum2: 0.999
type: "Adam"
I0124 14:24:06.614331 11446 solver.cpp:85] Creating training net specified in net_param.
I0124 14:24:06.614469 11446 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0124 14:24:06.614505 11446 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0124 14:24:06.614759 11446 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.04
    mirror: true
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 124
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/local/temporary/imagenet_clc_orig/ilsvrc12_train_shuffle_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 14:24:06.614897 11446 layer_factory.hpp:76] Creating layer data
I0124 14:24:06.615525 11446 net.cpp:106] Creating Layer data
I0124 14:24:06.615540 11446 net.cpp:411] data -> data
I0124 14:24:06.615569 11446 net.cpp:411] data -> label
I0124 14:24:06.617121 11538 db_lmdb.cpp:38] Opened lmdb /local/temporary/imagenet_clc_orig/ilsvrc12_train_shuffle_lmdb
I0124 14:24:06.636430 11446 data_layer.cpp:41] output data size: 128,3,128,128
I0124 14:24:06.687901 11446 net.cpp:150] Setting up data
I0124 14:24:06.687939 11446 net.cpp:157] Top shape: 128 3 128 128 (6291456)
I0124 14:24:06.687944 11446 net.cpp:157] Top shape: 128 (128)
I0124 14:24:06.687947 11446 net.cpp:165] Memory required for data: 25166336
I0124 14:24:06.687959 11446 layer_factory.hpp:76] Creating layer conv1
I0124 14:24:06.688005 11446 net.cpp:106] Creating Layer conv1
I0124 14:24:06.688011 11446 net.cpp:454] conv1 <- data
I0124 14:24:06.688026 11446 net.cpp:411] conv1 -> conv1
I0124 14:24:06.809846 11446 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 4356
I0124 14:24:06.809984 11446 net.cpp:150] Setting up conv1
I0124 14:24:06.809998 11446 net.cpp:157] Top shape: 128 96 30 30 (11059200)
I0124 14:24:06.810001 11446 net.cpp:165] Memory required for data: 69403136
I0124 14:24:06.810020 11446 layer_factory.hpp:76] Creating layer relu1
I0124 14:24:06.810039 11446 net.cpp:106] Creating Layer relu1
I0124 14:24:06.810045 11446 net.cpp:454] relu1 <- conv1
I0124 14:24:06.810055 11446 net.cpp:411] relu1 -> relu1
I0124 14:24:06.810273 11446 net.cpp:150] Setting up relu1
I0124 14:24:06.810286 11446 net.cpp:157] Top shape: 128 96 30 30 (11059200)
I0124 14:24:06.810292 11446 net.cpp:165] Memory required for data: 113639936
I0124 14:24:06.810297 11446 layer_factory.hpp:76] Creating layer pool1
I0124 14:24:06.810319 11446 net.cpp:106] Creating Layer pool1
I0124 14:24:06.810327 11446 net.cpp:454] pool1 <- relu1
I0124 14:24:06.810334 11446 net.cpp:411] pool1 -> pool1
I0124 14:24:06.810667 11446 net.cpp:150] Setting up pool1
I0124 14:24:06.810681 11446 net.cpp:157] Top shape: 128 96 15 15 (2764800)
I0124 14:24:06.810688 11446 net.cpp:165] Memory required for data: 124699136
I0124 14:24:06.810693 11446 layer_factory.hpp:76] Creating layer conv2
I0124 14:24:06.810720 11446 net.cpp:106] Creating Layer conv2
I0124 14:24:06.810726 11446 net.cpp:454] conv2 <- pool1
I0124 14:24:06.810736 11446 net.cpp:411] conv2 -> conv2
I0124 14:24:06.821019 11446 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 28800
I0124 14:24:06.821053 11446 net.cpp:150] Setting up conv2
I0124 14:24:06.821058 11446 net.cpp:157] Top shape: 128 256 15 15 (7372800)
I0124 14:24:06.821061 11446 net.cpp:165] Memory required for data: 154190336
I0124 14:24:06.821071 11446 layer_factory.hpp:76] Creating layer relu2
I0124 14:24:06.821090 11446 net.cpp:106] Creating Layer relu2
I0124 14:24:06.821094 11446 net.cpp:454] relu2 <- conv2
I0124 14:24:06.821110 11446 net.cpp:411] relu2 -> relu2
I0124 14:24:06.821285 11446 net.cpp:150] Setting up relu2
I0124 14:24:06.821293 11446 net.cpp:157] Top shape: 128 256 15 15 (7372800)
I0124 14:24:06.821297 11446 net.cpp:165] Memory required for data: 183681536
I0124 14:24:06.821301 11446 layer_factory.hpp:76] Creating layer pool2
I0124 14:24:06.821307 11446 net.cpp:106] Creating Layer pool2
I0124 14:24:06.821312 11446 net.cpp:454] pool2 <- relu2
I0124 14:24:06.821316 11446 net.cpp:411] pool2 -> pool2
I0124 14:24:06.821622 11446 net.cpp:150] Setting up pool2
I0124 14:24:06.821645 11446 net.cpp:157] Top shape: 128 256 7 7 (1605632)
I0124 14:24:06.821647 11446 net.cpp:165] Memory required for data: 190104064
I0124 14:24:06.821650 11446 layer_factory.hpp:76] Creating layer conv3
I0124 14:24:06.821671 11446 net.cpp:106] Creating Layer conv3
I0124 14:24:06.821674 11446 net.cpp:454] conv3 <- pool2
I0124 14:24:06.821681 11446 net.cpp:411] conv3 -> conv3
I0124 14:24:06.846443 11446 net.cpp:150] Setting up conv3
I0124 14:24:06.846473 11446 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I0124 14:24:06.846477 11446 net.cpp:165] Memory required for data: 199737856
I0124 14:24:06.846487 11446 layer_factory.hpp:76] Creating layer relu3
I0124 14:24:06.846496 11446 net.cpp:106] Creating Layer relu3
I0124 14:24:06.846500 11446 net.cpp:454] relu3 <- conv3
I0124 14:24:06.846506 11446 net.cpp:411] relu3 -> relu3
I0124 14:24:06.846843 11446 net.cpp:150] Setting up relu3
I0124 14:24:06.846866 11446 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I0124 14:24:06.846869 11446 net.cpp:165] Memory required for data: 209371648
I0124 14:24:06.846885 11446 layer_factory.hpp:76] Creating layer conv4
I0124 14:24:06.846895 11446 net.cpp:106] Creating Layer conv4
I0124 14:24:06.846899 11446 net.cpp:454] conv4 <- relu3
I0124 14:24:06.846905 11446 net.cpp:411] conv4 -> conv4
I0124 14:24:06.866870 11446 net.cpp:150] Setting up conv4
I0124 14:24:06.866886 11446 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I0124 14:24:06.866901 11446 net.cpp:165] Memory required for data: 219005440
I0124 14:24:06.866909 11446 layer_factory.hpp:76] Creating layer relu4
I0124 14:24:06.866915 11446 net.cpp:106] Creating Layer relu4
I0124 14:24:06.866919 11446 net.cpp:454] relu4 <- conv4
I0124 14:24:06.866925 11446 net.cpp:411] relu4 -> relu4
I0124 14:24:06.867115 11446 net.cpp:150] Setting up relu4
I0124 14:24:06.867125 11446 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I0124 14:24:06.867128 11446 net.cpp:165] Memory required for data: 228639232
I0124 14:24:06.867132 11446 layer_factory.hpp:76] Creating layer conv5
I0124 14:24:06.867141 11446 net.cpp:106] Creating Layer conv5
I0124 14:24:06.867146 11446 net.cpp:454] conv5 <- relu4
I0124 14:24:06.867152 11446 net.cpp:411] conv5 -> conv5
I0124 14:24:06.881937 11446 net.cpp:150] Setting up conv5
I0124 14:24:06.881955 11446 net.cpp:157] Top shape: 128 256 7 7 (1605632)
I0124 14:24:06.881958 11446 net.cpp:165] Memory required for data: 235061760
I0124 14:24:06.881969 11446 layer_factory.hpp:76] Creating layer relu5
I0124 14:24:06.881989 11446 net.cpp:106] Creating Layer relu5
I0124 14:24:06.881994 11446 net.cpp:454] relu5 <- conv5
I0124 14:24:06.882010 11446 net.cpp:411] relu5 -> relu5
I0124 14:24:06.882194 11446 net.cpp:150] Setting up relu5
I0124 14:24:06.882202 11446 net.cpp:157] Top shape: 128 256 7 7 (1605632)
I0124 14:24:06.882206 11446 net.cpp:165] Memory required for data: 241484288
I0124 14:24:06.882210 11446 layer_factory.hpp:76] Creating layer pool5
I0124 14:24:06.882216 11446 net.cpp:106] Creating Layer pool5
I0124 14:24:06.882222 11446 net.cpp:454] pool5 <- relu5
I0124 14:24:06.882227 11446 net.cpp:411] pool5 -> pool5
I0124 14:24:06.882581 11446 net.cpp:150] Setting up pool5
I0124 14:24:06.882592 11446 net.cpp:157] Top shape: 128 256 3 3 (294912)
I0124 14:24:06.882596 11446 net.cpp:165] Memory required for data: 242663936
I0124 14:24:06.882599 11446 layer_factory.hpp:76] Creating layer fc6
I0124 14:24:06.882608 11446 net.cpp:106] Creating Layer fc6
I0124 14:24:06.882613 11446 net.cpp:454] fc6 <- pool5
I0124 14:24:06.882632 11446 net.cpp:411] fc6 -> fc6
I0124 14:24:07.017632 11446 net.cpp:150] Setting up fc6
I0124 14:24:07.017669 11446 net.cpp:157] Top shape: 128 2048 (262144)
I0124 14:24:07.017673 11446 net.cpp:165] Memory required for data: 243712512
I0124 14:24:07.017683 11446 layer_factory.hpp:76] Creating layer relu6
I0124 14:24:07.017693 11446 net.cpp:106] Creating Layer relu6
I0124 14:24:07.017699 11446 net.cpp:454] relu6 <- fc6
I0124 14:24:07.017704 11446 net.cpp:411] relu6 -> relu6
I0124 14:24:07.017967 11446 net.cpp:150] Setting up relu6
I0124 14:24:07.017978 11446 net.cpp:157] Top shape: 128 2048 (262144)
I0124 14:24:07.017983 11446 net.cpp:165] Memory required for data: 244761088
I0124 14:24:07.017987 11446 layer_factory.hpp:76] Creating layer drop6
I0124 14:24:07.017997 11446 net.cpp:106] Creating Layer drop6
I0124 14:24:07.018000 11446 net.cpp:454] drop6 <- relu6
I0124 14:24:07.018007 11446 net.cpp:411] drop6 -> drop6
I0124 14:24:07.018049 11446 net.cpp:150] Setting up drop6
I0124 14:24:07.018056 11446 net.cpp:157] Top shape: 128 2048 (262144)
I0124 14:24:07.018059 11446 net.cpp:165] Memory required for data: 245809664
I0124 14:24:07.018062 11446 layer_factory.hpp:76] Creating layer fc7
I0124 14:24:07.018070 11446 net.cpp:106] Creating Layer fc7
I0124 14:24:07.018074 11446 net.cpp:454] fc7 <- drop6
I0124 14:24:07.018079 11446 net.cpp:411] fc7 -> fc7
I0124 14:24:07.138789 11446 net.cpp:150] Setting up fc7
I0124 14:24:07.138855 11446 net.cpp:157] Top shape: 128 2048 (262144)
I0124 14:24:07.138859 11446 net.cpp:165] Memory required for data: 246858240
I0124 14:24:07.138870 11446 layer_factory.hpp:76] Creating layer relu7
I0124 14:24:07.138890 11446 net.cpp:106] Creating Layer relu7
I0124 14:24:07.138895 11446 net.cpp:454] relu7 <- fc7
I0124 14:24:07.138903 11446 net.cpp:411] relu7 -> relu7
I0124 14:24:07.139359 11446 net.cpp:150] Setting up relu7
I0124 14:24:07.139370 11446 net.cpp:157] Top shape: 128 2048 (262144)
I0124 14:24:07.139385 11446 net.cpp:165] Memory required for data: 247906816
I0124 14:24:07.139389 11446 layer_factory.hpp:76] Creating layer drop7
I0124 14:24:07.139416 11446 net.cpp:106] Creating Layer drop7
I0124 14:24:07.139420 11446 net.cpp:454] drop7 <- relu7
I0124 14:24:07.139426 11446 net.cpp:411] drop7 -> drop7
I0124 14:24:07.139466 11446 net.cpp:150] Setting up drop7
I0124 14:24:07.139472 11446 net.cpp:157] Top shape: 128 2048 (262144)
I0124 14:24:07.139475 11446 net.cpp:165] Memory required for data: 248955392
I0124 14:24:07.139478 11446 layer_factory.hpp:76] Creating layer fc8
I0124 14:24:07.139485 11446 net.cpp:106] Creating Layer fc8
I0124 14:24:07.139490 11446 net.cpp:454] fc8 <- drop7
I0124 14:24:07.139495 11446 net.cpp:411] fc8 -> fc8
I0124 14:24:07.197937 11446 net.cpp:150] Setting up fc8
I0124 14:24:07.197973 11446 net.cpp:157] Top shape: 128 1000 (128000)
I0124 14:24:07.197978 11446 net.cpp:165] Memory required for data: 249467392
I0124 14:24:07.197988 11446 layer_factory.hpp:76] Creating layer loss
I0124 14:24:07.197998 11446 net.cpp:106] Creating Layer loss
I0124 14:24:07.198001 11446 net.cpp:454] loss <- fc8
I0124 14:24:07.198006 11446 net.cpp:454] loss <- label
I0124 14:24:07.198014 11446 net.cpp:411] loss -> loss
I0124 14:24:07.198029 11446 layer_factory.hpp:76] Creating layer loss
I0124 14:24:07.199126 11446 net.cpp:150] Setting up loss
I0124 14:24:07.199137 11446 net.cpp:157] Top shape: (1)
I0124 14:24:07.199151 11446 net.cpp:160]     with loss weight 1
I0124 14:24:07.199196 11446 net.cpp:165] Memory required for data: 249467396
I0124 14:24:07.199200 11446 net.cpp:226] loss needs backward computation.
I0124 14:24:07.199204 11446 net.cpp:226] fc8 needs backward computation.
I0124 14:24:07.199209 11446 net.cpp:226] drop7 needs backward computation.
I0124 14:24:07.199213 11446 net.cpp:226] relu7 needs backward computation.
I0124 14:24:07.199216 11446 net.cpp:226] fc7 needs backward computation.
I0124 14:24:07.199219 11446 net.cpp:226] drop6 needs backward computation.
I0124 14:24:07.199224 11446 net.cpp:226] relu6 needs backward computation.
I0124 14:24:07.199228 11446 net.cpp:226] fc6 needs backward computation.
I0124 14:24:07.199230 11446 net.cpp:226] pool5 needs backward computation.
I0124 14:24:07.199234 11446 net.cpp:226] relu5 needs backward computation.
I0124 14:24:07.199236 11446 net.cpp:226] conv5 needs backward computation.
I0124 14:24:07.199239 11446 net.cpp:226] relu4 needs backward computation.
I0124 14:24:07.199242 11446 net.cpp:226] conv4 needs backward computation.
I0124 14:24:07.199246 11446 net.cpp:226] relu3 needs backward computation.
I0124 14:24:07.199249 11446 net.cpp:226] conv3 needs backward computation.
I0124 14:24:07.199252 11446 net.cpp:226] pool2 needs backward computation.
I0124 14:24:07.199255 11446 net.cpp:226] relu2 needs backward computation.
I0124 14:24:07.199259 11446 net.cpp:226] conv2 needs backward computation.
I0124 14:24:07.199261 11446 net.cpp:226] pool1 needs backward computation.
I0124 14:24:07.199265 11446 net.cpp:226] relu1 needs backward computation.
I0124 14:24:07.199268 11446 net.cpp:226] conv1 needs backward computation.
I0124 14:24:07.199272 11446 net.cpp:228] data does not need backward computation.
I0124 14:24:07.199275 11446 net.cpp:270] This network produces output loss
I0124 14:24:07.199292 11446 net.cpp:283] Network initialization done.
I0124 14:24:07.199424 11446 solver.cpp:180] Creating test net (#0) specified by net_param
I0124 14:24:07.199466 11446 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0124 14:24:07.199820 11446 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.04
    mirror: false
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 123
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/local/temporary/imagenet_clc_orig/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 14:24:07.199966 11446 layer_factory.hpp:76] Creating layer data
I0124 14:24:07.200079 11446 net.cpp:106] Creating Layer data
I0124 14:24:07.200089 11446 net.cpp:411] data -> data
I0124 14:24:07.200098 11446 net.cpp:411] data -> label
I0124 14:24:07.201123 11623 db_lmdb.cpp:38] Opened lmdb /local/temporary/imagenet_clc_orig/ilsvrc12_val_lmdb
I0124 14:24:07.203742 11446 data_layer.cpp:41] output data size: 50,3,128,128
I0124 14:24:07.224396 11446 net.cpp:150] Setting up data
I0124 14:24:07.224428 11446 net.cpp:157] Top shape: 50 3 128 128 (2457600)
I0124 14:24:07.224436 11446 net.cpp:157] Top shape: 50 (50)
I0124 14:24:07.224441 11446 net.cpp:165] Memory required for data: 9830600
I0124 14:24:07.224448 11446 layer_factory.hpp:76] Creating layer label_data_1_split
I0124 14:24:07.224473 11446 net.cpp:106] Creating Layer label_data_1_split
I0124 14:24:07.224488 11446 net.cpp:454] label_data_1_split <- label
I0124 14:24:07.224526 11446 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0124 14:24:07.224539 11446 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0124 14:24:07.224730 11446 net.cpp:150] Setting up label_data_1_split
I0124 14:24:07.224738 11446 net.cpp:157] Top shape: 50 (50)
I0124 14:24:07.224745 11446 net.cpp:157] Top shape: 50 (50)
I0124 14:24:07.224747 11446 net.cpp:165] Memory required for data: 9831000
I0124 14:24:07.224750 11446 layer_factory.hpp:76] Creating layer conv1
I0124 14:24:07.224763 11446 net.cpp:106] Creating Layer conv1
I0124 14:24:07.224766 11446 net.cpp:454] conv1 <- data
I0124 14:24:07.224772 11446 net.cpp:411] conv1 -> conv1
I0124 14:24:07.226763 11446 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 4356
I0124 14:24:07.226810 11446 net.cpp:150] Setting up conv1
I0124 14:24:07.226831 11446 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I0124 14:24:07.226835 11446 net.cpp:165] Memory required for data: 27111000
I0124 14:24:07.226845 11446 layer_factory.hpp:76] Creating layer relu1
I0124 14:24:07.226853 11446 net.cpp:106] Creating Layer relu1
I0124 14:24:07.226856 11446 net.cpp:454] relu1 <- conv1
I0124 14:24:07.226861 11446 net.cpp:411] relu1 -> relu1
I0124 14:24:07.227190 11446 net.cpp:150] Setting up relu1
I0124 14:24:07.227201 11446 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I0124 14:24:07.227205 11446 net.cpp:165] Memory required for data: 44391000
I0124 14:24:07.227208 11446 layer_factory.hpp:76] Creating layer pool1
I0124 14:24:07.227229 11446 net.cpp:106] Creating Layer pool1
I0124 14:24:07.227232 11446 net.cpp:454] pool1 <- relu1
I0124 14:24:07.227238 11446 net.cpp:411] pool1 -> pool1
I0124 14:24:07.227450 11446 net.cpp:150] Setting up pool1
I0124 14:24:07.227458 11446 net.cpp:157] Top shape: 50 96 15 15 (1080000)
I0124 14:24:07.227463 11446 net.cpp:165] Memory required for data: 48711000
I0124 14:24:07.227465 11446 layer_factory.hpp:76] Creating layer conv2
I0124 14:24:07.227473 11446 net.cpp:106] Creating Layer conv2
I0124 14:24:07.227478 11446 net.cpp:454] conv2 <- pool1
I0124 14:24:07.227494 11446 net.cpp:411] conv2 -> conv2
I0124 14:24:07.238107 11446 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 28800
I0124 14:24:07.238186 11446 net.cpp:150] Setting up conv2
I0124 14:24:07.238194 11446 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I0124 14:24:07.238198 11446 net.cpp:165] Memory required for data: 60231000
I0124 14:24:07.238209 11446 layer_factory.hpp:76] Creating layer relu2
I0124 14:24:07.238219 11446 net.cpp:106] Creating Layer relu2
I0124 14:24:07.238224 11446 net.cpp:454] relu2 <- conv2
I0124 14:24:07.238230 11446 net.cpp:411] relu2 -> relu2
I0124 14:24:07.238442 11446 net.cpp:150] Setting up relu2
I0124 14:24:07.238452 11446 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I0124 14:24:07.238456 11446 net.cpp:165] Memory required for data: 71751000
I0124 14:24:07.238458 11446 layer_factory.hpp:76] Creating layer pool2
I0124 14:24:07.238467 11446 net.cpp:106] Creating Layer pool2
I0124 14:24:07.238471 11446 net.cpp:454] pool2 <- relu2
I0124 14:24:07.238478 11446 net.cpp:411] pool2 -> pool2
I0124 14:24:07.238816 11446 net.cpp:150] Setting up pool2
I0124 14:24:07.238826 11446 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0124 14:24:07.238831 11446 net.cpp:165] Memory required for data: 74259800
I0124 14:24:07.238834 11446 layer_factory.hpp:76] Creating layer conv3
I0124 14:24:07.238857 11446 net.cpp:106] Creating Layer conv3
I0124 14:24:07.238862 11446 net.cpp:454] conv3 <- pool2
I0124 14:24:07.238869 11446 net.cpp:411] conv3 -> conv3
I0124 14:24:07.265502 11446 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 27648
I0124 14:24:07.265535 11446 net.cpp:150] Setting up conv3
I0124 14:24:07.265545 11446 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0124 14:24:07.265548 11446 net.cpp:165] Memory required for data: 78023000
I0124 14:24:07.265564 11446 layer_factory.hpp:76] Creating layer relu3
I0124 14:24:07.265574 11446 net.cpp:106] Creating Layer relu3
I0124 14:24:07.265583 11446 net.cpp:454] relu3 <- conv3
I0124 14:24:07.265594 11446 net.cpp:411] relu3 -> relu3
I0124 14:24:07.265810 11446 net.cpp:150] Setting up relu3
I0124 14:24:07.265821 11446 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0124 14:24:07.265826 11446 net.cpp:165] Memory required for data: 81786200
I0124 14:24:07.265832 11446 layer_factory.hpp:76] Creating layer conv4
I0124 14:24:07.265862 11446 net.cpp:106] Creating Layer conv4
I0124 14:24:07.265868 11446 net.cpp:454] conv4 <- relu3
I0124 14:24:07.265882 11446 net.cpp:411] conv4 -> conv4
I0124 14:24:07.287120 11446 net.cpp:150] Setting up conv4
I0124 14:24:07.287158 11446 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0124 14:24:07.287161 11446 net.cpp:165] Memory required for data: 85549400
I0124 14:24:07.287171 11446 layer_factory.hpp:76] Creating layer relu4
I0124 14:24:07.287180 11446 net.cpp:106] Creating Layer relu4
I0124 14:24:07.287184 11446 net.cpp:454] relu4 <- conv4
I0124 14:24:07.287192 11446 net.cpp:411] relu4 -> relu4
I0124 14:24:07.287407 11446 net.cpp:150] Setting up relu4
I0124 14:24:07.287416 11446 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0124 14:24:07.287420 11446 net.cpp:165] Memory required for data: 89312600
I0124 14:24:07.287423 11446 layer_factory.hpp:76] Creating layer conv5
I0124 14:24:07.287437 11446 net.cpp:106] Creating Layer conv5
I0124 14:24:07.287443 11446 net.cpp:454] conv5 <- relu4
I0124 14:24:07.287449 11446 net.cpp:411] conv5 -> conv5
I0124 14:24:07.305888 11446 net.cpp:150] Setting up conv5
I0124 14:24:07.305925 11446 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0124 14:24:07.305929 11446 net.cpp:165] Memory required for data: 91821400
I0124 14:24:07.305943 11446 layer_factory.hpp:76] Creating layer relu5
I0124 14:24:07.305953 11446 net.cpp:106] Creating Layer relu5
I0124 14:24:07.305956 11446 net.cpp:454] relu5 <- conv5
I0124 14:24:07.305964 11446 net.cpp:411] relu5 -> relu5
I0124 14:24:07.306300 11446 net.cpp:150] Setting up relu5
I0124 14:24:07.306313 11446 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0124 14:24:07.306315 11446 net.cpp:165] Memory required for data: 94330200
I0124 14:24:07.306319 11446 layer_factory.hpp:76] Creating layer pool5
I0124 14:24:07.306329 11446 net.cpp:106] Creating Layer pool5
I0124 14:24:07.306331 11446 net.cpp:454] pool5 <- relu5
I0124 14:24:07.306361 11446 net.cpp:411] pool5 -> pool5
I0124 14:24:07.306584 11446 net.cpp:150] Setting up pool5
I0124 14:24:07.306593 11446 net.cpp:157] Top shape: 50 256 3 3 (115200)
I0124 14:24:07.306596 11446 net.cpp:165] Memory required for data: 94791000
I0124 14:24:07.306601 11446 layer_factory.hpp:76] Creating layer fc6
I0124 14:24:07.306609 11446 net.cpp:106] Creating Layer fc6
I0124 14:24:07.306612 11446 net.cpp:454] fc6 <- pool5
I0124 14:24:07.306618 11446 net.cpp:411] fc6 -> fc6
I0124 14:24:07.446413 11446 net.cpp:150] Setting up fc6
I0124 14:24:07.446451 11446 net.cpp:157] Top shape: 50 2048 (102400)
I0124 14:24:07.446455 11446 net.cpp:165] Memory required for data: 95200600
I0124 14:24:07.446465 11446 layer_factory.hpp:76] Creating layer relu6
I0124 14:24:07.446478 11446 net.cpp:106] Creating Layer relu6
I0124 14:24:07.446482 11446 net.cpp:454] relu6 <- fc6
I0124 14:24:07.446490 11446 net.cpp:411] relu6 -> relu6
I0124 14:24:07.446971 11446 net.cpp:150] Setting up relu6
I0124 14:24:07.446982 11446 net.cpp:157] Top shape: 50 2048 (102400)
I0124 14:24:07.446986 11446 net.cpp:165] Memory required for data: 95610200
I0124 14:24:07.446990 11446 layer_factory.hpp:76] Creating layer drop6
I0124 14:24:07.446998 11446 net.cpp:106] Creating Layer drop6
I0124 14:24:07.447001 11446 net.cpp:454] drop6 <- relu6
I0124 14:24:07.447006 11446 net.cpp:411] drop6 -> drop6
I0124 14:24:07.447051 11446 net.cpp:150] Setting up drop6
I0124 14:24:07.447057 11446 net.cpp:157] Top shape: 50 2048 (102400)
I0124 14:24:07.447060 11446 net.cpp:165] Memory required for data: 96019800
I0124 14:24:07.447063 11446 layer_factory.hpp:76] Creating layer fc7
I0124 14:24:07.447073 11446 net.cpp:106] Creating Layer fc7
I0124 14:24:07.447079 11446 net.cpp:454] fc7 <- drop6
I0124 14:24:07.447088 11446 net.cpp:411] fc7 -> fc7
I0124 14:24:07.571804 11446 net.cpp:150] Setting up fc7
I0124 14:24:07.571830 11446 net.cpp:157] Top shape: 50 2048 (102400)
I0124 14:24:07.571835 11446 net.cpp:165] Memory required for data: 96429400
I0124 14:24:07.571846 11446 layer_factory.hpp:76] Creating layer relu7
I0124 14:24:07.571858 11446 net.cpp:106] Creating Layer relu7
I0124 14:24:07.571877 11446 net.cpp:454] relu7 <- fc7
I0124 14:24:07.571899 11446 net.cpp:411] relu7 -> relu7
I0124 14:24:07.572170 11446 net.cpp:150] Setting up relu7
I0124 14:24:07.572181 11446 net.cpp:157] Top shape: 50 2048 (102400)
I0124 14:24:07.572185 11446 net.cpp:165] Memory required for data: 96839000
I0124 14:24:07.572187 11446 layer_factory.hpp:76] Creating layer drop7
I0124 14:24:07.572203 11446 net.cpp:106] Creating Layer drop7
I0124 14:24:07.572208 11446 net.cpp:454] drop7 <- relu7
I0124 14:24:07.572214 11446 net.cpp:411] drop7 -> drop7
I0124 14:24:07.572258 11446 net.cpp:150] Setting up drop7
I0124 14:24:07.572264 11446 net.cpp:157] Top shape: 50 2048 (102400)
I0124 14:24:07.572268 11446 net.cpp:165] Memory required for data: 97248600
I0124 14:24:07.572271 11446 layer_factory.hpp:76] Creating layer fc8
I0124 14:24:07.572284 11446 net.cpp:106] Creating Layer fc8
I0124 14:24:07.572289 11446 net.cpp:454] fc8 <- drop7
I0124 14:24:07.572293 11446 net.cpp:411] fc8 -> fc8
I0124 14:24:07.629952 11446 net.cpp:150] Setting up fc8
I0124 14:24:07.629979 11446 net.cpp:157] Top shape: 50 1000 (50000)
I0124 14:24:07.629982 11446 net.cpp:165] Memory required for data: 97448600
I0124 14:24:07.629992 11446 layer_factory.hpp:76] Creating layer fc8_fc8_0_split
I0124 14:24:07.630008 11446 net.cpp:106] Creating Layer fc8_fc8_0_split
I0124 14:24:07.630014 11446 net.cpp:454] fc8_fc8_0_split <- fc8
I0124 14:24:07.630023 11446 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0124 14:24:07.630033 11446 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0124 14:24:07.630080 11446 net.cpp:150] Setting up fc8_fc8_0_split
I0124 14:24:07.630086 11446 net.cpp:157] Top shape: 50 1000 (50000)
I0124 14:24:07.630090 11446 net.cpp:157] Top shape: 50 1000 (50000)
I0124 14:24:07.630094 11446 net.cpp:165] Memory required for data: 97848600
I0124 14:24:07.630096 11446 layer_factory.hpp:76] Creating layer accuracy
I0124 14:24:07.630137 11446 net.cpp:106] Creating Layer accuracy
I0124 14:24:07.630142 11446 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0124 14:24:07.630147 11446 net.cpp:454] accuracy <- label_data_1_split_0
I0124 14:24:07.630152 11446 net.cpp:411] accuracy -> accuracy
I0124 14:24:07.630162 11446 net.cpp:150] Setting up accuracy
I0124 14:24:07.630167 11446 net.cpp:157] Top shape: (1)
I0124 14:24:07.630169 11446 net.cpp:165] Memory required for data: 97848604
I0124 14:24:07.630172 11446 layer_factory.hpp:76] Creating layer loss
I0124 14:24:07.630177 11446 net.cpp:106] Creating Layer loss
I0124 14:24:07.630180 11446 net.cpp:454] loss <- fc8_fc8_0_split_1
I0124 14:24:07.630183 11446 net.cpp:454] loss <- label_data_1_split_1
I0124 14:24:07.630190 11446 net.cpp:411] loss -> loss
I0124 14:24:07.630200 11446 layer_factory.hpp:76] Creating layer loss
I0124 14:24:07.630818 11446 net.cpp:150] Setting up loss
I0124 14:24:07.630830 11446 net.cpp:157] Top shape: (1)
I0124 14:24:07.630843 11446 net.cpp:160]     with loss weight 1
I0124 14:24:07.630853 11446 net.cpp:165] Memory required for data: 97848608
I0124 14:24:07.630856 11446 net.cpp:226] loss needs backward computation.
I0124 14:24:07.630862 11446 net.cpp:228] accuracy does not need backward computation.
I0124 14:24:07.630867 11446 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0124 14:24:07.630868 11446 net.cpp:226] fc8 needs backward computation.
I0124 14:24:07.630872 11446 net.cpp:226] drop7 needs backward computation.
I0124 14:24:07.630877 11446 net.cpp:226] relu7 needs backward computation.
I0124 14:24:07.630879 11446 net.cpp:226] fc7 needs backward computation.
I0124 14:24:07.630882 11446 net.cpp:226] drop6 needs backward computation.
I0124 14:24:07.630897 11446 net.cpp:226] relu6 needs backward computation.
I0124 14:24:07.630899 11446 net.cpp:226] fc6 needs backward computation.
I0124 14:24:07.630903 11446 net.cpp:226] pool5 needs backward computation.
I0124 14:24:07.630905 11446 net.cpp:226] relu5 needs backward computation.
I0124 14:24:07.630908 11446 net.cpp:226] conv5 needs backward computation.
I0124 14:24:07.630911 11446 net.cpp:226] relu4 needs backward computation.
I0124 14:24:07.630914 11446 net.cpp:226] conv4 needs backward computation.
I0124 14:24:07.630918 11446 net.cpp:226] relu3 needs backward computation.
I0124 14:24:07.630921 11446 net.cpp:226] conv3 needs backward computation.
I0124 14:24:07.630923 11446 net.cpp:226] pool2 needs backward computation.
I0124 14:24:07.630928 11446 net.cpp:226] relu2 needs backward computation.
I0124 14:24:07.630930 11446 net.cpp:226] conv2 needs backward computation.
I0124 14:24:07.630934 11446 net.cpp:226] pool1 needs backward computation.
I0124 14:24:07.630935 11446 net.cpp:226] relu1 needs backward computation.
I0124 14:24:07.630939 11446 net.cpp:226] conv1 needs backward computation.
I0124 14:24:07.630942 11446 net.cpp:228] label_data_1_split does not need backward computation.
I0124 14:24:07.630946 11446 net.cpp:228] data does not need backward computation.
I0124 14:24:07.630949 11446 net.cpp:270] This network produces output accuracy
I0124 14:24:07.630952 11446 net.cpp:270] This network produces output loss
I0124 14:24:07.630973 11446 net.cpp:283] Network initialization done.
I0124 14:24:07.631091 11446 solver.cpp:59] Solver scaffolding done.
I0124 14:24:07.631878 11446 caffe.cpp:128] Finetuning from caffenet128_lsuv_adagrad.prototxt.caffemodel
I0124 14:24:07.738852 11446 caffe.cpp:212] Starting Optimization
I0124 14:24:07.738878 11446 solver.cpp:287] Solving CaffeNet
I0124 14:24:07.738881 11446 solver.cpp:288] Learning Rate Policy: fixed
I0124 14:24:07.802559 11446 solver.cpp:236] Iteration 0, loss = 7.43359
I0124 14:24:07.802592 11446 solver.cpp:252]     Train net output #0: loss = 7.43359 (* 1 = 7.43359 loss)
I0124 14:24:07.802599 11446 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0124 14:24:08.196527 11446 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 14:24:12.309450 11446 solver.cpp:236] Iteration 20, loss = 6.90231
I0124 14:24:12.309499 11446 solver.cpp:252]     Train net output #0: loss = 6.90231 (* 1 = 6.90231 loss)
I0124 14:24:12.309557 11446 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I0124 14:24:17.961007 11446 solver.cpp:236] Iteration 40, loss = 6.91266
I0124 14:24:17.961058 11446 solver.cpp:252]     Train net output #0: loss = 6.91266 (* 1 = 6.91266 loss)
I0124 14:24:17.961068 11446 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I0124 14:24:22.881085 11446 solver.cpp:236] Iteration 60, loss = 6.90482
I0124 14:24:22.881162 11446 solver.cpp:252]     Train net output #0: loss = 6.90482 (* 1 = 6.90482 loss)
I0124 14:24:22.881176 11446 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I0124 14:24:27.598984 11446 solver.cpp:236] Iteration 80, loss = 6.90554
I0124 14:24:27.599046 11446 solver.cpp:252]     Train net output #0: loss = 6.90554 (* 1 = 6.90554 loss)
I0124 14:24:27.599057 11446 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I0124 14:24:32.318269 11446 solver.cpp:236] Iteration 100, loss = 6.90715
I0124 14:24:32.318339 11446 solver.cpp:252]     Train net output #0: loss = 6.90715 (* 1 = 6.90715 loss)
I0124 14:24:32.318351 11446 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I0124 14:24:37.086962 11446 solver.cpp:236] Iteration 120, loss = 6.91068
I0124 14:24:37.087267 11446 solver.cpp:252]     Train net output #0: loss = 6.91068 (* 1 = 6.91068 loss)
I0124 14:24:37.087291 11446 sgd_solver.cpp:106] Iteration 120, lr = 0.001
I0124 14:24:42.016047 11446 solver.cpp:236] Iteration 140, loss = 6.91723
I0124 14:24:42.016101 11446 solver.cpp:252]     Train net output #0: loss = 6.91723 (* 1 = 6.91723 loss)
I0124 14:24:42.016109 11446 sgd_solver.cpp:106] Iteration 140, lr = 0.001
I0124 14:24:47.102653 11446 solver.cpp:236] Iteration 160, loss = 6.91438
I0124 14:24:47.102733 11446 solver.cpp:252]     Train net output #0: loss = 6.91438 (* 1 = 6.91438 loss)
I0124 14:24:47.102752 11446 sgd_solver.cpp:106] Iteration 160, lr = 0.001
I0124 14:24:52.001004 11446 solver.cpp:236] Iteration 180, loss = 6.91401
I0124 14:24:52.001076 11446 solver.cpp:252]     Train net output #0: loss = 6.91401 (* 1 = 6.91401 loss)
I0124 14:24:52.001087 11446 sgd_solver.cpp:106] Iteration 180, lr = 0.001
I0124 14:24:56.951817 11446 solver.cpp:236] Iteration 200, loss = 6.90657
I0124 14:24:56.951877 11446 solver.cpp:252]     Train net output #0: loss = 6.90657 (* 1 = 6.90657 loss)
I0124 14:24:56.951891 11446 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0124 14:25:02.216481 11446 solver.cpp:236] Iteration 220, loss = 6.91219
I0124 14:25:02.216548 11446 solver.cpp:252]     Train net output #0: loss = 6.91219 (* 1 = 6.91219 loss)
I0124 14:25:02.216560 11446 sgd_solver.cpp:106] Iteration 220, lr = 0.001
I0124 14:25:07.443109 11446 solver.cpp:236] Iteration 240, loss = 6.91063
I0124 14:25:07.443212 11446 solver.cpp:252]     Train net output #0: loss = 6.91063 (* 1 = 6.91063 loss)
I0124 14:25:07.443223 11446 sgd_solver.cpp:106] Iteration 240, lr = 0.001
I0124 14:25:12.161399 11446 solver.cpp:236] Iteration 260, loss = 6.90609
I0124 14:25:12.161455 11446 solver.cpp:252]     Train net output #0: loss = 6.90609 (* 1 = 6.90609 loss)
I0124 14:25:12.161463 11446 sgd_solver.cpp:106] Iteration 260, lr = 0.001
I0124 14:25:16.738513 11446 solver.cpp:236] Iteration 280, loss = 6.90831
I0124 14:25:16.738561 11446 solver.cpp:252]     Train net output #0: loss = 6.90831 (* 1 = 6.90831 loss)
I0124 14:25:16.738569 11446 sgd_solver.cpp:106] Iteration 280, lr = 0.001
I0124 14:25:21.390173 11446 solver.cpp:236] Iteration 300, loss = 6.90598
I0124 14:25:21.390256 11446 solver.cpp:252]     Train net output #0: loss = 6.90598 (* 1 = 6.90598 loss)
I0124 14:25:21.390267 11446 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I0124 14:25:26.070106 11446 solver.cpp:236] Iteration 320, loss = 6.91142
I0124 14:25:26.070163 11446 solver.cpp:252]     Train net output #0: loss = 6.91142 (* 1 = 6.91142 loss)
I0124 14:25:26.070169 11446 sgd_solver.cpp:106] Iteration 320, lr = 0.001
I0124 14:25:31.014557 11446 solver.cpp:236] Iteration 340, loss = 6.90172
I0124 14:25:31.014628 11446 solver.cpp:252]     Train net output #0: loss = 6.90172 (* 1 = 6.90172 loss)
I0124 14:25:31.014639 11446 sgd_solver.cpp:106] Iteration 340, lr = 0.001
I0124 14:25:36.199141 11446 solver.cpp:236] Iteration 360, loss = 6.91229
I0124 14:25:36.199220 11446 solver.cpp:252]     Train net output #0: loss = 6.91229 (* 1 = 6.91229 loss)
I0124 14:25:36.199231 11446 sgd_solver.cpp:106] Iteration 360, lr = 0.001
I0124 14:25:41.316424 11446 solver.cpp:236] Iteration 380, loss = 6.90822
I0124 14:25:41.316704 11446 solver.cpp:252]     Train net output #0: loss = 6.90822 (* 1 = 6.90822 loss)
I0124 14:25:41.316726 11446 sgd_solver.cpp:106] Iteration 380, lr = 0.001
I0124 14:25:46.316902 11446 solver.cpp:236] Iteration 400, loss = 6.90447
I0124 14:25:46.316949 11446 solver.cpp:252]     Train net output #0: loss = 6.90447 (* 1 = 6.90447 loss)
I0124 14:25:46.316958 11446 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0124 14:25:51.792773 11446 solver.cpp:236] Iteration 420, loss = 6.91324
I0124 14:25:51.792827 11446 solver.cpp:252]     Train net output #0: loss = 6.91324 (* 1 = 6.91324 loss)
I0124 14:25:51.792843 11446 sgd_solver.cpp:106] Iteration 420, lr = 0.001
I0124 14:25:56.650210 11446 solver.cpp:236] Iteration 440, loss = 6.90611
I0124 14:25:56.650261 11446 solver.cpp:252]     Train net output #0: loss = 6.90611 (* 1 = 6.90611 loss)
I0124 14:25:56.650274 11446 sgd_solver.cpp:106] Iteration 440, lr = 0.001
I0124 14:26:01.427484 11446 solver.cpp:236] Iteration 460, loss = 6.9065
I0124 14:26:01.427537 11446 solver.cpp:252]     Train net output #0: loss = 6.9065 (* 1 = 6.9065 loss)
I0124 14:26:01.427552 11446 sgd_solver.cpp:106] Iteration 460, lr = 0.001
I0124 14:26:06.217464 11446 solver.cpp:236] Iteration 480, loss = 6.9085
I0124 14:26:06.217519 11446 solver.cpp:252]     Train net output #0: loss = 6.9085 (* 1 = 6.9085 loss)
I0124 14:26:06.217532 11446 sgd_solver.cpp:106] Iteration 480, lr = 0.001
I0124 14:26:10.955070 11446 solver.cpp:236] Iteration 500, loss = 6.90705
I0124 14:26:10.955127 11446 solver.cpp:252]     Train net output #0: loss = 6.90705 (* 1 = 6.90705 loss)
I0124 14:26:10.955137 11446 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I0124 14:26:15.867962 11446 solver.cpp:236] Iteration 520, loss = 6.90306
I0124 14:26:15.868158 11446 solver.cpp:252]     Train net output #0: loss = 6.90306 (* 1 = 6.90306 loss)
I0124 14:26:15.868170 11446 sgd_solver.cpp:106] Iteration 520, lr = 0.001
I0124 14:26:20.965071 11446 solver.cpp:236] Iteration 540, loss = 6.90291
I0124 14:26:20.965117 11446 solver.cpp:252]     Train net output #0: loss = 6.90291 (* 1 = 6.90291 loss)
I0124 14:26:20.965127 11446 sgd_solver.cpp:106] Iteration 540, lr = 0.001
I0124 14:26:25.967972 11446 solver.cpp:236] Iteration 560, loss = 6.90916
I0124 14:26:25.968039 11446 solver.cpp:252]     Train net output #0: loss = 6.90916 (* 1 = 6.90916 loss)
I0124 14:26:25.968057 11446 sgd_solver.cpp:106] Iteration 560, lr = 0.001
I0124 14:26:31.215088 11446 solver.cpp:236] Iteration 580, loss = 6.90579
I0124 14:26:31.215158 11446 solver.cpp:252]     Train net output #0: loss = 6.90579 (* 1 = 6.90579 loss)
I0124 14:26:31.215170 11446 sgd_solver.cpp:106] Iteration 580, lr = 0.001
I0124 14:26:36.871719 11446 solver.cpp:236] Iteration 600, loss = 6.90578
I0124 14:26:36.871779 11446 solver.cpp:252]     Train net output #0: loss = 6.90578 (* 1 = 6.90578 loss)
I0124 14:26:36.871791 11446 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0124 14:26:42.032305 11446 solver.cpp:236] Iteration 620, loss = 6.90847
I0124 14:26:42.032357 11446 solver.cpp:252]     Train net output #0: loss = 6.90847 (* 1 = 6.90847 loss)
I0124 14:26:42.032367 11446 sgd_solver.cpp:106] Iteration 620, lr = 0.001
I0124 14:26:46.794373 11446 solver.cpp:236] Iteration 640, loss = 6.90719
I0124 14:26:46.794636 11446 solver.cpp:252]     Train net output #0: loss = 6.90719 (* 1 = 6.90719 loss)
I0124 14:26:46.794658 11446 sgd_solver.cpp:106] Iteration 640, lr = 0.001
I0124 14:26:51.589792 11446 solver.cpp:236] Iteration 660, loss = 6.91224
I0124 14:26:51.589854 11446 solver.cpp:252]     Train net output #0: loss = 6.91224 (* 1 = 6.91224 loss)
I0124 14:26:51.589870 11446 sgd_solver.cpp:106] Iteration 660, lr = 0.001
I0124 14:26:56.429491 11446 solver.cpp:236] Iteration 680, loss = 6.91621
I0124 14:26:56.429571 11446 solver.cpp:252]     Train net output #0: loss = 6.91621 (* 1 = 6.91621 loss)
I0124 14:26:56.429589 11446 sgd_solver.cpp:106] Iteration 680, lr = 0.001
I0124 14:27:01.295558 11446 solver.cpp:236] Iteration 700, loss = 6.91802
I0124 14:27:01.295634 11446 solver.cpp:252]     Train net output #0: loss = 6.91802 (* 1 = 6.91802 loss)
I0124 14:27:01.295647 11446 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I0124 14:27:06.267962 11446 solver.cpp:236] Iteration 720, loss = 6.91181
I0124 14:27:06.268026 11446 solver.cpp:252]     Train net output #0: loss = 6.91181 (* 1 = 6.91181 loss)
I0124 14:27:06.268038 11446 sgd_solver.cpp:106] Iteration 720, lr = 0.001
I0124 14:27:11.253218 11446 solver.cpp:236] Iteration 740, loss = 6.91004
I0124 14:27:11.253280 11446 solver.cpp:252]     Train net output #0: loss = 6.91004 (* 1 = 6.91004 loss)
I0124 14:27:11.253295 11446 sgd_solver.cpp:106] Iteration 740, lr = 0.001
I0124 14:27:16.550799 11446 solver.cpp:236] Iteration 760, loss = 6.91022
I0124 14:27:16.550936 11446 solver.cpp:252]     Train net output #0: loss = 6.91022 (* 1 = 6.91022 loss)
I0124 14:27:16.550957 11446 sgd_solver.cpp:106] Iteration 760, lr = 0.001
I0124 14:27:21.453862 11446 solver.cpp:236] Iteration 780, loss = 6.91105
I0124 14:27:21.454190 11446 solver.cpp:252]     Train net output #0: loss = 6.91105 (* 1 = 6.91105 loss)
I0124 14:27:21.454205 11446 sgd_solver.cpp:106] Iteration 780, lr = 0.001
I0124 14:27:26.506281 11446 solver.cpp:236] Iteration 800, loss = 6.9082
I0124 14:27:26.506340 11446 solver.cpp:252]     Train net output #0: loss = 6.9082 (* 1 = 6.9082 loss)
I0124 14:27:26.506350 11446 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0124 14:27:31.165267 11446 solver.cpp:236] Iteration 820, loss = 6.90793
I0124 14:27:31.165303 11446 solver.cpp:252]     Train net output #0: loss = 6.90793 (* 1 = 6.90793 loss)
I0124 14:27:31.165309 11446 sgd_solver.cpp:106] Iteration 820, lr = 0.001
I0124 14:27:35.757556 11446 solver.cpp:236] Iteration 840, loss = 6.91
I0124 14:27:35.757596 11446 solver.cpp:252]     Train net output #0: loss = 6.91 (* 1 = 6.91 loss)
I0124 14:27:35.757606 11446 sgd_solver.cpp:106] Iteration 840, lr = 0.001
