I0124 13:39:48.115717 28535 caffe.cpp:184] Using GPUs 0
I0124 13:39:48.258903 28535 solver.cpp:48] Initializing solver from parameters: 
test_iter: 200
test_interval: 2000
base_lr: 0.01
display: 20
max_iter: 320000
lr_policy: "fixed"
momentum: 0.95
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "snapshots1/caffenet128_adam"
solver_mode: GPU
device_id: 0
net_param {
  name: "CaffeNet"
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TRAIN
    }
    transform_param {
      scale: 0.04
      mirror: true
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 124
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/home/old-ufo/datasets/imagenet/ilsvrc12_train_shuffle_lmdb"
      batch_size: 256
      backend: LMDB
    }
  }
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TEST
    }
    transform_param {
      scale: 0.04
      mirror: false
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 123
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/home/old-ufo/datasets/imagenet/ilsvrc12_val_lmdb"
      batch_size: 250
      backend: LMDB
    }
  }
  layer {
    name: "conv1"
    type: "Convolution"
    bottom: "data"
    top: "conv1"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 96
      kernel_size: 11
      stride: 4
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu1"
    type: "ReLU"
    bottom: "conv1"
    top: "relu1"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "relu1"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv2"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 2
      kernel_size: 5
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu2"
    type: "ReLU"
    bottom: "conv2"
    top: "conv2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "conv2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv3"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu3"
    type: "ReLU"
    bottom: "conv3"
    top: "conv3"
  }
  layer {
    name: "conv4"
    type: "Convolution"
    bottom: "conv3"
    top: "conv4"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu4"
    type: "ReLU"
    bottom: "conv4"
    top: "conv4"
  }
  layer {
    name: "conv5"
    type: "Convolution"
    bottom: "conv4"
    top: "conv5"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu5"
    type: "ReLU"
    bottom: "conv5"
    top: "conv5"
  }
  layer {
    name: "pool5"
    type: "Pooling"
    bottom: "conv5"
    top: "pool5"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "pool5"
    top: "fc6"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu6"
    type: "ReLU"
    bottom: "fc6"
    top: "fc6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "fc6"
    top: "fc6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "fc6"
    top: "fc7"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu7"
    type: "ReLU"
    bottom: "fc7"
    top: "fc7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "fc7"
    top: "fc7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc8"
    type: "InnerProduct"
    bottom: "fc7"
    top: "fc8"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 1000
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "accuracy"
    type: "Accuracy"
    bottom: "fc8"
    bottom: "label"
    top: "accuracy"
    include {
      phase: TEST
    }
  }
  layer {
    name: "loss"
    type: "SoftmaxWithLoss"
    bottom: "fc8"
    bottom: "label"
    top: "loss"
  }
}
test_initialization: false
average_loss: 20
iter_size: 1
momentum2: 0.999
type: "Adam"
I0124 13:39:48.691696 28535 solver.cpp:86] Creating training net specified in net_param.
I0124 13:39:48.691825 28535 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0124 13:39:48.691851 28535 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0124 13:39:48.692005 28535 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.04
    mirror: true
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 124
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/old-ufo/datasets/imagenet/ilsvrc12_train_shuffle_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 13:39:48.692122 28535 layer_factory.hpp:76] Creating layer data
I0124 13:39:48.692677 28535 net.cpp:106] Creating Layer data
I0124 13:39:48.692692 28535 net.cpp:411] data -> data
I0124 13:39:48.692730 28535 net.cpp:411] data -> label
I0124 13:39:48.693405 28539 db_lmdb.cpp:38] Opened lmdb /home/old-ufo/datasets/imagenet/ilsvrc12_train_shuffle_lmdb
I0124 13:39:48.712023 28535 data_layer.cpp:41] output data size: 256,3,128,128
I0124 13:39:48.781827 28535 net.cpp:150] Setting up data
I0124 13:39:48.781852 28535 net.cpp:157] Top shape: 256 3 128 128 (12582912)
I0124 13:39:48.781858 28535 net.cpp:157] Top shape: 256 (256)
I0124 13:39:48.781862 28535 net.cpp:165] Memory required for data: 50332672
I0124 13:39:48.781873 28535 layer_factory.hpp:76] Creating layer conv1
I0124 13:39:48.781893 28535 net.cpp:106] Creating Layer conv1
I0124 13:39:48.781899 28535 net.cpp:454] conv1 <- data
I0124 13:39:48.781914 28535 net.cpp:411] conv1 -> conv1
I0124 13:39:48.958947 28535 net.cpp:150] Setting up conv1
I0124 13:39:48.958976 28535 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I0124 13:39:48.958981 28535 net.cpp:165] Memory required for data: 138806272
I0124 13:39:48.959003 28535 layer_factory.hpp:76] Creating layer relu1
I0124 13:39:48.959017 28535 net.cpp:106] Creating Layer relu1
I0124 13:39:48.959022 28535 net.cpp:454] relu1 <- conv1
I0124 13:39:48.959029 28535 net.cpp:411] relu1 -> relu1
I0124 13:39:48.959756 28535 net.cpp:150] Setting up relu1
I0124 13:39:48.959770 28535 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I0124 13:39:48.959775 28535 net.cpp:165] Memory required for data: 227279872
I0124 13:39:48.959780 28535 layer_factory.hpp:76] Creating layer pool1
I0124 13:39:48.959789 28535 net.cpp:106] Creating Layer pool1
I0124 13:39:48.959792 28535 net.cpp:454] pool1 <- relu1
I0124 13:39:48.959800 28535 net.cpp:411] pool1 -> pool1
I0124 13:39:48.960507 28535 net.cpp:150] Setting up pool1
I0124 13:39:48.960520 28535 net.cpp:157] Top shape: 256 96 15 15 (5529600)
I0124 13:39:48.960523 28535 net.cpp:165] Memory required for data: 249398272
I0124 13:39:48.960528 28535 layer_factory.hpp:76] Creating layer conv2
I0124 13:39:48.960539 28535 net.cpp:106] Creating Layer conv2
I0124 13:39:48.960543 28535 net.cpp:454] conv2 <- pool1
I0124 13:39:48.960551 28535 net.cpp:411] conv2 -> conv2
I0124 13:39:48.976433 28535 net.cpp:150] Setting up conv2
I0124 13:39:48.976461 28535 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I0124 13:39:48.976467 28535 net.cpp:165] Memory required for data: 308380672
I0124 13:39:48.976483 28535 layer_factory.hpp:76] Creating layer relu2
I0124 13:39:48.976495 28535 net.cpp:106] Creating Layer relu2
I0124 13:39:48.976501 28535 net.cpp:454] relu2 <- conv2
I0124 13:39:48.976510 28535 net.cpp:397] relu2 -> conv2 (in-place)
I0124 13:39:48.977237 28535 net.cpp:150] Setting up relu2
I0124 13:39:48.977249 28535 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I0124 13:39:48.977254 28535 net.cpp:165] Memory required for data: 367363072
I0124 13:39:48.977258 28535 layer_factory.hpp:76] Creating layer pool2
I0124 13:39:48.977267 28535 net.cpp:106] Creating Layer pool2
I0124 13:39:48.977270 28535 net.cpp:454] pool2 <- conv2
I0124 13:39:48.977277 28535 net.cpp:411] pool2 -> pool2
I0124 13:39:48.978127 28535 net.cpp:150] Setting up pool2
I0124 13:39:48.978140 28535 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I0124 13:39:48.978145 28535 net.cpp:165] Memory required for data: 380208128
I0124 13:39:48.978150 28535 layer_factory.hpp:76] Creating layer conv3
I0124 13:39:48.978163 28535 net.cpp:106] Creating Layer conv3
I0124 13:39:48.978168 28535 net.cpp:454] conv3 <- pool2
I0124 13:39:48.978174 28535 net.cpp:411] conv3 -> conv3
I0124 13:39:49.012564 28535 net.cpp:150] Setting up conv3
I0124 13:39:49.012591 28535 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 13:39:49.012596 28535 net.cpp:165] Memory required for data: 399475712
I0124 13:39:49.012612 28535 layer_factory.hpp:76] Creating layer relu3
I0124 13:39:49.012625 28535 net.cpp:106] Creating Layer relu3
I0124 13:39:49.012630 28535 net.cpp:454] relu3 <- conv3
I0124 13:39:49.012639 28535 net.cpp:397] relu3 -> conv3 (in-place)
I0124 13:39:49.013375 28535 net.cpp:150] Setting up relu3
I0124 13:39:49.013387 28535 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 13:39:49.013392 28535 net.cpp:165] Memory required for data: 418743296
I0124 13:39:49.013414 28535 layer_factory.hpp:76] Creating layer conv4
I0124 13:39:49.013427 28535 net.cpp:106] Creating Layer conv4
I0124 13:39:49.013432 28535 net.cpp:454] conv4 <- conv3
I0124 13:39:49.013440 28535 net.cpp:411] conv4 -> conv4
I0124 13:39:49.041963 28535 net.cpp:150] Setting up conv4
I0124 13:39:49.041990 28535 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 13:39:49.041996 28535 net.cpp:165] Memory required for data: 438010880
I0124 13:39:49.042006 28535 layer_factory.hpp:76] Creating layer relu4
I0124 13:39:49.042019 28535 net.cpp:106] Creating Layer relu4
I0124 13:39:49.042026 28535 net.cpp:454] relu4 <- conv4
I0124 13:39:49.042032 28535 net.cpp:397] relu4 -> conv4 (in-place)
I0124 13:39:49.042747 28535 net.cpp:150] Setting up relu4
I0124 13:39:49.042759 28535 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 13:39:49.042763 28535 net.cpp:165] Memory required for data: 457278464
I0124 13:39:49.042768 28535 layer_factory.hpp:76] Creating layer conv5
I0124 13:39:49.042780 28535 net.cpp:106] Creating Layer conv5
I0124 13:39:49.042784 28535 net.cpp:454] conv5 <- conv4
I0124 13:39:49.042790 28535 net.cpp:411] conv5 -> conv5
I0124 13:39:49.063573 28535 net.cpp:150] Setting up conv5
I0124 13:39:49.063599 28535 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I0124 13:39:49.063604 28535 net.cpp:165] Memory required for data: 470123520
I0124 13:39:49.063618 28535 layer_factory.hpp:76] Creating layer relu5
I0124 13:39:49.063629 28535 net.cpp:106] Creating Layer relu5
I0124 13:39:49.063633 28535 net.cpp:454] relu5 <- conv5
I0124 13:39:49.063642 28535 net.cpp:397] relu5 -> conv5 (in-place)
I0124 13:39:49.064373 28535 net.cpp:150] Setting up relu5
I0124 13:39:49.064384 28535 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I0124 13:39:49.064388 28535 net.cpp:165] Memory required for data: 482968576
I0124 13:39:49.064393 28535 layer_factory.hpp:76] Creating layer pool5
I0124 13:39:49.064400 28535 net.cpp:106] Creating Layer pool5
I0124 13:39:49.064404 28535 net.cpp:454] pool5 <- conv5
I0124 13:39:49.064411 28535 net.cpp:411] pool5 -> pool5
I0124 13:39:49.065227 28535 net.cpp:150] Setting up pool5
I0124 13:39:49.065245 28535 net.cpp:157] Top shape: 256 256 3 3 (589824)
I0124 13:39:49.065249 28535 net.cpp:165] Memory required for data: 485327872
I0124 13:39:49.065253 28535 layer_factory.hpp:76] Creating layer fc6
I0124 13:39:49.065266 28535 net.cpp:106] Creating Layer fc6
I0124 13:39:49.065270 28535 net.cpp:454] fc6 <- pool5
I0124 13:39:49.065279 28535 net.cpp:411] fc6 -> fc6
I0124 13:39:49.233930 28535 net.cpp:150] Setting up fc6
I0124 13:39:49.233958 28535 net.cpp:157] Top shape: 256 2048 (524288)
I0124 13:39:49.233963 28535 net.cpp:165] Memory required for data: 487425024
I0124 13:39:49.233975 28535 layer_factory.hpp:76] Creating layer relu6
I0124 13:39:49.233986 28535 net.cpp:106] Creating Layer relu6
I0124 13:39:49.233992 28535 net.cpp:454] relu6 <- fc6
I0124 13:39:49.234002 28535 net.cpp:397] relu6 -> fc6 (in-place)
I0124 13:39:49.234896 28535 net.cpp:150] Setting up relu6
I0124 13:39:49.234910 28535 net.cpp:157] Top shape: 256 2048 (524288)
I0124 13:39:49.234915 28535 net.cpp:165] Memory required for data: 489522176
I0124 13:39:49.234920 28535 layer_factory.hpp:76] Creating layer drop6
I0124 13:39:49.234931 28535 net.cpp:106] Creating Layer drop6
I0124 13:39:49.234936 28535 net.cpp:454] drop6 <- fc6
I0124 13:39:49.234942 28535 net.cpp:397] drop6 -> fc6 (in-place)
I0124 13:39:49.234977 28535 net.cpp:150] Setting up drop6
I0124 13:39:49.234985 28535 net.cpp:157] Top shape: 256 2048 (524288)
I0124 13:39:49.234988 28535 net.cpp:165] Memory required for data: 491619328
I0124 13:39:49.234992 28535 layer_factory.hpp:76] Creating layer fc7
I0124 13:39:49.235002 28535 net.cpp:106] Creating Layer fc7
I0124 13:39:49.235005 28535 net.cpp:454] fc7 <- fc6
I0124 13:39:49.235013 28535 net.cpp:411] fc7 -> fc7
I0124 13:39:49.384465 28535 net.cpp:150] Setting up fc7
I0124 13:39:49.384490 28535 net.cpp:157] Top shape: 256 2048 (524288)
I0124 13:39:49.384495 28535 net.cpp:165] Memory required for data: 493716480
I0124 13:39:49.384529 28535 layer_factory.hpp:76] Creating layer relu7
I0124 13:39:49.384539 28535 net.cpp:106] Creating Layer relu7
I0124 13:39:49.384544 28535 net.cpp:454] relu7 <- fc7
I0124 13:39:49.384553 28535 net.cpp:397] relu7 -> fc7 (in-place)
I0124 13:39:49.385421 28535 net.cpp:150] Setting up relu7
I0124 13:39:49.385432 28535 net.cpp:157] Top shape: 256 2048 (524288)
I0124 13:39:49.385435 28535 net.cpp:165] Memory required for data: 495813632
I0124 13:39:49.385439 28535 layer_factory.hpp:76] Creating layer drop7
I0124 13:39:49.385454 28535 net.cpp:106] Creating Layer drop7
I0124 13:39:49.385459 28535 net.cpp:454] drop7 <- fc7
I0124 13:39:49.385465 28535 net.cpp:397] drop7 -> fc7 (in-place)
I0124 13:39:49.385493 28535 net.cpp:150] Setting up drop7
I0124 13:39:49.385499 28535 net.cpp:157] Top shape: 256 2048 (524288)
I0124 13:39:49.385504 28535 net.cpp:165] Memory required for data: 497910784
I0124 13:39:49.385506 28535 layer_factory.hpp:76] Creating layer fc8
I0124 13:39:49.385514 28535 net.cpp:106] Creating Layer fc8
I0124 13:39:49.385519 28535 net.cpp:454] fc8 <- fc7
I0124 13:39:49.385525 28535 net.cpp:411] fc8 -> fc8
I0124 13:39:49.458711 28535 net.cpp:150] Setting up fc8
I0124 13:39:49.458739 28535 net.cpp:157] Top shape: 256 1000 (256000)
I0124 13:39:49.458744 28535 net.cpp:165] Memory required for data: 498934784
I0124 13:39:49.458755 28535 layer_factory.hpp:76] Creating layer loss
I0124 13:39:49.458765 28535 net.cpp:106] Creating Layer loss
I0124 13:39:49.458770 28535 net.cpp:454] loss <- fc8
I0124 13:39:49.458775 28535 net.cpp:454] loss <- label
I0124 13:39:49.458784 28535 net.cpp:411] loss -> loss
I0124 13:39:49.458797 28535 layer_factory.hpp:76] Creating layer loss
I0124 13:39:49.460530 28535 net.cpp:150] Setting up loss
I0124 13:39:49.460544 28535 net.cpp:157] Top shape: (1)
I0124 13:39:49.460548 28535 net.cpp:160]     with loss weight 1
I0124 13:39:49.460566 28535 net.cpp:165] Memory required for data: 498934788
I0124 13:39:49.460571 28535 net.cpp:226] loss needs backward computation.
I0124 13:39:49.460575 28535 net.cpp:226] fc8 needs backward computation.
I0124 13:39:49.460578 28535 net.cpp:226] drop7 needs backward computation.
I0124 13:39:49.460582 28535 net.cpp:226] relu7 needs backward computation.
I0124 13:39:49.460585 28535 net.cpp:226] fc7 needs backward computation.
I0124 13:39:49.460589 28535 net.cpp:226] drop6 needs backward computation.
I0124 13:39:49.460592 28535 net.cpp:226] relu6 needs backward computation.
I0124 13:39:49.460595 28535 net.cpp:226] fc6 needs backward computation.
I0124 13:39:49.460599 28535 net.cpp:226] pool5 needs backward computation.
I0124 13:39:49.460603 28535 net.cpp:226] relu5 needs backward computation.
I0124 13:39:49.460608 28535 net.cpp:226] conv5 needs backward computation.
I0124 13:39:49.460610 28535 net.cpp:226] relu4 needs backward computation.
I0124 13:39:49.460614 28535 net.cpp:226] conv4 needs backward computation.
I0124 13:39:49.460618 28535 net.cpp:226] relu3 needs backward computation.
I0124 13:39:49.460621 28535 net.cpp:226] conv3 needs backward computation.
I0124 13:39:49.460625 28535 net.cpp:226] pool2 needs backward computation.
I0124 13:39:49.460628 28535 net.cpp:226] relu2 needs backward computation.
I0124 13:39:49.460631 28535 net.cpp:226] conv2 needs backward computation.
I0124 13:39:49.460635 28535 net.cpp:226] pool1 needs backward computation.
I0124 13:39:49.460638 28535 net.cpp:226] relu1 needs backward computation.
I0124 13:39:49.460643 28535 net.cpp:226] conv1 needs backward computation.
I0124 13:39:49.460646 28535 net.cpp:228] data does not need backward computation.
I0124 13:39:49.460650 28535 net.cpp:270] This network produces output loss
I0124 13:39:49.460666 28535 net.cpp:283] Network initialization done.
I0124 13:39:49.460769 28535 solver.cpp:181] Creating test net (#0) specified by net_param
I0124 13:39:49.460813 28535 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0124 13:39:49.461024 28535 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.04
    mirror: false
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 123
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/old-ufo/datasets/imagenet/ilsvrc12_val_lmdb"
    batch_size: 250
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 13:39:49.461169 28535 layer_factory.hpp:76] Creating layer data
I0124 13:39:49.461385 28535 net.cpp:106] Creating Layer data
I0124 13:39:49.461392 28535 net.cpp:411] data -> data
I0124 13:39:49.461402 28535 net.cpp:411] data -> label
I0124 13:39:49.462229 28548 db_lmdb.cpp:38] Opened lmdb /home/old-ufo/datasets/imagenet/ilsvrc12_val_lmdb
I0124 13:39:49.464907 28535 data_layer.cpp:41] output data size: 250,3,128,128
I0124 13:39:49.536384 28535 net.cpp:150] Setting up data
I0124 13:39:49.536422 28535 net.cpp:157] Top shape: 250 3 128 128 (12288000)
I0124 13:39:49.536429 28535 net.cpp:157] Top shape: 250 (250)
I0124 13:39:49.536433 28535 net.cpp:165] Memory required for data: 49153000
I0124 13:39:49.536442 28535 layer_factory.hpp:76] Creating layer label_data_1_split
I0124 13:39:49.536455 28535 net.cpp:106] Creating Layer label_data_1_split
I0124 13:39:49.536463 28535 net.cpp:454] label_data_1_split <- label
I0124 13:39:49.536471 28535 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0124 13:39:49.536485 28535 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0124 13:39:49.536617 28535 net.cpp:150] Setting up label_data_1_split
I0124 13:39:49.536628 28535 net.cpp:157] Top shape: 250 (250)
I0124 13:39:49.536633 28535 net.cpp:157] Top shape: 250 (250)
I0124 13:39:49.536636 28535 net.cpp:165] Memory required for data: 49155000
I0124 13:39:49.536640 28535 layer_factory.hpp:76] Creating layer conv1
I0124 13:39:49.536653 28535 net.cpp:106] Creating Layer conv1
I0124 13:39:49.536658 28535 net.cpp:454] conv1 <- data
I0124 13:39:49.536664 28535 net.cpp:411] conv1 -> conv1
I0124 13:39:49.544785 28535 net.cpp:150] Setting up conv1
I0124 13:39:49.544818 28535 net.cpp:157] Top shape: 250 96 30 30 (21600000)
I0124 13:39:49.544826 28535 net.cpp:165] Memory required for data: 135555000
I0124 13:39:49.544845 28535 layer_factory.hpp:76] Creating layer relu1
I0124 13:39:49.544858 28535 net.cpp:106] Creating Layer relu1
I0124 13:39:49.544863 28535 net.cpp:454] relu1 <- conv1
I0124 13:39:49.544875 28535 net.cpp:411] relu1 -> relu1
I0124 13:39:49.545748 28535 net.cpp:150] Setting up relu1
I0124 13:39:49.545765 28535 net.cpp:157] Top shape: 250 96 30 30 (21600000)
I0124 13:39:49.545769 28535 net.cpp:165] Memory required for data: 221955000
I0124 13:39:49.545774 28535 layer_factory.hpp:76] Creating layer pool1
I0124 13:39:49.545788 28535 net.cpp:106] Creating Layer pool1
I0124 13:39:49.545794 28535 net.cpp:454] pool1 <- relu1
I0124 13:39:49.545800 28535 net.cpp:411] pool1 -> pool1
I0124 13:39:49.546635 28535 net.cpp:150] Setting up pool1
I0124 13:39:49.546649 28535 net.cpp:157] Top shape: 250 96 15 15 (5400000)
I0124 13:39:49.546653 28535 net.cpp:165] Memory required for data: 243555000
I0124 13:39:49.546658 28535 layer_factory.hpp:76] Creating layer conv2
I0124 13:39:49.546672 28535 net.cpp:106] Creating Layer conv2
I0124 13:39:49.546679 28535 net.cpp:454] conv2 <- pool1
I0124 13:39:49.546686 28535 net.cpp:411] conv2 -> conv2
I0124 13:39:49.563431 28535 net.cpp:150] Setting up conv2
I0124 13:39:49.563459 28535 net.cpp:157] Top shape: 250 256 15 15 (14400000)
I0124 13:39:49.563465 28535 net.cpp:165] Memory required for data: 301155000
I0124 13:39:49.563479 28535 layer_factory.hpp:76] Creating layer relu2
I0124 13:39:49.563493 28535 net.cpp:106] Creating Layer relu2
I0124 13:39:49.563524 28535 net.cpp:454] relu2 <- conv2
I0124 13:39:49.563534 28535 net.cpp:397] relu2 -> conv2 (in-place)
I0124 13:39:49.564340 28535 net.cpp:150] Setting up relu2
I0124 13:39:49.564353 28535 net.cpp:157] Top shape: 250 256 15 15 (14400000)
I0124 13:39:49.564357 28535 net.cpp:165] Memory required for data: 358755000
I0124 13:39:49.564362 28535 layer_factory.hpp:76] Creating layer pool2
I0124 13:39:49.564373 28535 net.cpp:106] Creating Layer pool2
I0124 13:39:49.564378 28535 net.cpp:454] pool2 <- conv2
I0124 13:39:49.564385 28535 net.cpp:411] pool2 -> pool2
I0124 13:39:49.565528 28535 net.cpp:150] Setting up pool2
I0124 13:39:49.565542 28535 net.cpp:157] Top shape: 250 256 7 7 (3136000)
I0124 13:39:49.565546 28535 net.cpp:165] Memory required for data: 371299000
I0124 13:39:49.565551 28535 layer_factory.hpp:76] Creating layer conv3
I0124 13:39:49.565563 28535 net.cpp:106] Creating Layer conv3
I0124 13:39:49.565568 28535 net.cpp:454] conv3 <- pool2
I0124 13:39:49.565575 28535 net.cpp:411] conv3 -> conv3
I0124 13:39:49.601009 28535 net.cpp:150] Setting up conv3
I0124 13:39:49.601040 28535 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 13:39:49.601047 28535 net.cpp:165] Memory required for data: 390115000
I0124 13:39:49.601066 28535 layer_factory.hpp:76] Creating layer relu3
I0124 13:39:49.601078 28535 net.cpp:106] Creating Layer relu3
I0124 13:39:49.601084 28535 net.cpp:454] relu3 <- conv3
I0124 13:39:49.601091 28535 net.cpp:397] relu3 -> conv3 (in-place)
I0124 13:39:49.601989 28535 net.cpp:150] Setting up relu3
I0124 13:39:49.602006 28535 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 13:39:49.602012 28535 net.cpp:165] Memory required for data: 408931000
I0124 13:39:49.602017 28535 layer_factory.hpp:76] Creating layer conv4
I0124 13:39:49.602030 28535 net.cpp:106] Creating Layer conv4
I0124 13:39:49.602035 28535 net.cpp:454] conv4 <- conv3
I0124 13:39:49.602044 28535 net.cpp:411] conv4 -> conv4
I0124 13:39:49.632231 28535 net.cpp:150] Setting up conv4
I0124 13:39:49.632261 28535 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 13:39:49.632266 28535 net.cpp:165] Memory required for data: 427747000
I0124 13:39:49.632279 28535 layer_factory.hpp:76] Creating layer relu4
I0124 13:39:49.632290 28535 net.cpp:106] Creating Layer relu4
I0124 13:39:49.632295 28535 net.cpp:454] relu4 <- conv4
I0124 13:39:49.632304 28535 net.cpp:397] relu4 -> conv4 (in-place)
I0124 13:39:49.633110 28535 net.cpp:150] Setting up relu4
I0124 13:39:49.633124 28535 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 13:39:49.633131 28535 net.cpp:165] Memory required for data: 446563000
I0124 13:39:49.633136 28535 layer_factory.hpp:76] Creating layer conv5
I0124 13:39:49.633150 28535 net.cpp:106] Creating Layer conv5
I0124 13:39:49.633155 28535 net.cpp:454] conv5 <- conv4
I0124 13:39:49.633164 28535 net.cpp:411] conv5 -> conv5
I0124 13:39:49.655513 28535 net.cpp:150] Setting up conv5
I0124 13:39:49.655657 28535 net.cpp:157] Top shape: 250 256 7 7 (3136000)
I0124 13:39:49.655717 28535 net.cpp:165] Memory required for data: 459107000
I0124 13:39:49.655786 28535 layer_factory.hpp:76] Creating layer relu5
I0124 13:39:49.655854 28535 net.cpp:106] Creating Layer relu5
I0124 13:39:49.655915 28535 net.cpp:454] relu5 <- conv5
I0124 13:39:49.656008 28535 net.cpp:397] relu5 -> conv5 (in-place)
I0124 13:39:49.656855 28535 net.cpp:150] Setting up relu5
I0124 13:39:49.657881 28535 net.cpp:157] Top shape: 250 256 7 7 (3136000)
I0124 13:39:49.657904 28535 net.cpp:165] Memory required for data: 471651000
I0124 13:39:49.657922 28535 layer_factory.hpp:76] Creating layer pool5
I0124 13:39:49.657944 28535 net.cpp:106] Creating Layer pool5
I0124 13:39:49.657961 28535 net.cpp:454] pool5 <- conv5
I0124 13:39:49.657981 28535 net.cpp:411] pool5 -> pool5
I0124 13:39:49.659199 28535 net.cpp:150] Setting up pool5
I0124 13:39:49.659240 28535 net.cpp:157] Top shape: 250 256 3 3 (576000)
I0124 13:39:49.659255 28535 net.cpp:165] Memory required for data: 473955000
I0124 13:39:49.659281 28535 layer_factory.hpp:76] Creating layer fc6
I0124 13:39:49.659313 28535 net.cpp:106] Creating Layer fc6
I0124 13:39:49.659328 28535 net.cpp:454] fc6 <- pool5
I0124 13:39:49.659346 28535 net.cpp:411] fc6 -> fc6
I0124 13:39:49.827024 28535 net.cpp:150] Setting up fc6
I0124 13:39:49.827101 28535 net.cpp:157] Top shape: 250 2048 (512000)
I0124 13:39:49.827118 28535 net.cpp:165] Memory required for data: 476003000
I0124 13:39:49.827142 28535 layer_factory.hpp:76] Creating layer relu6
I0124 13:39:49.827167 28535 net.cpp:106] Creating Layer relu6
I0124 13:39:49.827184 28535 net.cpp:454] relu6 <- fc6
I0124 13:39:49.827203 28535 net.cpp:397] relu6 -> fc6 (in-place)
I0124 13:39:49.828248 28535 net.cpp:150] Setting up relu6
I0124 13:39:49.828279 28535 net.cpp:157] Top shape: 250 2048 (512000)
I0124 13:39:49.828295 28535 net.cpp:165] Memory required for data: 478051000
I0124 13:39:49.828310 28535 layer_factory.hpp:76] Creating layer drop6
I0124 13:39:49.828327 28535 net.cpp:106] Creating Layer drop6
I0124 13:39:49.828343 28535 net.cpp:454] drop6 <- fc6
I0124 13:39:49.828359 28535 net.cpp:397] drop6 -> fc6 (in-place)
I0124 13:39:49.828414 28535 net.cpp:150] Setting up drop6
I0124 13:39:49.828434 28535 net.cpp:157] Top shape: 250 2048 (512000)
I0124 13:39:49.828449 28535 net.cpp:165] Memory required for data: 480099000
I0124 13:39:49.828462 28535 layer_factory.hpp:76] Creating layer fc7
I0124 13:39:49.828485 28535 net.cpp:106] Creating Layer fc7
I0124 13:39:49.828500 28535 net.cpp:454] fc7 <- fc6
I0124 13:39:49.828516 28535 net.cpp:411] fc7 -> fc7
I0124 13:39:49.978440 28535 net.cpp:150] Setting up fc7
I0124 13:39:49.978505 28535 net.cpp:157] Top shape: 250 2048 (512000)
I0124 13:39:49.978528 28535 net.cpp:165] Memory required for data: 482147000
I0124 13:39:49.978554 28535 layer_factory.hpp:76] Creating layer relu7
I0124 13:39:49.978579 28535 net.cpp:106] Creating Layer relu7
I0124 13:39:49.978595 28535 net.cpp:454] relu7 <- fc7
I0124 13:39:49.978613 28535 net.cpp:397] relu7 -> fc7 (in-place)
I0124 13:39:49.979740 28535 net.cpp:150] Setting up relu7
I0124 13:39:49.979784 28535 net.cpp:157] Top shape: 250 2048 (512000)
I0124 13:39:49.979804 28535 net.cpp:165] Memory required for data: 484195000
I0124 13:39:49.979821 28535 layer_factory.hpp:76] Creating layer drop7
I0124 13:39:49.979841 28535 net.cpp:106] Creating Layer drop7
I0124 13:39:49.979857 28535 net.cpp:454] drop7 <- fc7
I0124 13:39:49.979876 28535 net.cpp:397] drop7 -> fc7 (in-place)
I0124 13:39:49.979944 28535 net.cpp:150] Setting up drop7
I0124 13:39:49.979966 28535 net.cpp:157] Top shape: 250 2048 (512000)
I0124 13:39:49.979980 28535 net.cpp:165] Memory required for data: 486243000
I0124 13:39:49.979995 28535 layer_factory.hpp:76] Creating layer fc8
I0124 13:39:49.980015 28535 net.cpp:106] Creating Layer fc8
I0124 13:39:49.980031 28535 net.cpp:454] fc8 <- fc7
I0124 13:39:49.980048 28535 net.cpp:411] fc8 -> fc8
I0124 13:39:50.053256 28535 net.cpp:150] Setting up fc8
I0124 13:39:50.053324 28535 net.cpp:157] Top shape: 250 1000 (250000)
I0124 13:39:50.053340 28535 net.cpp:165] Memory required for data: 487243000
I0124 13:39:50.053364 28535 layer_factory.hpp:76] Creating layer fc8_fc8_0_split
I0124 13:39:50.053386 28535 net.cpp:106] Creating Layer fc8_fc8_0_split
I0124 13:39:50.053405 28535 net.cpp:454] fc8_fc8_0_split <- fc8
I0124 13:39:50.053422 28535 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0124 13:39:50.053449 28535 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0124 13:39:50.053519 28535 net.cpp:150] Setting up fc8_fc8_0_split
I0124 13:39:50.053540 28535 net.cpp:157] Top shape: 250 1000 (250000)
I0124 13:39:50.053555 28535 net.cpp:157] Top shape: 250 1000 (250000)
I0124 13:39:50.053568 28535 net.cpp:165] Memory required for data: 489243000
I0124 13:39:50.053582 28535 layer_factory.hpp:76] Creating layer accuracy
I0124 13:39:50.053609 28535 net.cpp:106] Creating Layer accuracy
I0124 13:39:50.053625 28535 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0124 13:39:50.053640 28535 net.cpp:454] accuracy <- label_data_1_split_0
I0124 13:39:50.053658 28535 net.cpp:411] accuracy -> accuracy
I0124 13:39:50.053686 28535 net.cpp:150] Setting up accuracy
I0124 13:39:50.053719 28535 net.cpp:157] Top shape: (1)
I0124 13:39:50.053735 28535 net.cpp:165] Memory required for data: 489243004
I0124 13:39:50.053748 28535 layer_factory.hpp:76] Creating layer loss
I0124 13:39:50.053766 28535 net.cpp:106] Creating Layer loss
I0124 13:39:50.053781 28535 net.cpp:454] loss <- fc8_fc8_0_split_1
I0124 13:39:50.053796 28535 net.cpp:454] loss <- label_data_1_split_1
I0124 13:39:50.053812 28535 net.cpp:411] loss -> loss
I0124 13:39:50.053830 28535 layer_factory.hpp:76] Creating layer loss
I0124 13:39:50.055565 28535 net.cpp:150] Setting up loss
I0124 13:39:50.055613 28535 net.cpp:157] Top shape: (1)
I0124 13:39:50.055627 28535 net.cpp:160]     with loss weight 1
I0124 13:39:50.055650 28535 net.cpp:165] Memory required for data: 489243008
I0124 13:39:50.055666 28535 net.cpp:226] loss needs backward computation.
I0124 13:39:50.055682 28535 net.cpp:228] accuracy does not need backward computation.
I0124 13:39:50.055697 28535 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0124 13:39:50.055711 28535 net.cpp:226] fc8 needs backward computation.
I0124 13:39:50.055727 28535 net.cpp:226] drop7 needs backward computation.
I0124 13:39:50.055740 28535 net.cpp:226] relu7 needs backward computation.
I0124 13:39:50.055754 28535 net.cpp:226] fc7 needs backward computation.
I0124 13:39:50.055769 28535 net.cpp:226] drop6 needs backward computation.
I0124 13:39:50.055783 28535 net.cpp:226] relu6 needs backward computation.
I0124 13:39:50.055796 28535 net.cpp:226] fc6 needs backward computation.
I0124 13:39:50.055812 28535 net.cpp:226] pool5 needs backward computation.
I0124 13:39:50.055826 28535 net.cpp:226] relu5 needs backward computation.
I0124 13:39:50.055840 28535 net.cpp:226] conv5 needs backward computation.
I0124 13:39:50.055855 28535 net.cpp:226] relu4 needs backward computation.
I0124 13:39:50.055869 28535 net.cpp:226] conv4 needs backward computation.
I0124 13:39:50.055883 28535 net.cpp:226] relu3 needs backward computation.
I0124 13:39:50.055897 28535 net.cpp:226] conv3 needs backward computation.
I0124 13:39:50.055914 28535 net.cpp:226] pool2 needs backward computation.
I0124 13:39:50.055929 28535 net.cpp:226] relu2 needs backward computation.
I0124 13:39:50.055945 28535 net.cpp:226] conv2 needs backward computation.
I0124 13:39:50.055959 28535 net.cpp:226] pool1 needs backward computation.
I0124 13:39:50.055974 28535 net.cpp:226] relu1 needs backward computation.
I0124 13:39:50.055992 28535 net.cpp:226] conv1 needs backward computation.
I0124 13:39:50.056007 28535 net.cpp:228] label_data_1_split does not need backward computation.
I0124 13:39:50.056021 28535 net.cpp:228] data does not need backward computation.
I0124 13:39:50.056035 28535 net.cpp:270] This network produces output accuracy
I0124 13:39:50.056049 28535 net.cpp:270] This network produces output loss
I0124 13:39:50.056077 28535 net.cpp:283] Network initialization done.
I0124 13:39:50.056182 28535 solver.cpp:60] Solver scaffolding done.
I0124 13:39:50.057301 28535 caffe.cpp:128] Finetuning from ./caffenet_lsuv_adam_lr001_095_0999.prototxt.caffemodel
I0124 13:39:50.278898 28535 caffe.cpp:212] Starting Optimization
I0124 13:39:50.278966 28535 solver.cpp:288] Solving CaffeNet
I0124 13:39:50.278983 28535 solver.cpp:289] Learning Rate Policy: fixed
I0124 13:39:50.335625 28535 solver.cpp:237] Iteration 0, loss = 7.4398
I0124 13:39:50.335711 28535 solver.cpp:253]     Train net output #0: loss = 7.4398 (* 1 = 7.4398 loss)
I0124 13:39:50.335738 28535 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0124 13:39:50.451077 28535 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 13:39:57.614704 28535 solver.cpp:237] Iteration 20, loss = 19.1496
I0124 13:39:57.614744 28535 solver.cpp:253]     Train net output #0: loss = 6.91749 (* 1 = 6.91749 loss)
I0124 13:39:57.614753 28535 sgd_solver.cpp:106] Iteration 20, lr = 0.01
I0124 13:40:05.169591 28535 solver.cpp:237] Iteration 40, loss = 7.40521
I0124 13:40:05.169765 28535 solver.cpp:253]     Train net output #0: loss = 7.32454 (* 1 = 7.32454 loss)
I0124 13:40:05.169858 28535 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I0124 13:40:12.550874 28535 solver.cpp:237] Iteration 60, loss = 7.0224
I0124 13:40:12.550909 28535 solver.cpp:253]     Train net output #0: loss = 8.84617 (* 1 = 8.84617 loss)
I0124 13:40:12.550915 28535 sgd_solver.cpp:106] Iteration 60, lr = 0.01
I0124 13:40:20.027104 28535 solver.cpp:237] Iteration 80, loss = 7.41426
I0124 13:40:20.027217 28535 solver.cpp:253]     Train net output #0: loss = 6.926 (* 1 = 6.926 loss)
I0124 13:40:20.027228 28535 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I0124 13:40:27.690551 28535 solver.cpp:237] Iteration 100, loss = 7.00239
I0124 13:40:27.690641 28535 solver.cpp:253]     Train net output #0: loss = 6.91043 (* 1 = 6.91043 loss)
I0124 13:40:27.690668 28535 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I0124 13:40:35.252516 28535 solver.cpp:237] Iteration 120, loss = 11.5137
I0124 13:40:35.252550 28535 solver.cpp:253]     Train net output #0: loss = 6.91234 (* 1 = 6.91234 loss)
I0124 13:40:35.252558 28535 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I0124 13:40:42.949798 28535 solver.cpp:237] Iteration 140, loss = 9.4617
I0124 13:40:42.949836 28535 solver.cpp:253]     Train net output #0: loss = 7.06315 (* 1 = 7.06315 loss)
I0124 13:40:42.949843 28535 sgd_solver.cpp:106] Iteration 140, lr = 0.01
I0124 13:40:50.723299 28535 solver.cpp:237] Iteration 160, loss = 9.2163
I0124 13:40:50.723377 28535 solver.cpp:253]     Train net output #0: loss = 7.03393 (* 1 = 7.03393 loss)
I0124 13:40:50.723387 28535 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I0124 13:40:58.328801 28535 solver.cpp:237] Iteration 180, loss = 8.73233
I0124 13:40:58.328836 28535 solver.cpp:253]     Train net output #0: loss = 29.9524 (* 1 = 29.9524 loss)
I0124 13:40:58.328846 28535 sgd_solver.cpp:106] Iteration 180, lr = 0.01
I0124 13:41:05.959530 28535 solver.cpp:237] Iteration 200, loss = 8.17571
I0124 13:41:05.959735 28535 solver.cpp:253]     Train net output #0: loss = 8.45711 (* 1 = 8.45711 loss)
I0124 13:41:05.959821 28535 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I0124 13:41:13.983849 28535 solver.cpp:237] Iteration 220, loss = 8.70309
I0124 13:41:13.983889 28535 solver.cpp:253]     Train net output #0: loss = 8.20083 (* 1 = 8.20083 loss)
I0124 13:41:13.983899 28535 sgd_solver.cpp:106] Iteration 220, lr = 0.01
I0124 13:41:21.690208 28535 solver.cpp:237] Iteration 240, loss = 7.30908
I0124 13:41:21.690276 28535 solver.cpp:253]     Train net output #0: loss = 6.91154 (* 1 = 6.91154 loss)
I0124 13:41:21.690346 28535 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I0124 13:41:29.411790 28535 solver.cpp:237] Iteration 260, loss = 14.9537
I0124 13:41:29.411995 28535 solver.cpp:253]     Train net output #0: loss = 6.90173 (* 1 = 6.90173 loss)
I0124 13:41:29.412073 28535 sgd_solver.cpp:106] Iteration 260, lr = 0.01
I0124 13:41:37.208168 28535 solver.cpp:237] Iteration 280, loss = 6.92099
I0124 13:41:37.208272 28535 solver.cpp:253]     Train net output #0: loss = 6.91648 (* 1 = 6.91648 loss)
I0124 13:41:37.208295 28535 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I0124 13:41:45.243674 28535 solver.cpp:237] Iteration 300, loss = 7.09938
I0124 13:41:45.243712 28535 solver.cpp:253]     Train net output #0: loss = 6.91743 (* 1 = 6.91743 loss)
I0124 13:41:45.243721 28535 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I0124 13:41:52.891110 28535 solver.cpp:237] Iteration 320, loss = 6.91654
I0124 13:41:52.891193 28535 solver.cpp:253]     Train net output #0: loss = 6.91726 (* 1 = 6.91726 loss)
I0124 13:41:52.891202 28535 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I0124 13:42:00.694633 28535 solver.cpp:237] Iteration 340, loss = 10.3577
I0124 13:42:00.694672 28535 solver.cpp:253]     Train net output #0: loss = 6.92895 (* 1 = 6.92895 loss)
I0124 13:42:00.694680 28535 sgd_solver.cpp:106] Iteration 340, lr = 0.01
I0124 13:42:08.430547 28535 solver.cpp:237] Iteration 360, loss = 6.91842
I0124 13:42:08.430587 28535 solver.cpp:253]     Train net output #0: loss = 6.92922 (* 1 = 6.92922 loss)
I0124 13:42:08.430594 28535 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I0124 13:42:16.081574 28535 solver.cpp:237] Iteration 380, loss = 6.91634
I0124 13:42:16.081614 28535 solver.cpp:253]     Train net output #0: loss = 6.91831 (* 1 = 6.91831 loss)
I0124 13:42:16.081621 28535 sgd_solver.cpp:106] Iteration 380, lr = 0.01
I0124 13:42:23.666101 28535 solver.cpp:237] Iteration 400, loss = 6.91976
I0124 13:42:23.666203 28535 solver.cpp:253]     Train net output #0: loss = 6.92029 (* 1 = 6.92029 loss)
I0124 13:42:23.666213 28535 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I0124 13:42:31.315587 28535 solver.cpp:237] Iteration 420, loss = 10.4094
I0124 13:42:31.315626 28535 solver.cpp:253]     Train net output #0: loss = 6.91873 (* 1 = 6.91873 loss)
I0124 13:42:31.315634 28535 sgd_solver.cpp:106] Iteration 420, lr = 0.01
I0124 13:42:39.032850 28535 solver.cpp:237] Iteration 440, loss = 6.92412
I0124 13:42:39.032943 28535 solver.cpp:253]     Train net output #0: loss = 6.93622 (* 1 = 6.93622 loss)
I0124 13:42:39.032965 28535 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I0124 13:42:46.789463 28535 solver.cpp:237] Iteration 460, loss = 6.91748
I0124 13:42:46.789499 28535 solver.cpp:253]     Train net output #0: loss = 6.92197 (* 1 = 6.92197 loss)
I0124 13:42:46.789506 28535 sgd_solver.cpp:106] Iteration 460, lr = 0.01
I0124 13:42:54.428606 28535 solver.cpp:237] Iteration 480, loss = 6.91567
I0124 13:42:54.428689 28535 solver.cpp:253]     Train net output #0: loss = 6.90545 (* 1 = 6.90545 loss)
I0124 13:42:54.428697 28535 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I0124 13:43:02.086230 28535 solver.cpp:237] Iteration 500, loss = 6.9178
I0124 13:43:02.086268 28535 solver.cpp:253]     Train net output #0: loss = 6.92236 (* 1 = 6.92236 loss)
I0124 13:43:02.086277 28535 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I0124 13:43:09.732514 28535 solver.cpp:237] Iteration 520, loss = 6.92017
I0124 13:43:09.732554 28535 solver.cpp:253]     Train net output #0: loss = 6.91583 (* 1 = 6.91583 loss)
I0124 13:43:09.732561 28535 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I0124 13:43:17.375764 28535 solver.cpp:237] Iteration 540, loss = 6.91914
I0124 13:43:17.375802 28535 solver.cpp:253]     Train net output #0: loss = 6.92531 (* 1 = 6.92531 loss)
I0124 13:43:17.375811 28535 sgd_solver.cpp:106] Iteration 540, lr = 0.01
I0124 13:43:25.097298 28535 solver.cpp:237] Iteration 560, loss = 6.91328
I0124 13:43:25.097386 28535 solver.cpp:253]     Train net output #0: loss = 6.90812 (* 1 = 6.90812 loss)
I0124 13:43:25.097395 28535 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I0124 13:43:32.770298 28535 solver.cpp:237] Iteration 580, loss = 6.9157
I0124 13:43:32.770334 28535 solver.cpp:253]     Train net output #0: loss = 6.90149 (* 1 = 6.90149 loss)
I0124 13:43:32.770342 28535 sgd_solver.cpp:106] Iteration 580, lr = 0.01
I0124 13:43:40.495970 28535 solver.cpp:237] Iteration 600, loss = 6.91572
I0124 13:43:40.496009 28535 solver.cpp:253]     Train net output #0: loss = 6.90691 (* 1 = 6.90691 loss)
I0124 13:43:40.496016 28535 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I0124 13:43:48.339517 28535 solver.cpp:237] Iteration 620, loss = 6.91726
I0124 13:43:48.339555 28535 solver.cpp:253]     Train net output #0: loss = 6.9322 (* 1 = 6.9322 loss)
I0124 13:43:48.339563 28535 sgd_solver.cpp:106] Iteration 620, lr = 0.01
I0124 13:43:55.988184 28535 solver.cpp:237] Iteration 640, loss = 6.91719
I0124 13:43:55.988261 28535 solver.cpp:253]     Train net output #0: loss = 6.93001 (* 1 = 6.93001 loss)
I0124 13:43:55.988270 28535 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I0124 13:44:03.447077 28535 solver.cpp:237] Iteration 660, loss = 6.9185
I0124 13:44:03.447111 28535 solver.cpp:253]     Train net output #0: loss = 6.91335 (* 1 = 6.91335 loss)
I0124 13:44:03.447119 28535 sgd_solver.cpp:106] Iteration 660, lr = 0.01
I0124 13:44:10.912233 28535 solver.cpp:237] Iteration 680, loss = 6.91633
I0124 13:44:10.912266 28535 solver.cpp:253]     Train net output #0: loss = 6.91543 (* 1 = 6.91543 loss)
I0124 13:44:10.912274 28535 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I0124 13:44:18.347776 28535 solver.cpp:237] Iteration 700, loss = 6.9138
I0124 13:44:18.347810 28535 solver.cpp:253]     Train net output #0: loss = 6.91474 (* 1 = 6.91474 loss)
I0124 13:44:18.347817 28535 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I0124 13:44:25.796138 28535 solver.cpp:237] Iteration 720, loss = 6.91878
I0124 13:44:25.796172 28535 solver.cpp:253]     Train net output #0: loss = 6.9108 (* 1 = 6.9108 loss)
I0124 13:44:25.796180 28535 sgd_solver.cpp:106] Iteration 720, lr = 0.01
I0124 13:44:33.269518 28535 solver.cpp:237] Iteration 740, loss = 6.91926
I0124 13:44:33.269598 28535 solver.cpp:253]     Train net output #0: loss = 6.92725 (* 1 = 6.92725 loss)
I0124 13:44:33.269608 28535 sgd_solver.cpp:106] Iteration 740, lr = 0.01
I0124 13:44:40.666946 28535 solver.cpp:237] Iteration 760, loss = 6.91612
I0124 13:44:40.667140 28535 solver.cpp:253]     Train net output #0: loss = 6.91139 (* 1 = 6.91139 loss)
I0124 13:44:40.667213 28535 sgd_solver.cpp:106] Iteration 760, lr = 0.01
I0124 13:44:48.053338 28535 solver.cpp:237] Iteration 780, loss = 6.91759
I0124 13:44:48.053421 28535 solver.cpp:253]     Train net output #0: loss = 6.92416 (* 1 = 6.92416 loss)
I0124 13:44:48.053442 28535 sgd_solver.cpp:106] Iteration 780, lr = 0.01
I0124 13:44:55.641168 28535 solver.cpp:237] Iteration 800, loss = 6.91405
I0124 13:44:55.641206 28535 solver.cpp:253]     Train net output #0: loss = 6.9146 (* 1 = 6.9146 loss)
I0124 13:44:55.641214 28535 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I0124 13:45:03.299469 28535 solver.cpp:237] Iteration 820, loss = 7.71405
I0124 13:45:03.299543 28535 solver.cpp:253]     Train net output #0: loss = 6.92904 (* 1 = 6.92904 loss)
I0124 13:45:03.299614 28535 sgd_solver.cpp:106] Iteration 820, lr = 0.01
I0124 13:45:10.797549 28535 solver.cpp:237] Iteration 840, loss = 6.93171
I0124 13:45:10.797582 28535 solver.cpp:253]     Train net output #0: loss = 6.91736 (* 1 = 6.91736 loss)
I0124 13:45:10.797590 28535 sgd_solver.cpp:106] Iteration 840, lr = 0.01
I0124 13:45:18.324796 28535 solver.cpp:237] Iteration 860, loss = 6.91771
I0124 13:45:18.324832 28535 solver.cpp:253]     Train net output #0: loss = 6.93725 (* 1 = 6.93725 loss)
I0124 13:45:18.324839 28535 sgd_solver.cpp:106] Iteration 860, lr = 0.01
I0124 13:45:25.836269 28535 solver.cpp:237] Iteration 880, loss = 6.913
I0124 13:45:25.836352 28535 solver.cpp:253]     Train net output #0: loss = 6.92061 (* 1 = 6.92061 loss)
I0124 13:45:25.836375 28535 sgd_solver.cpp:106] Iteration 880, lr = 0.01
I0124 13:45:33.286594 28535 solver.cpp:237] Iteration 900, loss = 6.91341
I0124 13:45:33.286628 28535 solver.cpp:253]     Train net output #0: loss = 6.9218 (* 1 = 6.9218 loss)
I0124 13:45:33.286638 28535 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I0124 13:45:40.691679 28535 solver.cpp:237] Iteration 920, loss = 6.91559
I0124 13:45:40.691756 28535 solver.cpp:253]     Train net output #0: loss = 6.903 (* 1 = 6.903 loss)
I0124 13:45:40.691767 28535 sgd_solver.cpp:106] Iteration 920, lr = 0.01
I0124 13:45:48.200330 28535 solver.cpp:237] Iteration 940, loss = 6.91552
I0124 13:45:48.200366 28535 solver.cpp:253]     Train net output #0: loss = 6.91951 (* 1 = 6.91951 loss)
I0124 13:45:48.200373 28535 sgd_solver.cpp:106] Iteration 940, lr = 0.01
I0124 13:45:55.774862 28535 solver.cpp:237] Iteration 960, loss = 6.91513
I0124 13:45:55.774896 28535 solver.cpp:253]     Train net output #0: loss = 6.92573 (* 1 = 6.92573 loss)
I0124 13:45:55.774904 28535 sgd_solver.cpp:106] Iteration 960, lr = 0.01
I0124 13:46:03.445859 28535 solver.cpp:237] Iteration 980, loss = 6.91742
I0124 13:46:03.445894 28535 solver.cpp:253]     Train net output #0: loss = 6.89672 (* 1 = 6.89672 loss)
I0124 13:46:03.445902 28535 sgd_solver.cpp:106] Iteration 980, lr = 0.01
I0124 13:46:10.968523 28535 solver.cpp:237] Iteration 1000, loss = 6.91721
I0124 13:46:10.968591 28535 solver.cpp:253]     Train net output #0: loss = 6.90729 (* 1 = 6.90729 loss)
I0124 13:46:10.968600 28535 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I0124 13:46:11.339779 28535 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 13:46:18.492863 28535 solver.cpp:237] Iteration 1020, loss = 6.91655
I0124 13:46:18.492969 28535 solver.cpp:253]     Train net output #0: loss = 6.91693 (* 1 = 6.91693 loss)
I0124 13:46:18.493002 28535 sgd_solver.cpp:106] Iteration 1020, lr = 0.01
I0124 13:46:26.030921 28535 solver.cpp:237] Iteration 1040, loss = 6.91268
I0124 13:46:26.030956 28535 solver.cpp:253]     Train net output #0: loss = 6.91231 (* 1 = 6.91231 loss)
I0124 13:46:26.030962 28535 sgd_solver.cpp:106] Iteration 1040, lr = 0.01
I0124 13:46:33.618228 28535 solver.cpp:237] Iteration 1060, loss = 6.9154
I0124 13:46:33.618268 28535 solver.cpp:253]     Train net output #0: loss = 6.91213 (* 1 = 6.91213 loss)
I0124 13:46:33.618275 28535 sgd_solver.cpp:106] Iteration 1060, lr = 0.01
I0124 13:46:41.362489 28535 solver.cpp:237] Iteration 1080, loss = 6.91538
I0124 13:46:41.362608 28535 solver.cpp:253]     Train net output #0: loss = 6.88987 (* 1 = 6.88987 loss)
I0124 13:46:41.362617 28535 sgd_solver.cpp:106] Iteration 1080, lr = 0.01
I0124 13:46:49.048907 28535 solver.cpp:237] Iteration 1100, loss = 6.91694
I0124 13:46:49.048943 28535 solver.cpp:253]     Train net output #0: loss = 6.91943 (* 1 = 6.91943 loss)
I0124 13:46:49.048949 28535 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I0124 13:46:56.665623 28535 solver.cpp:237] Iteration 1120, loss = 6.91859
I0124 13:46:56.665711 28535 solver.cpp:253]     Train net output #0: loss = 6.91772 (* 1 = 6.91772 loss)
I0124 13:46:56.665735 28535 sgd_solver.cpp:106] Iteration 1120, lr = 0.01
I0124 13:47:04.225113 28535 solver.cpp:237] Iteration 1140, loss = 6.9138
I0124 13:47:04.225144 28535 solver.cpp:253]     Train net output #0: loss = 6.91674 (* 1 = 6.91674 loss)
I0124 13:47:04.225152 28535 sgd_solver.cpp:106] Iteration 1140, lr = 0.01
I0124 13:47:11.814945 28535 solver.cpp:237] Iteration 1160, loss = 6.91247
I0124 13:47:11.815052 28535 solver.cpp:253]     Train net output #0: loss = 6.89432 (* 1 = 6.89432 loss)
I0124 13:47:11.815116 28535 sgd_solver.cpp:106] Iteration 1160, lr = 0.01
I0124 13:47:19.419653 28535 solver.cpp:237] Iteration 1180, loss = 6.91792
I0124 13:47:19.419687 28535 solver.cpp:253]     Train net output #0: loss = 6.91038 (* 1 = 6.91038 loss)
I0124 13:47:19.419694 28535 sgd_solver.cpp:106] Iteration 1180, lr = 0.01
