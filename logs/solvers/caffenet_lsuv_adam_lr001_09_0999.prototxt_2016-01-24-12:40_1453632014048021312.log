I0124 12:41:10.653173 27399 caffe.cpp:184] Using GPUs 0
I0124 12:41:10.794234 27399 solver.cpp:48] Initializing solver from parameters: 
test_iter: 200
test_interval: 2000
base_lr: 0.01
display: 20
max_iter: 320000
lr_policy: "fixed"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "snapshots1/caffenet128_adam"
solver_mode: GPU
device_id: 0
net_param {
  name: "CaffeNet"
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TRAIN
    }
    transform_param {
      scale: 0.04
      mirror: true
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 124
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/home/old-ufo/datasets/imagenet/ilsvrc12_train_shuffle_lmdb"
      batch_size: 256
      backend: LMDB
    }
  }
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TEST
    }
    transform_param {
      scale: 0.04
      mirror: false
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 123
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/home/old-ufo/datasets/imagenet/ilsvrc12_val_lmdb"
      batch_size: 250
      backend: LMDB
    }
  }
  layer {
    name: "conv1"
    type: "Convolution"
    bottom: "data"
    top: "conv1"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 96
      kernel_size: 11
      stride: 4
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu1"
    type: "ReLU"
    bottom: "conv1"
    top: "relu1"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "relu1"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv2"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 2
      kernel_size: 5
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu2"
    type: "ReLU"
    bottom: "conv2"
    top: "conv2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "conv2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv3"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu3"
    type: "ReLU"
    bottom: "conv3"
    top: "conv3"
  }
  layer {
    name: "conv4"
    type: "Convolution"
    bottom: "conv3"
    top: "conv4"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu4"
    type: "ReLU"
    bottom: "conv4"
    top: "conv4"
  }
  layer {
    name: "conv5"
    type: "Convolution"
    bottom: "conv4"
    top: "conv5"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu5"
    type: "ReLU"
    bottom: "conv5"
    top: "conv5"
  }
  layer {
    name: "pool5"
    type: "Pooling"
    bottom: "conv5"
    top: "pool5"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "pool5"
    top: "fc6"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu6"
    type: "ReLU"
    bottom: "fc6"
    top: "fc6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "fc6"
    top: "fc6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "fc6"
    top: "fc7"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu7"
    type: "ReLU"
    bottom: "fc7"
    top: "fc7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "fc7"
    top: "fc7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc8"
    type: "InnerProduct"
    bottom: "fc7"
    top: "fc8"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 1000
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "accuracy"
    type: "Accuracy"
    bottom: "fc8"
    bottom: "label"
    top: "accuracy"
    include {
      phase: TEST
    }
  }
  layer {
    name: "loss"
    type: "SoftmaxWithLoss"
    bottom: "fc8"
    bottom: "label"
    top: "loss"
  }
}
test_initialization: false
average_loss: 20
iter_size: 1
momentum2: 0.999
type: "Adam"
I0124 12:41:11.224367 27399 solver.cpp:86] Creating training net specified in net_param.
I0124 12:41:11.224516 27399 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0124 12:41:11.224542 27399 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0124 12:41:11.224745 27399 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.04
    mirror: true
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 124
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/old-ufo/datasets/imagenet/ilsvrc12_train_shuffle_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 12:41:11.224874 27399 layer_factory.hpp:76] Creating layer data
I0124 12:41:11.225680 27399 net.cpp:106] Creating Layer data
I0124 12:41:11.225692 27399 net.cpp:411] data -> data
I0124 12:41:11.225759 27399 net.cpp:411] data -> label
I0124 12:41:11.227219 27403 db_lmdb.cpp:38] Opened lmdb /home/old-ufo/datasets/imagenet/ilsvrc12_train_shuffle_lmdb
I0124 12:41:11.241104 27399 data_layer.cpp:41] output data size: 256,3,128,128
I0124 12:41:11.309856 27399 net.cpp:150] Setting up data
I0124 12:41:11.309890 27399 net.cpp:157] Top shape: 256 3 128 128 (12582912)
I0124 12:41:11.309900 27399 net.cpp:157] Top shape: 256 (256)
I0124 12:41:11.309906 27399 net.cpp:165] Memory required for data: 50332672
I0124 12:41:11.309918 27399 layer_factory.hpp:76] Creating layer conv1
I0124 12:41:11.309937 27399 net.cpp:106] Creating Layer conv1
I0124 12:41:11.309942 27399 net.cpp:454] conv1 <- data
I0124 12:41:11.309959 27399 net.cpp:411] conv1 -> conv1
I0124 12:41:11.492379 27399 net.cpp:150] Setting up conv1
I0124 12:41:11.492408 27399 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I0124 12:41:11.492413 27399 net.cpp:165] Memory required for data: 138806272
I0124 12:41:11.492437 27399 layer_factory.hpp:76] Creating layer relu1
I0124 12:41:11.492460 27399 net.cpp:106] Creating Layer relu1
I0124 12:41:11.492465 27399 net.cpp:454] relu1 <- conv1
I0124 12:41:11.492472 27399 net.cpp:411] relu1 -> relu1
I0124 12:41:11.493185 27399 net.cpp:150] Setting up relu1
I0124 12:41:11.493198 27399 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I0124 12:41:11.493202 27399 net.cpp:165] Memory required for data: 227279872
I0124 12:41:11.493207 27399 layer_factory.hpp:76] Creating layer pool1
I0124 12:41:11.493216 27399 net.cpp:106] Creating Layer pool1
I0124 12:41:11.493221 27399 net.cpp:454] pool1 <- relu1
I0124 12:41:11.493227 27399 net.cpp:411] pool1 -> pool1
I0124 12:41:11.493945 27399 net.cpp:150] Setting up pool1
I0124 12:41:11.493958 27399 net.cpp:157] Top shape: 256 96 15 15 (5529600)
I0124 12:41:11.493963 27399 net.cpp:165] Memory required for data: 249398272
I0124 12:41:11.493968 27399 layer_factory.hpp:76] Creating layer conv2
I0124 12:41:11.493980 27399 net.cpp:106] Creating Layer conv2
I0124 12:41:11.493984 27399 net.cpp:454] conv2 <- pool1
I0124 12:41:11.493990 27399 net.cpp:411] conv2 -> conv2
I0124 12:41:11.509901 27399 net.cpp:150] Setting up conv2
I0124 12:41:11.509928 27399 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I0124 12:41:11.509933 27399 net.cpp:165] Memory required for data: 308380672
I0124 12:41:11.509950 27399 layer_factory.hpp:76] Creating layer relu2
I0124 12:41:11.509961 27399 net.cpp:106] Creating Layer relu2
I0124 12:41:11.509968 27399 net.cpp:454] relu2 <- conv2
I0124 12:41:11.509976 27399 net.cpp:397] relu2 -> conv2 (in-place)
I0124 12:41:11.510702 27399 net.cpp:150] Setting up relu2
I0124 12:41:11.510715 27399 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I0124 12:41:11.510720 27399 net.cpp:165] Memory required for data: 367363072
I0124 12:41:11.510725 27399 layer_factory.hpp:76] Creating layer pool2
I0124 12:41:11.510735 27399 net.cpp:106] Creating Layer pool2
I0124 12:41:11.510738 27399 net.cpp:454] pool2 <- conv2
I0124 12:41:11.510746 27399 net.cpp:411] pool2 -> pool2
I0124 12:41:11.511613 27399 net.cpp:150] Setting up pool2
I0124 12:41:11.511626 27399 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I0124 12:41:11.511631 27399 net.cpp:165] Memory required for data: 380208128
I0124 12:41:11.511634 27399 layer_factory.hpp:76] Creating layer conv3
I0124 12:41:11.511646 27399 net.cpp:106] Creating Layer conv3
I0124 12:41:11.511651 27399 net.cpp:454] conv3 <- pool2
I0124 12:41:11.511659 27399 net.cpp:411] conv3 -> conv3
I0124 12:41:11.546068 27399 net.cpp:150] Setting up conv3
I0124 12:41:11.546094 27399 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 12:41:11.546100 27399 net.cpp:165] Memory required for data: 399475712
I0124 12:41:11.546116 27399 layer_factory.hpp:76] Creating layer relu3
I0124 12:41:11.546130 27399 net.cpp:106] Creating Layer relu3
I0124 12:41:11.546136 27399 net.cpp:454] relu3 <- conv3
I0124 12:41:11.546144 27399 net.cpp:397] relu3 -> conv3 (in-place)
I0124 12:41:11.546898 27399 net.cpp:150] Setting up relu3
I0124 12:41:11.546912 27399 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 12:41:11.546916 27399 net.cpp:165] Memory required for data: 418743296
I0124 12:41:11.546941 27399 layer_factory.hpp:76] Creating layer conv4
I0124 12:41:11.546953 27399 net.cpp:106] Creating Layer conv4
I0124 12:41:11.546958 27399 net.cpp:454] conv4 <- conv3
I0124 12:41:11.546967 27399 net.cpp:411] conv4 -> conv4
I0124 12:41:11.575824 27399 net.cpp:150] Setting up conv4
I0124 12:41:11.575853 27399 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 12:41:11.575860 27399 net.cpp:165] Memory required for data: 438010880
I0124 12:41:11.575871 27399 layer_factory.hpp:76] Creating layer relu4
I0124 12:41:11.575883 27399 net.cpp:106] Creating Layer relu4
I0124 12:41:11.575889 27399 net.cpp:454] relu4 <- conv4
I0124 12:41:11.575901 27399 net.cpp:397] relu4 -> conv4 (in-place)
I0124 12:41:11.576655 27399 net.cpp:150] Setting up relu4
I0124 12:41:11.576669 27399 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 12:41:11.576673 27399 net.cpp:165] Memory required for data: 457278464
I0124 12:41:11.576678 27399 layer_factory.hpp:76] Creating layer conv5
I0124 12:41:11.576691 27399 net.cpp:106] Creating Layer conv5
I0124 12:41:11.576696 27399 net.cpp:454] conv5 <- conv4
I0124 12:41:11.576704 27399 net.cpp:411] conv5 -> conv5
I0124 12:41:11.597650 27399 net.cpp:150] Setting up conv5
I0124 12:41:11.597677 27399 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I0124 12:41:11.597682 27399 net.cpp:165] Memory required for data: 470123520
I0124 12:41:11.597704 27399 layer_factory.hpp:76] Creating layer relu5
I0124 12:41:11.597715 27399 net.cpp:106] Creating Layer relu5
I0124 12:41:11.597720 27399 net.cpp:454] relu5 <- conv5
I0124 12:41:11.597728 27399 net.cpp:397] relu5 -> conv5 (in-place)
I0124 12:41:11.598467 27399 net.cpp:150] Setting up relu5
I0124 12:41:11.598480 27399 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I0124 12:41:11.598484 27399 net.cpp:165] Memory required for data: 482968576
I0124 12:41:11.598489 27399 layer_factory.hpp:76] Creating layer pool5
I0124 12:41:11.598496 27399 net.cpp:106] Creating Layer pool5
I0124 12:41:11.598500 27399 net.cpp:454] pool5 <- conv5
I0124 12:41:11.598508 27399 net.cpp:411] pool5 -> pool5
I0124 12:41:11.599329 27399 net.cpp:150] Setting up pool5
I0124 12:41:11.599344 27399 net.cpp:157] Top shape: 256 256 3 3 (589824)
I0124 12:41:11.599347 27399 net.cpp:165] Memory required for data: 485327872
I0124 12:41:11.599352 27399 layer_factory.hpp:76] Creating layer fc6
I0124 12:41:11.599364 27399 net.cpp:106] Creating Layer fc6
I0124 12:41:11.599369 27399 net.cpp:454] fc6 <- pool5
I0124 12:41:11.599377 27399 net.cpp:411] fc6 -> fc6
I0124 12:41:11.767125 27399 net.cpp:150] Setting up fc6
I0124 12:41:11.767153 27399 net.cpp:157] Top shape: 256 2048 (524288)
I0124 12:41:11.767158 27399 net.cpp:165] Memory required for data: 487425024
I0124 12:41:11.767169 27399 layer_factory.hpp:76] Creating layer relu6
I0124 12:41:11.767181 27399 net.cpp:106] Creating Layer relu6
I0124 12:41:11.767187 27399 net.cpp:454] relu6 <- fc6
I0124 12:41:11.767195 27399 net.cpp:397] relu6 -> fc6 (in-place)
I0124 12:41:11.768100 27399 net.cpp:150] Setting up relu6
I0124 12:41:11.768113 27399 net.cpp:157] Top shape: 256 2048 (524288)
I0124 12:41:11.768117 27399 net.cpp:165] Memory required for data: 489522176
I0124 12:41:11.768122 27399 layer_factory.hpp:76] Creating layer drop6
I0124 12:41:11.768133 27399 net.cpp:106] Creating Layer drop6
I0124 12:41:11.768138 27399 net.cpp:454] drop6 <- fc6
I0124 12:41:11.768146 27399 net.cpp:397] drop6 -> fc6 (in-place)
I0124 12:41:11.768182 27399 net.cpp:150] Setting up drop6
I0124 12:41:11.768189 27399 net.cpp:157] Top shape: 256 2048 (524288)
I0124 12:41:11.768193 27399 net.cpp:165] Memory required for data: 491619328
I0124 12:41:11.768198 27399 layer_factory.hpp:76] Creating layer fc7
I0124 12:41:11.768208 27399 net.cpp:106] Creating Layer fc7
I0124 12:41:11.768211 27399 net.cpp:454] fc7 <- fc6
I0124 12:41:11.768218 27399 net.cpp:411] fc7 -> fc7
I0124 12:41:11.917537 27399 net.cpp:150] Setting up fc7
I0124 12:41:11.917565 27399 net.cpp:157] Top shape: 256 2048 (524288)
I0124 12:41:11.917570 27399 net.cpp:165] Memory required for data: 493716480
I0124 12:41:11.917609 27399 layer_factory.hpp:76] Creating layer relu7
I0124 12:41:11.917621 27399 net.cpp:106] Creating Layer relu7
I0124 12:41:11.917628 27399 net.cpp:454] relu7 <- fc7
I0124 12:41:11.917635 27399 net.cpp:397] relu7 -> fc7 (in-place)
I0124 12:41:11.918606 27399 net.cpp:150] Setting up relu7
I0124 12:41:11.918619 27399 net.cpp:157] Top shape: 256 2048 (524288)
I0124 12:41:11.918623 27399 net.cpp:165] Memory required for data: 495813632
I0124 12:41:11.918627 27399 layer_factory.hpp:76] Creating layer drop7
I0124 12:41:11.918637 27399 net.cpp:106] Creating Layer drop7
I0124 12:41:11.918640 27399 net.cpp:454] drop7 <- fc7
I0124 12:41:11.918648 27399 net.cpp:397] drop7 -> fc7 (in-place)
I0124 12:41:11.918680 27399 net.cpp:150] Setting up drop7
I0124 12:41:11.918689 27399 net.cpp:157] Top shape: 256 2048 (524288)
I0124 12:41:11.918692 27399 net.cpp:165] Memory required for data: 497910784
I0124 12:41:11.918695 27399 layer_factory.hpp:76] Creating layer fc8
I0124 12:41:11.918706 27399 net.cpp:106] Creating Layer fc8
I0124 12:41:11.918710 27399 net.cpp:454] fc8 <- fc7
I0124 12:41:11.918715 27399 net.cpp:411] fc8 -> fc8
I0124 12:41:11.991945 27399 net.cpp:150] Setting up fc8
I0124 12:41:11.991973 27399 net.cpp:157] Top shape: 256 1000 (256000)
I0124 12:41:11.991978 27399 net.cpp:165] Memory required for data: 498934784
I0124 12:41:11.991991 27399 layer_factory.hpp:76] Creating layer loss
I0124 12:41:11.992004 27399 net.cpp:106] Creating Layer loss
I0124 12:41:11.992010 27399 net.cpp:454] loss <- fc8
I0124 12:41:11.992017 27399 net.cpp:454] loss <- label
I0124 12:41:11.992025 27399 net.cpp:411] loss -> loss
I0124 12:41:11.992038 27399 layer_factory.hpp:76] Creating layer loss
I0124 12:41:11.993983 27399 net.cpp:150] Setting up loss
I0124 12:41:11.993999 27399 net.cpp:157] Top shape: (1)
I0124 12:41:11.994004 27399 net.cpp:160]     with loss weight 1
I0124 12:41:11.994020 27399 net.cpp:165] Memory required for data: 498934788
I0124 12:41:11.994025 27399 net.cpp:226] loss needs backward computation.
I0124 12:41:11.994030 27399 net.cpp:226] fc8 needs backward computation.
I0124 12:41:11.994035 27399 net.cpp:226] drop7 needs backward computation.
I0124 12:41:11.994038 27399 net.cpp:226] relu7 needs backward computation.
I0124 12:41:11.994041 27399 net.cpp:226] fc7 needs backward computation.
I0124 12:41:11.994045 27399 net.cpp:226] drop6 needs backward computation.
I0124 12:41:11.994050 27399 net.cpp:226] relu6 needs backward computation.
I0124 12:41:11.994052 27399 net.cpp:226] fc6 needs backward computation.
I0124 12:41:11.994056 27399 net.cpp:226] pool5 needs backward computation.
I0124 12:41:11.994060 27399 net.cpp:226] relu5 needs backward computation.
I0124 12:41:11.994063 27399 net.cpp:226] conv5 needs backward computation.
I0124 12:41:11.994068 27399 net.cpp:226] relu4 needs backward computation.
I0124 12:41:11.994071 27399 net.cpp:226] conv4 needs backward computation.
I0124 12:41:11.994076 27399 net.cpp:226] relu3 needs backward computation.
I0124 12:41:11.994079 27399 net.cpp:226] conv3 needs backward computation.
I0124 12:41:11.994083 27399 net.cpp:226] pool2 needs backward computation.
I0124 12:41:11.994087 27399 net.cpp:226] relu2 needs backward computation.
I0124 12:41:11.994091 27399 net.cpp:226] conv2 needs backward computation.
I0124 12:41:11.994096 27399 net.cpp:226] pool1 needs backward computation.
I0124 12:41:11.994099 27399 net.cpp:226] relu1 needs backward computation.
I0124 12:41:11.994102 27399 net.cpp:226] conv1 needs backward computation.
I0124 12:41:11.994107 27399 net.cpp:228] data does not need backward computation.
I0124 12:41:11.994110 27399 net.cpp:270] This network produces output loss
I0124 12:41:11.994127 27399 net.cpp:283] Network initialization done.
I0124 12:41:11.994230 27399 solver.cpp:181] Creating test net (#0) specified by net_param
I0124 12:41:11.994274 27399 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0124 12:41:11.994475 27399 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.04
    mirror: false
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 123
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/old-ufo/datasets/imagenet/ilsvrc12_val_lmdb"
    batch_size: 250
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 12:41:11.994616 27399 layer_factory.hpp:76] Creating layer data
I0124 12:41:11.994830 27399 net.cpp:106] Creating Layer data
I0124 12:41:11.994848 27399 net.cpp:411] data -> data
I0124 12:41:11.994859 27399 net.cpp:411] data -> label
I0124 12:41:11.995694 27412 db_lmdb.cpp:38] Opened lmdb /home/old-ufo/datasets/imagenet/ilsvrc12_val_lmdb
I0124 12:41:11.998345 27399 data_layer.cpp:41] output data size: 250,3,128,128
I0124 12:41:12.069275 27399 net.cpp:150] Setting up data
I0124 12:41:12.069308 27399 net.cpp:157] Top shape: 250 3 128 128 (12288000)
I0124 12:41:12.069315 27399 net.cpp:157] Top shape: 250 (250)
I0124 12:41:12.069319 27399 net.cpp:165] Memory required for data: 49153000
I0124 12:41:12.069327 27399 layer_factory.hpp:76] Creating layer label_data_1_split
I0124 12:41:12.069341 27399 net.cpp:106] Creating Layer label_data_1_split
I0124 12:41:12.069346 27399 net.cpp:454] label_data_1_split <- label
I0124 12:41:12.069355 27399 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0124 12:41:12.069365 27399 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0124 12:41:12.069535 27399 net.cpp:150] Setting up label_data_1_split
I0124 12:41:12.069545 27399 net.cpp:157] Top shape: 250 (250)
I0124 12:41:12.069550 27399 net.cpp:157] Top shape: 250 (250)
I0124 12:41:12.069553 27399 net.cpp:165] Memory required for data: 49155000
I0124 12:41:12.069557 27399 layer_factory.hpp:76] Creating layer conv1
I0124 12:41:12.069569 27399 net.cpp:106] Creating Layer conv1
I0124 12:41:12.069574 27399 net.cpp:454] conv1 <- data
I0124 12:41:12.069581 27399 net.cpp:411] conv1 -> conv1
I0124 12:41:12.078006 27399 net.cpp:150] Setting up conv1
I0124 12:41:12.078037 27399 net.cpp:157] Top shape: 250 96 30 30 (21600000)
I0124 12:41:12.078042 27399 net.cpp:165] Memory required for data: 135555000
I0124 12:41:12.078058 27399 layer_factory.hpp:76] Creating layer relu1
I0124 12:41:12.078073 27399 net.cpp:106] Creating Layer relu1
I0124 12:41:12.078079 27399 net.cpp:454] relu1 <- conv1
I0124 12:41:12.078086 27399 net.cpp:411] relu1 -> relu1
I0124 12:41:12.078985 27399 net.cpp:150] Setting up relu1
I0124 12:41:12.079002 27399 net.cpp:157] Top shape: 250 96 30 30 (21600000)
I0124 12:41:12.079007 27399 net.cpp:165] Memory required for data: 221955000
I0124 12:41:12.079013 27399 layer_factory.hpp:76] Creating layer pool1
I0124 12:41:12.079025 27399 net.cpp:106] Creating Layer pool1
I0124 12:41:12.079030 27399 net.cpp:454] pool1 <- relu1
I0124 12:41:12.079041 27399 net.cpp:411] pool1 -> pool1
I0124 12:41:12.079972 27399 net.cpp:150] Setting up pool1
I0124 12:41:12.079990 27399 net.cpp:157] Top shape: 250 96 15 15 (5400000)
I0124 12:41:12.079995 27399 net.cpp:165] Memory required for data: 243555000
I0124 12:41:12.080000 27399 layer_factory.hpp:76] Creating layer conv2
I0124 12:41:12.080014 27399 net.cpp:106] Creating Layer conv2
I0124 12:41:12.080019 27399 net.cpp:454] conv2 <- pool1
I0124 12:41:12.080029 27399 net.cpp:411] conv2 -> conv2
I0124 12:41:12.096714 27399 net.cpp:150] Setting up conv2
I0124 12:41:12.096742 27399 net.cpp:157] Top shape: 250 256 15 15 (14400000)
I0124 12:41:12.096748 27399 net.cpp:165] Memory required for data: 301155000
I0124 12:41:12.096762 27399 layer_factory.hpp:76] Creating layer relu2
I0124 12:41:12.096774 27399 net.cpp:106] Creating Layer relu2
I0124 12:41:12.096806 27399 net.cpp:454] relu2 <- conv2
I0124 12:41:12.096817 27399 net.cpp:397] relu2 -> conv2 (in-place)
I0124 12:41:12.097614 27399 net.cpp:150] Setting up relu2
I0124 12:41:12.097625 27399 net.cpp:157] Top shape: 250 256 15 15 (14400000)
I0124 12:41:12.097630 27399 net.cpp:165] Memory required for data: 358755000
I0124 12:41:12.097635 27399 layer_factory.hpp:76] Creating layer pool2
I0124 12:41:12.097645 27399 net.cpp:106] Creating Layer pool2
I0124 12:41:12.097651 27399 net.cpp:454] pool2 <- conv2
I0124 12:41:12.097658 27399 net.cpp:411] pool2 -> pool2
I0124 12:41:12.098532 27399 net.cpp:150] Setting up pool2
I0124 12:41:12.098551 27399 net.cpp:157] Top shape: 250 256 7 7 (3136000)
I0124 12:41:12.098556 27399 net.cpp:165] Memory required for data: 371299000
I0124 12:41:12.098559 27399 layer_factory.hpp:76] Creating layer conv3
I0124 12:41:12.098575 27399 net.cpp:106] Creating Layer conv3
I0124 12:41:12.098580 27399 net.cpp:454] conv3 <- pool2
I0124 12:41:12.098589 27399 net.cpp:411] conv3 -> conv3
I0124 12:41:12.133920 27399 net.cpp:150] Setting up conv3
I0124 12:41:12.133949 27399 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 12:41:12.133954 27399 net.cpp:165] Memory required for data: 390115000
I0124 12:41:12.133968 27399 layer_factory.hpp:76] Creating layer relu3
I0124 12:41:12.133980 27399 net.cpp:106] Creating Layer relu3
I0124 12:41:12.133986 27399 net.cpp:454] relu3 <- conv3
I0124 12:41:12.133994 27399 net.cpp:397] relu3 -> conv3 (in-place)
I0124 12:41:12.134805 27399 net.cpp:150] Setting up relu3
I0124 12:41:12.134819 27399 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 12:41:12.134822 27399 net.cpp:165] Memory required for data: 408931000
I0124 12:41:12.134827 27399 layer_factory.hpp:76] Creating layer conv4
I0124 12:41:12.134840 27399 net.cpp:106] Creating Layer conv4
I0124 12:41:12.134843 27399 net.cpp:454] conv4 <- conv3
I0124 12:41:12.134852 27399 net.cpp:411] conv4 -> conv4
I0124 12:41:12.164448 27399 net.cpp:150] Setting up conv4
I0124 12:41:12.164475 27399 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 12:41:12.164480 27399 net.cpp:165] Memory required for data: 427747000
I0124 12:41:12.164490 27399 layer_factory.hpp:76] Creating layer relu4
I0124 12:41:12.164500 27399 net.cpp:106] Creating Layer relu4
I0124 12:41:12.164505 27399 net.cpp:454] relu4 <- conv4
I0124 12:41:12.164512 27399 net.cpp:397] relu4 -> conv4 (in-place)
I0124 12:41:12.165304 27399 net.cpp:150] Setting up relu4
I0124 12:41:12.165318 27399 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 12:41:12.165323 27399 net.cpp:165] Memory required for data: 446563000
I0124 12:41:12.165326 27399 layer_factory.hpp:76] Creating layer conv5
I0124 12:41:12.165338 27399 net.cpp:106] Creating Layer conv5
I0124 12:41:12.165343 27399 net.cpp:454] conv5 <- conv4
I0124 12:41:12.165350 27399 net.cpp:411] conv5 -> conv5
I0124 12:41:12.186942 27399 net.cpp:150] Setting up conv5
I0124 12:41:12.186969 27399 net.cpp:157] Top shape: 250 256 7 7 (3136000)
I0124 12:41:12.186975 27399 net.cpp:165] Memory required for data: 459107000
I0124 12:41:12.186988 27399 layer_factory.hpp:76] Creating layer relu5
I0124 12:41:12.186998 27399 net.cpp:106] Creating Layer relu5
I0124 12:41:12.187003 27399 net.cpp:454] relu5 <- conv5
I0124 12:41:12.187012 27399 net.cpp:397] relu5 -> conv5 (in-place)
I0124 12:41:12.187782 27399 net.cpp:150] Setting up relu5
I0124 12:41:12.187795 27399 net.cpp:157] Top shape: 250 256 7 7 (3136000)
I0124 12:41:12.187799 27399 net.cpp:165] Memory required for data: 471651000
I0124 12:41:12.187803 27399 layer_factory.hpp:76] Creating layer pool5
I0124 12:41:12.187814 27399 net.cpp:106] Creating Layer pool5
I0124 12:41:12.187819 27399 net.cpp:454] pool5 <- conv5
I0124 12:41:12.187826 27399 net.cpp:411] pool5 -> pool5
I0124 12:41:12.188652 27399 net.cpp:150] Setting up pool5
I0124 12:41:12.188663 27399 net.cpp:157] Top shape: 250 256 3 3 (576000)
I0124 12:41:12.188668 27399 net.cpp:165] Memory required for data: 473955000
I0124 12:41:12.188671 27399 layer_factory.hpp:76] Creating layer fc6
I0124 12:41:12.188699 27399 net.cpp:106] Creating Layer fc6
I0124 12:41:12.188702 27399 net.cpp:454] fc6 <- pool5
I0124 12:41:12.188714 27399 net.cpp:411] fc6 -> fc6
I0124 12:41:12.356142 27399 net.cpp:150] Setting up fc6
I0124 12:41:12.356173 27399 net.cpp:157] Top shape: 250 2048 (512000)
I0124 12:41:12.356178 27399 net.cpp:165] Memory required for data: 476003000
I0124 12:41:12.356189 27399 layer_factory.hpp:76] Creating layer relu6
I0124 12:41:12.356204 27399 net.cpp:106] Creating Layer relu6
I0124 12:41:12.356211 27399 net.cpp:454] relu6 <- fc6
I0124 12:41:12.356220 27399 net.cpp:397] relu6 -> fc6 (in-place)
I0124 12:41:12.357422 27399 net.cpp:150] Setting up relu6
I0124 12:41:12.357437 27399 net.cpp:157] Top shape: 250 2048 (512000)
I0124 12:41:12.357442 27399 net.cpp:165] Memory required for data: 478051000
I0124 12:41:12.357446 27399 layer_factory.hpp:76] Creating layer drop6
I0124 12:41:12.357456 27399 net.cpp:106] Creating Layer drop6
I0124 12:41:12.357460 27399 net.cpp:454] drop6 <- fc6
I0124 12:41:12.357467 27399 net.cpp:397] drop6 -> fc6 (in-place)
I0124 12:41:12.357514 27399 net.cpp:150] Setting up drop6
I0124 12:41:12.357522 27399 net.cpp:157] Top shape: 250 2048 (512000)
I0124 12:41:12.357525 27399 net.cpp:165] Memory required for data: 480099000
I0124 12:41:12.357529 27399 layer_factory.hpp:76] Creating layer fc7
I0124 12:41:12.357537 27399 net.cpp:106] Creating Layer fc7
I0124 12:41:12.357543 27399 net.cpp:454] fc7 <- fc6
I0124 12:41:12.357549 27399 net.cpp:411] fc7 -> fc7
I0124 12:41:12.506841 27399 net.cpp:150] Setting up fc7
I0124 12:41:12.506871 27399 net.cpp:157] Top shape: 250 2048 (512000)
I0124 12:41:12.506877 27399 net.cpp:165] Memory required for data: 482147000
I0124 12:41:12.506889 27399 layer_factory.hpp:76] Creating layer relu7
I0124 12:41:12.506901 27399 net.cpp:106] Creating Layer relu7
I0124 12:41:12.506906 27399 net.cpp:454] relu7 <- fc7
I0124 12:41:12.506916 27399 net.cpp:397] relu7 -> fc7 (in-place)
I0124 12:41:12.507987 27399 net.cpp:150] Setting up relu7
I0124 12:41:12.508002 27399 net.cpp:157] Top shape: 250 2048 (512000)
I0124 12:41:12.508005 27399 net.cpp:165] Memory required for data: 484195000
I0124 12:41:12.508010 27399 layer_factory.hpp:76] Creating layer drop7
I0124 12:41:12.508019 27399 net.cpp:106] Creating Layer drop7
I0124 12:41:12.508024 27399 net.cpp:454] drop7 <- fc7
I0124 12:41:12.508030 27399 net.cpp:397] drop7 -> fc7 (in-place)
I0124 12:41:12.508077 27399 net.cpp:150] Setting up drop7
I0124 12:41:12.508085 27399 net.cpp:157] Top shape: 250 2048 (512000)
I0124 12:41:12.508087 27399 net.cpp:165] Memory required for data: 486243000
I0124 12:41:12.508091 27399 layer_factory.hpp:76] Creating layer fc8
I0124 12:41:12.508100 27399 net.cpp:106] Creating Layer fc8
I0124 12:41:12.508103 27399 net.cpp:454] fc8 <- fc7
I0124 12:41:12.508111 27399 net.cpp:411] fc8 -> fc8
I0124 12:41:12.581620 27399 net.cpp:150] Setting up fc8
I0124 12:41:12.581650 27399 net.cpp:157] Top shape: 250 1000 (250000)
I0124 12:41:12.581655 27399 net.cpp:165] Memory required for data: 487243000
I0124 12:41:12.581666 27399 layer_factory.hpp:76] Creating layer fc8_fc8_0_split
I0124 12:41:12.581678 27399 net.cpp:106] Creating Layer fc8_fc8_0_split
I0124 12:41:12.581684 27399 net.cpp:454] fc8_fc8_0_split <- fc8
I0124 12:41:12.581692 27399 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0124 12:41:12.581709 27399 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0124 12:41:12.581794 27399 net.cpp:150] Setting up fc8_fc8_0_split
I0124 12:41:12.581802 27399 net.cpp:157] Top shape: 250 1000 (250000)
I0124 12:41:12.581807 27399 net.cpp:157] Top shape: 250 1000 (250000)
I0124 12:41:12.581811 27399 net.cpp:165] Memory required for data: 489243000
I0124 12:41:12.581815 27399 layer_factory.hpp:76] Creating layer accuracy
I0124 12:41:12.581828 27399 net.cpp:106] Creating Layer accuracy
I0124 12:41:12.581833 27399 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0124 12:41:12.581838 27399 net.cpp:454] accuracy <- label_data_1_split_0
I0124 12:41:12.581845 27399 net.cpp:411] accuracy -> accuracy
I0124 12:41:12.581874 27399 net.cpp:150] Setting up accuracy
I0124 12:41:12.581882 27399 net.cpp:157] Top shape: (1)
I0124 12:41:12.581887 27399 net.cpp:165] Memory required for data: 489243004
I0124 12:41:12.581900 27399 layer_factory.hpp:76] Creating layer loss
I0124 12:41:12.581908 27399 net.cpp:106] Creating Layer loss
I0124 12:41:12.581912 27399 net.cpp:454] loss <- fc8_fc8_0_split_1
I0124 12:41:12.581918 27399 net.cpp:454] loss <- label_data_1_split_1
I0124 12:41:12.581923 27399 net.cpp:411] loss -> loss
I0124 12:41:12.581929 27399 layer_factory.hpp:76] Creating layer loss
I0124 12:41:12.583878 27399 net.cpp:150] Setting up loss
I0124 12:41:12.583895 27399 net.cpp:157] Top shape: (1)
I0124 12:41:12.583899 27399 net.cpp:160]     with loss weight 1
I0124 12:41:12.583911 27399 net.cpp:165] Memory required for data: 489243008
I0124 12:41:12.583916 27399 net.cpp:226] loss needs backward computation.
I0124 12:41:12.583921 27399 net.cpp:228] accuracy does not need backward computation.
I0124 12:41:12.583926 27399 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0124 12:41:12.583930 27399 net.cpp:226] fc8 needs backward computation.
I0124 12:41:12.583935 27399 net.cpp:226] drop7 needs backward computation.
I0124 12:41:12.583937 27399 net.cpp:226] relu7 needs backward computation.
I0124 12:41:12.583941 27399 net.cpp:226] fc7 needs backward computation.
I0124 12:41:12.583945 27399 net.cpp:226] drop6 needs backward computation.
I0124 12:41:12.583950 27399 net.cpp:226] relu6 needs backward computation.
I0124 12:41:12.583952 27399 net.cpp:226] fc6 needs backward computation.
I0124 12:41:12.583956 27399 net.cpp:226] pool5 needs backward computation.
I0124 12:41:12.583961 27399 net.cpp:226] relu5 needs backward computation.
I0124 12:41:12.583964 27399 net.cpp:226] conv5 needs backward computation.
I0124 12:41:12.583968 27399 net.cpp:226] relu4 needs backward computation.
I0124 12:41:12.583972 27399 net.cpp:226] conv4 needs backward computation.
I0124 12:41:12.583976 27399 net.cpp:226] relu3 needs backward computation.
I0124 12:41:12.583979 27399 net.cpp:226] conv3 needs backward computation.
I0124 12:41:12.583983 27399 net.cpp:226] pool2 needs backward computation.
I0124 12:41:12.583987 27399 net.cpp:226] relu2 needs backward computation.
I0124 12:41:12.583992 27399 net.cpp:226] conv2 needs backward computation.
I0124 12:41:12.583995 27399 net.cpp:226] pool1 needs backward computation.
I0124 12:41:12.584002 27399 net.cpp:226] relu1 needs backward computation.
I0124 12:41:12.584007 27399 net.cpp:226] conv1 needs backward computation.
I0124 12:41:12.584010 27399 net.cpp:228] label_data_1_split does not need backward computation.
I0124 12:41:12.584015 27399 net.cpp:228] data does not need backward computation.
I0124 12:41:12.584019 27399 net.cpp:270] This network produces output accuracy
I0124 12:41:12.584023 27399 net.cpp:270] This network produces output loss
I0124 12:41:12.584044 27399 net.cpp:283] Network initialization done.
I0124 12:41:12.584151 27399 solver.cpp:60] Solver scaffolding done.
I0124 12:41:12.585281 27399 caffe.cpp:128] Finetuning from ./caffenet_lsuv_adam.prototxt.caffemodel
I0124 12:41:13.122155 27399 caffe.cpp:212] Starting Optimization
I0124 12:41:13.122184 27399 solver.cpp:288] Solving CaffeNet
I0124 12:41:13.122187 27399 solver.cpp:289] Learning Rate Policy: fixed
I0124 12:41:13.173336 27399 solver.cpp:237] Iteration 0, loss = 7.38819
I0124 12:41:13.173420 27399 solver.cpp:253]     Train net output #0: loss = 7.38819 (* 1 = 7.38819 loss)
I0124 12:41:13.173441 27399 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0124 12:41:13.351068 27399 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 12:41:19.824612 27399 solver.cpp:237] Iteration 20, loss = 18.8887
I0124 12:41:19.824647 27399 solver.cpp:253]     Train net output #0: loss = 6.91639 (* 1 = 6.91639 loss)
I0124 12:41:19.824654 27399 sgd_solver.cpp:106] Iteration 20, lr = 0.01
I0124 12:41:27.230008 27399 solver.cpp:237] Iteration 40, loss = 6.91283
I0124 12:41:27.230046 27399 solver.cpp:253]     Train net output #0: loss = 6.90322 (* 1 = 6.90322 loss)
I0124 12:41:27.230109 27399 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I0124 12:41:34.716215 27399 solver.cpp:237] Iteration 60, loss = 7.85029
I0124 12:41:34.716253 27399 solver.cpp:253]     Train net output #0: loss = 6.91284 (* 1 = 6.91284 loss)
I0124 12:41:34.716262 27399 sgd_solver.cpp:106] Iteration 60, lr = 0.01
I0124 12:41:42.233723 27399 solver.cpp:237] Iteration 80, loss = 6.91911
I0124 12:41:42.233829 27399 solver.cpp:253]     Train net output #0: loss = 6.92614 (* 1 = 6.92614 loss)
I0124 12:41:42.233841 27399 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I0124 12:41:49.656966 27399 solver.cpp:237] Iteration 100, loss = 6.91335
I0124 12:41:49.657006 27399 solver.cpp:253]     Train net output #0: loss = 6.90739 (* 1 = 6.90739 loss)
I0124 12:41:49.657014 27399 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I0124 12:41:57.156946 27399 solver.cpp:237] Iteration 120, loss = 6.91559
I0124 12:41:57.156985 27399 solver.cpp:253]     Train net output #0: loss = 6.91357 (* 1 = 6.91357 loss)
I0124 12:41:57.156991 27399 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I0124 12:42:04.619211 27399 solver.cpp:237] Iteration 140, loss = 6.91369
I0124 12:42:04.619249 27399 solver.cpp:253]     Train net output #0: loss = 6.9054 (* 1 = 6.9054 loss)
I0124 12:42:04.619258 27399 sgd_solver.cpp:106] Iteration 140, lr = 0.01
I0124 12:42:12.137101 27399 solver.cpp:237] Iteration 160, loss = 6.91425
I0124 12:42:12.137140 27399 solver.cpp:253]     Train net output #0: loss = 6.91154 (* 1 = 6.91154 loss)
I0124 12:42:12.137150 27399 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I0124 12:42:19.669493 27399 solver.cpp:237] Iteration 180, loss = 6.91838
I0124 12:42:19.669575 27399 solver.cpp:253]     Train net output #0: loss = 6.91446 (* 1 = 6.91446 loss)
I0124 12:42:19.669586 27399 sgd_solver.cpp:106] Iteration 180, lr = 0.01
I0124 12:42:27.156641 27399 solver.cpp:237] Iteration 200, loss = 6.91777
I0124 12:42:27.156702 27399 solver.cpp:253]     Train net output #0: loss = 6.90789 (* 1 = 6.90789 loss)
I0124 12:42:27.156714 27399 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I0124 12:42:34.619267 27399 solver.cpp:237] Iteration 220, loss = 6.91539
I0124 12:42:34.619366 27399 solver.cpp:253]     Train net output #0: loss = 6.91759 (* 1 = 6.91759 loss)
I0124 12:42:34.619397 27399 sgd_solver.cpp:106] Iteration 220, lr = 0.01
I0124 12:42:42.180619 27399 solver.cpp:237] Iteration 240, loss = 6.91564
I0124 12:42:42.180657 27399 solver.cpp:253]     Train net output #0: loss = 6.91155 (* 1 = 6.91155 loss)
I0124 12:42:42.180666 27399 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I0124 12:42:49.777305 27399 solver.cpp:237] Iteration 260, loss = 6.91674
I0124 12:42:49.777400 27399 solver.cpp:253]     Train net output #0: loss = 6.90312 (* 1 = 6.90312 loss)
I0124 12:42:49.777416 27399 sgd_solver.cpp:106] Iteration 260, lr = 0.01
I0124 12:42:57.400779 27399 solver.cpp:237] Iteration 280, loss = 6.91421
I0124 12:42:57.400816 27399 solver.cpp:253]     Train net output #0: loss = 6.91569 (* 1 = 6.91569 loss)
I0124 12:42:57.400825 27399 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I0124 12:43:05.020499 27399 solver.cpp:237] Iteration 300, loss = 6.91617
I0124 12:43:05.020536 27399 solver.cpp:253]     Train net output #0: loss = 6.91353 (* 1 = 6.91353 loss)
I0124 12:43:05.020545 27399 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I0124 12:43:12.555418 27399 solver.cpp:237] Iteration 320, loss = 6.91464
I0124 12:43:12.555454 27399 solver.cpp:253]     Train net output #0: loss = 6.91522 (* 1 = 6.91522 loss)
I0124 12:43:12.555465 27399 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I0124 12:43:20.181596 27399 solver.cpp:237] Iteration 340, loss = 6.91656
I0124 12:43:20.181679 27399 solver.cpp:253]     Train net output #0: loss = 6.92274 (* 1 = 6.92274 loss)
I0124 12:43:20.181687 27399 sgd_solver.cpp:106] Iteration 340, lr = 0.01
I0124 12:43:27.800349 27399 solver.cpp:237] Iteration 360, loss = 6.9157
I0124 12:43:27.800389 27399 solver.cpp:253]     Train net output #0: loss = 6.92231 (* 1 = 6.92231 loss)
I0124 12:43:27.800398 27399 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I0124 12:43:35.250012 27399 solver.cpp:237] Iteration 380, loss = 6.91408
I0124 12:43:35.250051 27399 solver.cpp:253]     Train net output #0: loss = 6.91722 (* 1 = 6.91722 loss)
I0124 12:43:35.250058 27399 sgd_solver.cpp:106] Iteration 380, lr = 0.01
I0124 12:43:42.605340 27399 solver.cpp:237] Iteration 400, loss = 6.92023
I0124 12:43:42.605373 27399 solver.cpp:253]     Train net output #0: loss = 6.92179 (* 1 = 6.92179 loss)
I0124 12:43:42.605382 27399 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I0124 12:43:50.014319 27399 solver.cpp:237] Iteration 420, loss = 6.91673
I0124 12:43:50.014358 27399 solver.cpp:253]     Train net output #0: loss = 6.91429 (* 1 = 6.91429 loss)
I0124 12:43:50.014367 27399 sgd_solver.cpp:106] Iteration 420, lr = 0.01
I0124 12:43:57.460911 27399 solver.cpp:237] Iteration 440, loss = 6.91756
I0124 12:43:57.461014 27399 solver.cpp:253]     Train net output #0: loss = 6.92642 (* 1 = 6.92642 loss)
I0124 12:43:57.461026 27399 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I0124 12:44:05.039672 27399 solver.cpp:237] Iteration 460, loss = 6.91305
I0124 12:44:05.039705 27399 solver.cpp:253]     Train net output #0: loss = 6.92246 (* 1 = 6.92246 loss)
I0124 12:44:05.039712 27399 sgd_solver.cpp:106] Iteration 460, lr = 0.01
I0124 12:44:12.704097 27399 solver.cpp:237] Iteration 480, loss = 6.91304
I0124 12:44:12.704136 27399 solver.cpp:253]     Train net output #0: loss = 6.90891 (* 1 = 6.90891 loss)
I0124 12:44:12.704145 27399 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I0124 12:44:20.305166 27399 solver.cpp:237] Iteration 500, loss = 6.91828
I0124 12:44:20.305202 27399 solver.cpp:253]     Train net output #0: loss = 6.92116 (* 1 = 6.92116 loss)
I0124 12:44:20.305212 27399 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I0124 12:44:27.759488 27399 solver.cpp:237] Iteration 520, loss = 6.91711
I0124 12:44:27.759565 27399 solver.cpp:253]     Train net output #0: loss = 6.92178 (* 1 = 6.92178 loss)
I0124 12:44:27.759577 27399 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I0124 12:44:35.199215 27399 solver.cpp:237] Iteration 540, loss = 6.91949
I0124 12:44:35.199251 27399 solver.cpp:253]     Train net output #0: loss = 6.92046 (* 1 = 6.92046 loss)
I0124 12:44:35.199262 27399 sgd_solver.cpp:106] Iteration 540, lr = 0.01
I0124 12:44:42.705271 27399 solver.cpp:237] Iteration 560, loss = 6.91278
I0124 12:44:42.705307 27399 solver.cpp:253]     Train net output #0: loss = 6.90405 (* 1 = 6.90405 loss)
I0124 12:44:42.705317 27399 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I0124 12:44:50.148764 27399 solver.cpp:237] Iteration 580, loss = 6.91455
I0124 12:44:50.148798 27399 solver.cpp:253]     Train net output #0: loss = 6.90029 (* 1 = 6.90029 loss)
I0124 12:44:50.148808 27399 sgd_solver.cpp:106] Iteration 580, lr = 0.01
I0124 12:44:57.586374 27399 solver.cpp:237] Iteration 600, loss = 6.91531
I0124 12:44:57.586412 27399 solver.cpp:253]     Train net output #0: loss = 6.90569 (* 1 = 6.90569 loss)
I0124 12:44:57.586421 27399 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I0124 12:45:05.026257 27399 solver.cpp:237] Iteration 620, loss = 6.91612
I0124 12:45:05.026708 27399 solver.cpp:253]     Train net output #0: loss = 6.92789 (* 1 = 6.92789 loss)
I0124 12:45:05.026721 27399 sgd_solver.cpp:106] Iteration 620, lr = 0.01
I0124 12:45:12.532099 27399 solver.cpp:237] Iteration 640, loss = 6.9162
I0124 12:45:12.532135 27399 solver.cpp:253]     Train net output #0: loss = 6.92896 (* 1 = 6.92896 loss)
I0124 12:45:12.532145 27399 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I0124 12:45:20.086292 27399 solver.cpp:237] Iteration 660, loss = 6.91811
I0124 12:45:20.086328 27399 solver.cpp:253]     Train net output #0: loss = 6.91469 (* 1 = 6.91469 loss)
I0124 12:45:20.086338 27399 sgd_solver.cpp:106] Iteration 660, lr = 0.01
I0124 12:45:27.681223 27399 solver.cpp:237] Iteration 680, loss = 6.916
I0124 12:45:27.681260 27399 solver.cpp:253]     Train net output #0: loss = 6.9119 (* 1 = 6.9119 loss)
I0124 12:45:27.681270 27399 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I0124 12:45:35.340361 27399 solver.cpp:237] Iteration 700, loss = 6.91336
I0124 12:45:35.340458 27399 solver.cpp:253]     Train net output #0: loss = 6.91679 (* 1 = 6.91679 loss)
I0124 12:45:35.340467 27399 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I0124 12:45:43.213201 27399 solver.cpp:237] Iteration 720, loss = 6.91831
I0124 12:45:43.213238 27399 solver.cpp:253]     Train net output #0: loss = 6.90318 (* 1 = 6.90318 loss)
I0124 12:45:43.213248 27399 sgd_solver.cpp:106] Iteration 720, lr = 0.01
I0124 12:45:51.003640 27399 solver.cpp:237] Iteration 740, loss = 6.92014
I0124 12:45:51.003679 27399 solver.cpp:253]     Train net output #0: loss = 6.92724 (* 1 = 6.92724 loss)
I0124 12:45:51.003686 27399 sgd_solver.cpp:106] Iteration 740, lr = 0.01
I0124 12:45:58.672562 27399 solver.cpp:237] Iteration 760, loss = 6.91566
I0124 12:45:58.672775 27399 solver.cpp:253]     Train net output #0: loss = 6.91077 (* 1 = 6.91077 loss)
I0124 12:45:58.672858 27399 sgd_solver.cpp:106] Iteration 760, lr = 0.01
I0124 12:46:06.347543 27399 solver.cpp:237] Iteration 780, loss = 6.91694
I0124 12:46:06.347780 27399 solver.cpp:253]     Train net output #0: loss = 6.92456 (* 1 = 6.92456 loss)
I0124 12:46:06.347791 27399 sgd_solver.cpp:106] Iteration 780, lr = 0.01
I0124 12:46:13.885864 27399 solver.cpp:237] Iteration 800, loss = 6.91444
I0124 12:46:13.885900 27399 solver.cpp:253]     Train net output #0: loss = 6.91624 (* 1 = 6.91624 loss)
I0124 12:46:13.885907 27399 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I0124 12:46:21.365217 27399 solver.cpp:237] Iteration 820, loss = 6.91713
I0124 12:46:21.365252 27399 solver.cpp:253]     Train net output #0: loss = 6.9268 (* 1 = 6.9268 loss)
I0124 12:46:21.365259 27399 sgd_solver.cpp:106] Iteration 820, lr = 0.01
I0124 12:46:28.926962 27399 solver.cpp:237] Iteration 840, loss = 6.91449
I0124 12:46:28.927002 27399 solver.cpp:253]     Train net output #0: loss = 6.91856 (* 1 = 6.91856 loss)
I0124 12:46:28.927011 27399 sgd_solver.cpp:106] Iteration 840, lr = 0.01
I0124 12:46:36.574295 27399 solver.cpp:237] Iteration 860, loss = 6.91849
I0124 12:46:36.574388 27399 solver.cpp:253]     Train net output #0: loss = 6.93567 (* 1 = 6.93567 loss)
I0124 12:46:36.574398 27399 sgd_solver.cpp:106] Iteration 860, lr = 0.01
I0124 12:46:44.315217 27399 solver.cpp:237] Iteration 880, loss = 6.91317
I0124 12:46:44.315253 27399 solver.cpp:253]     Train net output #0: loss = 6.92221 (* 1 = 6.92221 loss)
I0124 12:46:44.315263 27399 sgd_solver.cpp:106] Iteration 880, lr = 0.01
I0124 12:46:52.073932 27399 solver.cpp:237] Iteration 900, loss = 6.91345
I0124 12:46:52.073972 27399 solver.cpp:253]     Train net output #0: loss = 6.92437 (* 1 = 6.92437 loss)
I0124 12:46:52.073982 27399 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I0124 12:46:59.722467 27399 solver.cpp:237] Iteration 920, loss = 6.91636
I0124 12:46:59.722506 27399 solver.cpp:253]     Train net output #0: loss = 6.905 (* 1 = 6.905 loss)
I0124 12:46:59.722515 27399 sgd_solver.cpp:106] Iteration 920, lr = 0.01
I0124 12:47:07.365622 27399 solver.cpp:237] Iteration 940, loss = 6.91538
I0124 12:47:07.365702 27399 solver.cpp:253]     Train net output #0: loss = 6.92152 (* 1 = 6.92152 loss)
I0124 12:47:07.365712 27399 sgd_solver.cpp:106] Iteration 940, lr = 0.01
I0124 12:47:14.951612 27399 solver.cpp:237] Iteration 960, loss = 6.91523
I0124 12:47:14.951652 27399 solver.cpp:253]     Train net output #0: loss = 6.92493 (* 1 = 6.92493 loss)
I0124 12:47:14.951660 27399 sgd_solver.cpp:106] Iteration 960, lr = 0.01
I0124 12:47:22.437161 27399 solver.cpp:237] Iteration 980, loss = 6.91821
I0124 12:47:22.437198 27399 solver.cpp:253]     Train net output #0: loss = 6.89814 (* 1 = 6.89814 loss)
I0124 12:47:22.437208 27399 sgd_solver.cpp:106] Iteration 980, lr = 0.01
I0124 12:47:29.896152 27399 solver.cpp:237] Iteration 1000, loss = 6.9178
I0124 12:47:29.896193 27399 solver.cpp:253]     Train net output #0: loss = 6.90786 (* 1 = 6.90786 loss)
I0124 12:47:29.896203 27399 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I0124 12:47:30.644006 27399 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 12:47:37.423661 27399 solver.cpp:237] Iteration 1020, loss = 6.91617
I0124 12:47:37.423780 27399 solver.cpp:253]     Train net output #0: loss = 6.91881 (* 1 = 6.91881 loss)
I0124 12:47:37.423794 27399 sgd_solver.cpp:106] Iteration 1020, lr = 0.01
I0124 12:47:44.887506 27399 solver.cpp:237] Iteration 1040, loss = 6.91368
I0124 12:47:44.887542 27399 solver.cpp:253]     Train net output #0: loss = 6.91392 (* 1 = 6.91392 loss)
I0124 12:47:44.887552 27399 sgd_solver.cpp:106] Iteration 1040, lr = 0.01
I0124 12:47:52.484874 27399 solver.cpp:237] Iteration 1060, loss = 6.91584
I0124 12:47:52.484913 27399 solver.cpp:253]     Train net output #0: loss = 6.90768 (* 1 = 6.90768 loss)
I0124 12:47:52.484923 27399 sgd_solver.cpp:106] Iteration 1060, lr = 0.01
I0124 12:47:59.981909 27399 solver.cpp:237] Iteration 1080, loss = 6.9156
I0124 12:47:59.981950 27399 solver.cpp:253]     Train net output #0: loss = 6.8883 (* 1 = 6.8883 loss)
I0124 12:47:59.981966 27399 sgd_solver.cpp:106] Iteration 1080, lr = 0.01
I0124 12:48:07.471138 27399 solver.cpp:237] Iteration 1100, loss = 6.91718
I0124 12:48:07.471220 27399 solver.cpp:253]     Train net output #0: loss = 6.91953 (* 1 = 6.91953 loss)
I0124 12:48:07.471230 27399 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I0124 12:48:14.845985 27399 solver.cpp:237] Iteration 1120, loss = 6.9191
I0124 12:48:14.846025 27399 solver.cpp:253]     Train net output #0: loss = 6.91987 (* 1 = 6.91987 loss)
I0124 12:48:14.846035 27399 sgd_solver.cpp:106] Iteration 1120, lr = 0.01
I0124 12:48:22.265909 27399 solver.cpp:237] Iteration 1140, loss = 6.9147
I0124 12:48:22.265945 27399 solver.cpp:253]     Train net output #0: loss = 6.91737 (* 1 = 6.91737 loss)
I0124 12:48:22.265952 27399 sgd_solver.cpp:106] Iteration 1140, lr = 0.01
I0124 12:48:29.744957 27399 solver.cpp:237] Iteration 1160, loss = 6.9122
I0124 12:48:29.744995 27399 solver.cpp:253]     Train net output #0: loss = 6.89169 (* 1 = 6.89169 loss)
I0124 12:48:29.745005 27399 sgd_solver.cpp:106] Iteration 1160, lr = 0.01
I0124 12:48:37.168730 27399 solver.cpp:237] Iteration 1180, loss = 6.91809
I0124 12:48:37.168768 27399 solver.cpp:253]     Train net output #0: loss = 6.91162 (* 1 = 6.91162 loss)
I0124 12:48:37.168778 27399 sgd_solver.cpp:106] Iteration 1180, lr = 0.01
I0124 12:48:44.625160 27399 solver.cpp:237] Iteration 1200, loss = 6.9138
I0124 12:48:44.625341 27399 solver.cpp:253]     Train net output #0: loss = 6.90091 (* 1 = 6.90091 loss)
I0124 12:48:44.625351 27399 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I0124 12:48:52.043860 27399 solver.cpp:237] Iteration 1220, loss = 6.91793
I0124 12:48:52.043894 27399 solver.cpp:253]     Train net output #0: loss = 6.92444 (* 1 = 6.92444 loss)
I0124 12:48:52.043903 27399 sgd_solver.cpp:106] Iteration 1220, lr = 0.01
I0124 12:48:59.509192 27399 solver.cpp:237] Iteration 1240, loss = 6.91431
I0124 12:48:59.509229 27399 solver.cpp:253]     Train net output #0: loss = 6.90017 (* 1 = 6.90017 loss)
I0124 12:48:59.509239 27399 sgd_solver.cpp:106] Iteration 1240, lr = 0.01
I0124 12:49:07.179705 27399 solver.cpp:237] Iteration 1260, loss = 6.91711
I0124 12:49:07.179740 27399 solver.cpp:253]     Train net output #0: loss = 6.91846 (* 1 = 6.91846 loss)
I0124 12:49:07.179749 27399 sgd_solver.cpp:106] Iteration 1260, lr = 0.01
I0124 12:49:14.749747 27399 solver.cpp:237] Iteration 1280, loss = 6.91985
I0124 12:49:14.749821 27399 solver.cpp:253]     Train net output #0: loss = 6.90741 (* 1 = 6.90741 loss)
I0124 12:49:14.749830 27399 sgd_solver.cpp:106] Iteration 1280, lr = 0.01
I0124 12:49:21.403137 27404 blocking_queue.cpp:50] Waiting for data
I0124 12:49:22.478883 27399 solver.cpp:237] Iteration 1300, loss = 6.91386
I0124 12:49:22.478920 27399 solver.cpp:253]     Train net output #0: loss = 6.92763 (* 1 = 6.92763 loss)
I0124 12:49:22.478929 27399 sgd_solver.cpp:106] Iteration 1300, lr = 0.01
I0124 12:49:29.962401 27399 solver.cpp:237] Iteration 1320, loss = 6.91877
I0124 12:49:29.962436 27399 solver.cpp:253]     Train net output #0: loss = 6.9313 (* 1 = 6.9313 loss)
I0124 12:49:29.962443 27399 sgd_solver.cpp:106] Iteration 1320, lr = 0.01
I0124 12:49:37.487514 27399 solver.cpp:237] Iteration 1340, loss = 6.91493
I0124 12:49:37.487555 27399 solver.cpp:253]     Train net output #0: loss = 6.91182 (* 1 = 6.91182 loss)
I0124 12:49:37.487562 27399 sgd_solver.cpp:106] Iteration 1340, lr = 0.01
I0124 12:49:45.066232 27399 solver.cpp:237] Iteration 1360, loss = 6.91562
I0124 12:49:45.066329 27399 solver.cpp:253]     Train net output #0: loss = 6.91567 (* 1 = 6.91567 loss)
I0124 12:49:45.066337 27399 sgd_solver.cpp:106] Iteration 1360, lr = 0.01
I0124 12:49:52.629994 27399 solver.cpp:237] Iteration 1380, loss = 6.91535
I0124 12:49:52.630028 27399 solver.cpp:253]     Train net output #0: loss = 6.92176 (* 1 = 6.92176 loss)
I0124 12:49:52.630035 27399 sgd_solver.cpp:106] Iteration 1380, lr = 0.01
I0124 12:50:00.162581 27399 solver.cpp:237] Iteration 1400, loss = 6.91462
I0124 12:50:00.162618 27399 solver.cpp:253]     Train net output #0: loss = 6.90309 (* 1 = 6.90309 loss)
I0124 12:50:00.162626 27399 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I0124 12:50:07.789569 27399 solver.cpp:237] Iteration 1420, loss = 6.91806
I0124 12:50:07.789603 27399 solver.cpp:253]     Train net output #0: loss = 6.91216 (* 1 = 6.91216 loss)
I0124 12:50:07.789610 27399 sgd_solver.cpp:106] Iteration 1420, lr = 0.01
I0124 12:50:15.334192 27399 solver.cpp:237] Iteration 1440, loss = 6.91605
I0124 12:50:15.334262 27399 solver.cpp:253]     Train net output #0: loss = 6.92189 (* 1 = 6.92189 loss)
I0124 12:50:15.334271 27399 sgd_solver.cpp:106] Iteration 1440, lr = 0.01
I0124 12:50:22.787782 27399 solver.cpp:237] Iteration 1460, loss = 6.91378
I0124 12:50:22.787892 27399 solver.cpp:253]     Train net output #0: loss = 6.91675 (* 1 = 6.91675 loss)
I0124 12:50:22.787922 27399 sgd_solver.cpp:106] Iteration 1460, lr = 0.01
I0124 12:50:30.253113 27399 solver.cpp:237] Iteration 1480, loss = 6.91917
I0124 12:50:30.253150 27399 solver.cpp:253]     Train net output #0: loss = 6.92487 (* 1 = 6.92487 loss)
I0124 12:50:30.253160 27399 sgd_solver.cpp:106] Iteration 1480, lr = 0.01
I0124 12:50:37.750910 27399 solver.cpp:237] Iteration 1500, loss = 6.91425
I0124 12:50:37.750947 27399 solver.cpp:253]     Train net output #0: loss = 6.9221 (* 1 = 6.9221 loss)
I0124 12:50:37.750957 27399 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I0124 12:50:45.211472 27399 solver.cpp:237] Iteration 1520, loss = 6.91363
I0124 12:50:45.211514 27399 solver.cpp:253]     Train net output #0: loss = 6.90999 (* 1 = 6.90999 loss)
I0124 12:50:45.211525 27399 sgd_solver.cpp:106] Iteration 1520, lr = 0.01
I0124 12:50:52.638768 27399 solver.cpp:237] Iteration 1540, loss = 6.91891
I0124 12:50:52.639164 27399 solver.cpp:253]     Train net output #0: loss = 6.93086 (* 1 = 6.93086 loss)
I0124 12:50:52.639178 27399 sgd_solver.cpp:106] Iteration 1540, lr = 0.01
I0124 12:51:00.099485 27399 solver.cpp:237] Iteration 1560, loss = 6.9156
I0124 12:51:00.099525 27399 solver.cpp:253]     Train net output #0: loss = 6.91533 (* 1 = 6.91533 loss)
I0124 12:51:00.099534 27399 sgd_solver.cpp:106] Iteration 1560, lr = 0.01
I0124 12:51:07.672845 27399 solver.cpp:237] Iteration 1580, loss = 6.91799
I0124 12:51:07.672885 27399 solver.cpp:253]     Train net output #0: loss = 6.92114 (* 1 = 6.92114 loss)
I0124 12:51:07.672895 27399 sgd_solver.cpp:106] Iteration 1580, lr = 0.01
I0124 12:51:15.105654 27399 solver.cpp:237] Iteration 1600, loss = 6.91812
I0124 12:51:15.105692 27399 solver.cpp:253]     Train net output #0: loss = 6.92162 (* 1 = 6.92162 loss)
I0124 12:51:15.105707 27399 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I0124 12:51:22.512258 27399 solver.cpp:237] Iteration 1620, loss = 6.91634
I0124 12:51:22.512295 27399 solver.cpp:253]     Train net output #0: loss = 6.90932 (* 1 = 6.90932 loss)
I0124 12:51:22.512305 27399 sgd_solver.cpp:106] Iteration 1620, lr = 0.01
I0124 12:51:29.978018 27399 solver.cpp:237] Iteration 1640, loss = 6.91751
I0124 12:51:29.978096 27399 solver.cpp:253]     Train net output #0: loss = 6.90508 (* 1 = 6.90508 loss)
I0124 12:51:29.978175 27399 sgd_solver.cpp:106] Iteration 1640, lr = 0.01
I0124 12:51:37.397611 27399 solver.cpp:237] Iteration 1660, loss = 6.91653
I0124 12:51:37.397650 27399 solver.cpp:253]     Train net output #0: loss = 6.90779 (* 1 = 6.90779 loss)
I0124 12:51:37.397658 27399 sgd_solver.cpp:106] Iteration 1660, lr = 0.01
I0124 12:51:44.972551 27399 solver.cpp:237] Iteration 1680, loss = 6.91545
I0124 12:51:44.972762 27399 solver.cpp:253]     Train net output #0: loss = 6.9136 (* 1 = 6.9136 loss)
I0124 12:51:44.972847 27399 sgd_solver.cpp:106] Iteration 1680, lr = 0.01
I0124 12:51:52.427297 27399 solver.cpp:237] Iteration 1700, loss = 6.91799
I0124 12:51:52.427347 27399 solver.cpp:253]     Train net output #0: loss = 6.90078 (* 1 = 6.90078 loss)
I0124 12:51:52.427356 27399 sgd_solver.cpp:106] Iteration 1700, lr = 0.01
I0124 12:51:59.877213 27399 solver.cpp:237] Iteration 1720, loss = 6.91271
I0124 12:51:59.877249 27399 solver.cpp:253]     Train net output #0: loss = 6.89232 (* 1 = 6.89232 loss)
I0124 12:51:59.877255 27399 sgd_solver.cpp:106] Iteration 1720, lr = 0.01
I0124 12:52:07.346966 27399 solver.cpp:237] Iteration 1740, loss = 6.91407
I0124 12:52:07.347059 27399 solver.cpp:253]     Train net output #0: loss = 6.91704 (* 1 = 6.91704 loss)
I0124 12:52:07.347069 27399 sgd_solver.cpp:106] Iteration 1740, lr = 0.01
I0124 12:52:14.799105 27399 solver.cpp:237] Iteration 1760, loss = 6.92046
I0124 12:52:14.799140 27399 solver.cpp:253]     Train net output #0: loss = 6.91609 (* 1 = 6.91609 loss)
I0124 12:52:14.799150 27399 sgd_solver.cpp:106] Iteration 1760, lr = 0.01
I0124 12:52:22.251019 27399 solver.cpp:237] Iteration 1780, loss = 6.91556
I0124 12:52:22.251060 27399 solver.cpp:253]     Train net output #0: loss = 6.93357 (* 1 = 6.93357 loss)
I0124 12:52:22.251147 27399 sgd_solver.cpp:106] Iteration 1780, lr = 0.01
I0124 12:52:29.663193 27399 solver.cpp:237] Iteration 1800, loss = 6.91622
I0124 12:52:29.663226 27399 solver.cpp:253]     Train net output #0: loss = 6.9219 (* 1 = 6.9219 loss)
I0124 12:52:29.663235 27399 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I0124 12:52:37.071218 27399 solver.cpp:237] Iteration 1820, loss = 6.91918
I0124 12:52:37.071264 27399 solver.cpp:253]     Train net output #0: loss = 6.90006 (* 1 = 6.90006 loss)
I0124 12:52:37.071274 27399 sgd_solver.cpp:106] Iteration 1820, lr = 0.01
I0124 12:52:44.628139 27399 solver.cpp:237] Iteration 1840, loss = 6.91641
I0124 12:52:44.628621 27399 solver.cpp:253]     Train net output #0: loss = 6.91513 (* 1 = 6.91513 loss)
I0124 12:52:44.628631 27399 sgd_solver.cpp:106] Iteration 1840, lr = 0.01
I0124 12:52:52.188004 27399 solver.cpp:237] Iteration 1860, loss = 6.91403
I0124 12:52:52.188113 27399 solver.cpp:253]     Train net output #0: loss = 6.91146 (* 1 = 6.91146 loss)
I0124 12:52:52.188150 27399 sgd_solver.cpp:106] Iteration 1860, lr = 0.01
I0124 12:52:59.732697 27399 solver.cpp:237] Iteration 1880, loss = 6.91659
I0124 12:52:59.732736 27399 solver.cpp:253]     Train net output #0: loss = 6.91016 (* 1 = 6.91016 loss)
I0124 12:52:59.732745 27399 sgd_solver.cpp:106] Iteration 1880, lr = 0.01
I0124 12:53:07.207176 27399 solver.cpp:237] Iteration 1900, loss = 6.91773
I0124 12:53:07.207212 27399 solver.cpp:253]     Train net output #0: loss = 6.91987 (* 1 = 6.91987 loss)
I0124 12:53:07.207222 27399 sgd_solver.cpp:106] Iteration 1900, lr = 0.01
I0124 12:53:14.650807 27399 solver.cpp:237] Iteration 1920, loss = 6.91815
I0124 12:53:14.650895 27399 solver.cpp:253]     Train net output #0: loss = 6.93269 (* 1 = 6.93269 loss)
I0124 12:53:14.650986 27399 sgd_solver.cpp:106] Iteration 1920, lr = 0.01
I0124 12:53:22.161767 27399 solver.cpp:237] Iteration 1940, loss = 6.9164
I0124 12:53:22.161803 27399 solver.cpp:253]     Train net output #0: loss = 6.92657 (* 1 = 6.92657 loss)
I0124 12:53:22.161813 27399 sgd_solver.cpp:106] Iteration 1940, lr = 0.01
I0124 12:53:29.578938 27399 solver.cpp:237] Iteration 1960, loss = 6.91445
I0124 12:53:29.578974 27399 solver.cpp:253]     Train net output #0: loss = 6.90871 (* 1 = 6.90871 loss)
I0124 12:53:29.578985 27399 sgd_solver.cpp:106] Iteration 1960, lr = 0.01
I0124 12:53:37.038071 27399 solver.cpp:237] Iteration 1980, loss = 6.91564
I0124 12:53:37.038106 27399 solver.cpp:253]     Train net output #0: loss = 6.92905 (* 1 = 6.92905 loss)
I0124 12:53:37.038115 27399 sgd_solver.cpp:106] Iteration 1980, lr = 0.01
I0124 12:53:44.305785 27399 solver.cpp:341] Iteration 2000, Testing net (#0)
I0124 12:53:45.611968 27399 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 12:54:58.457814 27399 solver.cpp:409]     Test net output #0: accuracy = 0.001
I0124 12:54:58.457902 27399 solver.cpp:409]     Test net output #1: loss = 6.91845 (* 1 = 6.91845 loss)
I0124 12:54:58.496983 27399 solver.cpp:237] Iteration 2000, loss = 6.91792
I0124 12:54:58.497066 27399 solver.cpp:253]     Train net output #0: loss = 6.91015 (* 1 = 6.91015 loss)
I0124 12:54:58.497087 27399 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I0124 12:55:05.308140 27399 solver.cpp:237] Iteration 2020, loss = 6.91725
I0124 12:55:05.308178 27399 solver.cpp:253]     Train net output #0: loss = 6.91605 (* 1 = 6.91605 loss)
I0124 12:55:05.308187 27399 sgd_solver.cpp:106] Iteration 2020, lr = 0.01
I0124 12:55:12.779297 27399 solver.cpp:237] Iteration 2040, loss = 6.91465
I0124 12:55:12.779335 27399 solver.cpp:253]     Train net output #0: loss = 6.91237 (* 1 = 6.91237 loss)
I0124 12:55:12.779343 27399 sgd_solver.cpp:106] Iteration 2040, lr = 0.01
I0124 12:55:20.266099 27399 solver.cpp:237] Iteration 2060, loss = 6.91528
I0124 12:55:20.266136 27399 solver.cpp:253]     Train net output #0: loss = 6.91412 (* 1 = 6.91412 loss)
I0124 12:55:20.266145 27399 sgd_solver.cpp:106] Iteration 2060, lr = 0.01
I0124 12:55:27.893896 27399 solver.cpp:237] Iteration 2080, loss = 6.91921
I0124 12:55:27.893934 27399 solver.cpp:253]     Train net output #0: loss = 6.91151 (* 1 = 6.91151 loss)
I0124 12:55:27.893942 27399 sgd_solver.cpp:106] Iteration 2080, lr = 0.01
I0124 12:55:35.568568 27399 solver.cpp:237] Iteration 2100, loss = 6.91447
I0124 12:55:35.568639 27399 solver.cpp:253]     Train net output #0: loss = 6.9065 (* 1 = 6.9065 loss)
I0124 12:55:35.568648 27399 sgd_solver.cpp:106] Iteration 2100, lr = 0.01
I0124 12:55:43.260452 27399 solver.cpp:237] Iteration 2120, loss = 6.91548
I0124 12:55:43.260488 27399 solver.cpp:253]     Train net output #0: loss = 6.91381 (* 1 = 6.91381 loss)
I0124 12:55:43.260496 27399 sgd_solver.cpp:106] Iteration 2120, lr = 0.01
I0124 12:55:50.891134 27399 solver.cpp:237] Iteration 2140, loss = 6.9136
I0124 12:55:50.891171 27399 solver.cpp:253]     Train net output #0: loss = 6.90701 (* 1 = 6.90701 loss)
I0124 12:55:50.891180 27399 sgd_solver.cpp:106] Iteration 2140, lr = 0.01
I0124 12:55:58.475700 27399 solver.cpp:237] Iteration 2160, loss = 6.91622
I0124 12:55:58.475736 27399 solver.cpp:253]     Train net output #0: loss = 6.92542 (* 1 = 6.92542 loss)
I0124 12:55:58.475744 27399 sgd_solver.cpp:106] Iteration 2160, lr = 0.01
I0124 12:56:06.007104 27399 solver.cpp:237] Iteration 2180, loss = 6.91588
I0124 12:56:06.007182 27399 solver.cpp:253]     Train net output #0: loss = 6.90533 (* 1 = 6.90533 loss)
I0124 12:56:06.007194 27399 sgd_solver.cpp:106] Iteration 2180, lr = 0.01
I0124 12:56:13.507066 27399 solver.cpp:237] Iteration 2200, loss = 6.91592
I0124 12:56:13.507300 27399 solver.cpp:253]     Train net output #0: loss = 6.90966 (* 1 = 6.90966 loss)
I0124 12:56:13.507380 27399 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I0124 12:56:21.051878 27399 solver.cpp:237] Iteration 2220, loss = 6.9156
I0124 12:56:21.051914 27399 solver.cpp:253]     Train net output #0: loss = 6.92885 (* 1 = 6.92885 loss)
I0124 12:56:21.051924 27399 sgd_solver.cpp:106] Iteration 2220, lr = 0.01
I0124 12:56:28.672718 27399 solver.cpp:237] Iteration 2240, loss = 6.91796
I0124 12:56:28.672757 27399 solver.cpp:253]     Train net output #0: loss = 6.90617 (* 1 = 6.90617 loss)
I0124 12:56:28.672766 27399 sgd_solver.cpp:106] Iteration 2240, lr = 0.01
I0124 12:56:36.312950 27399 solver.cpp:237] Iteration 2260, loss = 6.91963
I0124 12:56:36.313025 27399 solver.cpp:253]     Train net output #0: loss = 6.90961 (* 1 = 6.90961 loss)
I0124 12:56:36.313036 27399 sgd_solver.cpp:106] Iteration 2260, lr = 0.01
I0124 12:56:43.782086 27399 solver.cpp:237] Iteration 2280, loss = 6.91564
I0124 12:56:43.782135 27399 solver.cpp:253]     Train net output #0: loss = 6.92905 (* 1 = 6.92905 loss)
I0124 12:56:43.782146 27399 sgd_solver.cpp:106] Iteration 2280, lr = 0.01
I0124 12:56:51.282099 27399 solver.cpp:237] Iteration 2300, loss = 6.91379
I0124 12:56:51.282136 27399 solver.cpp:253]     Train net output #0: loss = 6.91204 (* 1 = 6.91204 loss)
I0124 12:56:51.282145 27399 sgd_solver.cpp:106] Iteration 2300, lr = 0.01
I0124 12:56:58.667198 27399 solver.cpp:237] Iteration 2320, loss = 6.91464
I0124 12:56:58.667295 27399 solver.cpp:253]     Train net output #0: loss = 6.91547 (* 1 = 6.91547 loss)
I0124 12:56:58.667316 27399 sgd_solver.cpp:106] Iteration 2320, lr = 0.01
I0124 12:57:06.334977 27399 solver.cpp:237] Iteration 2340, loss = 6.91706
I0124 12:57:06.335109 27399 solver.cpp:253]     Train net output #0: loss = 6.89634 (* 1 = 6.89634 loss)
I0124 12:57:06.335119 27399 sgd_solver.cpp:106] Iteration 2340, lr = 0.01
I0124 12:57:13.958437 27399 solver.cpp:237] Iteration 2360, loss = 6.91704
I0124 12:57:13.958477 27399 solver.cpp:253]     Train net output #0: loss = 6.91603 (* 1 = 6.91603 loss)
I0124 12:57:13.958492 27399 sgd_solver.cpp:106] Iteration 2360, lr = 0.01
I0124 12:57:21.319736 27399 solver.cpp:237] Iteration 2380, loss = 6.91643
I0124 12:57:21.319772 27399 solver.cpp:253]     Train net output #0: loss = 6.90091 (* 1 = 6.90091 loss)
I0124 12:57:21.319782 27399 sgd_solver.cpp:106] Iteration 2380, lr = 0.01
I0124 12:57:28.750123 27399 solver.cpp:237] Iteration 2400, loss = 6.91617
I0124 12:57:28.750157 27399 solver.cpp:253]     Train net output #0: loss = 6.92059 (* 1 = 6.92059 loss)
I0124 12:57:28.750164 27399 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
I0124 12:57:36.266018 27399 solver.cpp:237] Iteration 2420, loss = 6.9189
I0124 12:57:36.266054 27399 solver.cpp:253]     Train net output #0: loss = 6.92197 (* 1 = 6.92197 loss)
I0124 12:57:36.266062 27399 sgd_solver.cpp:106] Iteration 2420, lr = 0.01
I0124 12:57:43.716013 27399 solver.cpp:237] Iteration 2440, loss = 6.91466
I0124 12:57:43.716461 27399 solver.cpp:253]     Train net output #0: loss = 6.91228 (* 1 = 6.91228 loss)
I0124 12:57:43.716528 27399 sgd_solver.cpp:106] Iteration 2440, lr = 0.01
I0124 12:57:51.188917 27399 solver.cpp:237] Iteration 2460, loss = 6.9157
I0124 12:57:51.188954 27399 solver.cpp:253]     Train net output #0: loss = 6.91823 (* 1 = 6.91823 loss)
I0124 12:57:51.188964 27399 sgd_solver.cpp:106] Iteration 2460, lr = 0.01
I0124 12:57:58.657004 27399 solver.cpp:237] Iteration 2480, loss = 6.91569
I0124 12:57:58.657042 27399 solver.cpp:253]     Train net output #0: loss = 6.90266 (* 1 = 6.90266 loss)
I0124 12:57:58.657052 27399 sgd_solver.cpp:106] Iteration 2480, lr = 0.01
I0124 12:58:06.170598 27399 solver.cpp:237] Iteration 2500, loss = 6.91344
I0124 12:58:06.170635 27399 solver.cpp:253]     Train net output #0: loss = 6.90239 (* 1 = 6.90239 loss)
I0124 12:58:06.170645 27399 sgd_solver.cpp:106] Iteration 2500, lr = 0.01
I0124 12:58:13.747890 27399 solver.cpp:237] Iteration 2520, loss = 6.91696
I0124 12:58:13.747963 27399 solver.cpp:253]     Train net output #0: loss = 6.91144 (* 1 = 6.91144 loss)
I0124 12:58:13.747972 27399 sgd_solver.cpp:106] Iteration 2520, lr = 0.01
I0124 12:58:21.280325 27399 solver.cpp:237] Iteration 2540, loss = 6.91628
I0124 12:58:21.280370 27399 solver.cpp:253]     Train net output #0: loss = 6.9166 (* 1 = 6.9166 loss)
I0124 12:58:21.280380 27399 sgd_solver.cpp:106] Iteration 2540, lr = 0.01
I0124 12:58:28.873026 27399 solver.cpp:237] Iteration 2560, loss = 6.91891
I0124 12:58:28.873067 27399 solver.cpp:253]     Train net output #0: loss = 6.9311 (* 1 = 6.9311 loss)
I0124 12:58:28.873076 27399 sgd_solver.cpp:106] Iteration 2560, lr = 0.01
I0124 12:58:36.544170 27399 solver.cpp:237] Iteration 2580, loss = 6.91398
I0124 12:58:36.544209 27399 solver.cpp:253]     Train net output #0: loss = 6.92142 (* 1 = 6.92142 loss)
I0124 12:58:36.544216 27399 sgd_solver.cpp:106] Iteration 2580, lr = 0.01
I0124 12:58:44.375092 27399 solver.cpp:237] Iteration 2600, loss = 6.91716
I0124 12:58:44.375214 27399 solver.cpp:253]     Train net output #0: loss = 6.92043 (* 1 = 6.92043 loss)
I0124 12:58:44.375226 27399 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I0124 12:58:52.179432 27399 solver.cpp:237] Iteration 2620, loss = 6.91813
I0124 12:58:52.179472 27399 solver.cpp:253]     Train net output #0: loss = 6.91563 (* 1 = 6.91563 loss)
I0124 12:58:52.179481 27399 sgd_solver.cpp:106] Iteration 2620, lr = 0.01
I0124 12:58:59.831604 27399 solver.cpp:237] Iteration 2640, loss = 6.91653
I0124 12:58:59.831640 27399 solver.cpp:253]     Train net output #0: loss = 6.90541 (* 1 = 6.90541 loss)
I0124 12:58:59.831650 27399 sgd_solver.cpp:106] Iteration 2640, lr = 0.01
I0124 12:59:07.453800 27399 solver.cpp:237] Iteration 2660, loss = 6.91601
I0124 12:59:07.453833 27399 solver.cpp:253]     Train net output #0: loss = 6.90531 (* 1 = 6.90531 loss)
I0124 12:59:07.453840 27399 sgd_solver.cpp:106] Iteration 2660, lr = 0.01
I0124 12:59:14.854598 27399 solver.cpp:237] Iteration 2680, loss = 6.91746
I0124 12:59:14.854943 27399 solver.cpp:253]     Train net output #0: loss = 6.91314 (* 1 = 6.91314 loss)
I0124 12:59:14.855031 27399 sgd_solver.cpp:106] Iteration 2680, lr = 0.01
I0124 12:59:22.310367 27399 solver.cpp:237] Iteration 2700, loss = 6.91713
I0124 12:59:22.310406 27399 solver.cpp:253]     Train net output #0: loss = 6.90226 (* 1 = 6.90226 loss)
I0124 12:59:22.310415 27399 sgd_solver.cpp:106] Iteration 2700, lr = 0.01
I0124 12:59:29.914934 27399 solver.cpp:237] Iteration 2720, loss = 6.91562
I0124 12:59:29.914971 27399 solver.cpp:253]     Train net output #0: loss = 6.9072 (* 1 = 6.9072 loss)
I0124 12:59:29.914979 27399 sgd_solver.cpp:106] Iteration 2720, lr = 0.01
I0124 12:59:37.481653 27399 solver.cpp:237] Iteration 2740, loss = 6.91915
I0124 12:59:37.481689 27399 solver.cpp:253]     Train net output #0: loss = 6.92151 (* 1 = 6.92151 loss)
I0124 12:59:37.481703 27399 sgd_solver.cpp:106] Iteration 2740, lr = 0.01
I0124 12:59:45.064234 27399 solver.cpp:237] Iteration 2760, loss = 6.9177
I0124 12:59:45.064373 27399 solver.cpp:253]     Train net output #0: loss = 6.93523 (* 1 = 6.93523 loss)
I0124 12:59:45.064401 27399 sgd_solver.cpp:106] Iteration 2760, lr = 0.01
I0124 12:59:52.634445 27399 solver.cpp:237] Iteration 2780, loss = 6.91637
I0124 12:59:52.634484 27399 solver.cpp:253]     Train net output #0: loss = 6.9226 (* 1 = 6.9226 loss)
I0124 12:59:52.634492 27399 sgd_solver.cpp:106] Iteration 2780, lr = 0.01
I0124 13:00:00.094071 27399 solver.cpp:237] Iteration 2800, loss = 6.91574
I0124 13:00:00.094108 27399 solver.cpp:253]     Train net output #0: loss = 6.90791 (* 1 = 6.90791 loss)
I0124 13:00:00.094118 27399 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I0124 13:00:03.013341 27399 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 13:00:07.462710 27399 solver.cpp:237] Iteration 2820, loss = 6.91226
I0124 13:00:07.462749 27399 solver.cpp:253]     Train net output #0: loss = 6.92416 (* 1 = 6.92416 loss)
I0124 13:00:07.462759 27399 sgd_solver.cpp:106] Iteration 2820, lr = 0.01
I0124 13:00:14.911582 27399 solver.cpp:237] Iteration 2840, loss = 6.91802
I0124 13:00:14.911619 27399 solver.cpp:253]     Train net output #0: loss = 6.91536 (* 1 = 6.91536 loss)
I0124 13:00:14.911628 27399 sgd_solver.cpp:106] Iteration 2840, lr = 0.01
I0124 13:00:22.414556 27399 solver.cpp:237] Iteration 2860, loss = 6.91328
I0124 13:00:22.414636 27399 solver.cpp:253]     Train net output #0: loss = 6.92899 (* 1 = 6.92899 loss)
I0124 13:00:22.414646 27399 sgd_solver.cpp:106] Iteration 2860, lr = 0.01
I0124 13:00:29.967118 27399 solver.cpp:237] Iteration 2880, loss = 6.91236
I0124 13:00:29.967339 27399 solver.cpp:253]     Train net output #0: loss = 6.90094 (* 1 = 6.90094 loss)
I0124 13:00:29.967429 27399 sgd_solver.cpp:106] Iteration 2880, lr = 0.01
I0124 13:00:37.570314 27399 solver.cpp:237] Iteration 2900, loss = 6.91644
I0124 13:00:37.570350 27399 solver.cpp:253]     Train net output #0: loss = 6.92415 (* 1 = 6.92415 loss)
I0124 13:00:37.570359 27399 sgd_solver.cpp:106] Iteration 2900, lr = 0.01
I0124 13:00:45.004298 27399 solver.cpp:237] Iteration 2920, loss = 6.91868
I0124 13:00:45.004336 27399 solver.cpp:253]     Train net output #0: loss = 6.90865 (* 1 = 6.90865 loss)
I0124 13:00:45.004346 27399 sgd_solver.cpp:106] Iteration 2920, lr = 0.01
I0124 13:00:52.481528 27399 solver.cpp:237] Iteration 2940, loss = 6.91516
I0124 13:00:52.481621 27399 solver.cpp:253]     Train net output #0: loss = 6.91149 (* 1 = 6.91149 loss)
I0124 13:00:52.481703 27399 sgd_solver.cpp:106] Iteration 2940, lr = 0.01
I0124 13:00:59.956660 27399 solver.cpp:237] Iteration 2960, loss = 6.91607
I0124 13:00:59.956698 27399 solver.cpp:253]     Train net output #0: loss = 6.9217 (* 1 = 6.9217 loss)
I0124 13:00:59.956708 27399 sgd_solver.cpp:106] Iteration 2960, lr = 0.01
I0124 13:01:07.416595 27399 solver.cpp:237] Iteration 2980, loss = 6.91455
I0124 13:01:07.416640 27399 solver.cpp:253]     Train net output #0: loss = 6.90802 (* 1 = 6.90802 loss)
I0124 13:01:07.416648 27399 sgd_solver.cpp:106] Iteration 2980, lr = 0.01
I0124 13:01:14.806637 27399 solver.cpp:237] Iteration 3000, loss = 6.91544
I0124 13:01:14.806673 27399 solver.cpp:253]     Train net output #0: loss = 6.90464 (* 1 = 6.90464 loss)
I0124 13:01:14.806679 27399 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I0124 13:01:22.225422 27399 solver.cpp:237] Iteration 3020, loss = 6.91897
I0124 13:01:22.225457 27399 solver.cpp:253]     Train net output #0: loss = 6.92007 (* 1 = 6.92007 loss)
I0124 13:01:22.225466 27399 sgd_solver.cpp:106] Iteration 3020, lr = 0.01
I0124 13:01:29.618484 27399 solver.cpp:237] Iteration 3040, loss = 6.91543
I0124 13:01:29.618667 27399 solver.cpp:253]     Train net output #0: loss = 6.90407 (* 1 = 6.90407 loss)
I0124 13:01:29.618676 27399 sgd_solver.cpp:106] Iteration 3040, lr = 0.01
I0124 13:01:37.012121 27399 solver.cpp:237] Iteration 3060, loss = 6.91754
I0124 13:01:37.012321 27399 solver.cpp:253]     Train net output #0: loss = 6.9237 (* 1 = 6.9237 loss)
I0124 13:01:37.012405 27399 sgd_solver.cpp:106] Iteration 3060, lr = 0.01
I0124 13:01:44.513042 27399 solver.cpp:237] Iteration 3080, loss = 6.92075
I0124 13:01:44.513080 27399 solver.cpp:253]     Train net output #0: loss = 6.91333 (* 1 = 6.91333 loss)
I0124 13:01:44.513092 27399 sgd_solver.cpp:106] Iteration 3080, lr = 0.01
I0124 13:01:51.894642 27399 solver.cpp:237] Iteration 3100, loss = 6.91282
I0124 13:01:51.894678 27399 solver.cpp:253]     Train net output #0: loss = 6.91605 (* 1 = 6.91605 loss)
I0124 13:01:51.894686 27399 sgd_solver.cpp:106] Iteration 3100, lr = 0.01
I0124 13:01:59.312033 27399 solver.cpp:237] Iteration 3120, loss = 6.91202
I0124 13:01:59.312072 27399 solver.cpp:253]     Train net output #0: loss = 6.91279 (* 1 = 6.91279 loss)
I0124 13:01:59.312080 27399 sgd_solver.cpp:106] Iteration 3120, lr = 0.01
I0124 13:02:06.682816 27399 solver.cpp:237] Iteration 3140, loss = 6.92097
I0124 13:02:06.683087 27399 solver.cpp:253]     Train net output #0: loss = 6.92283 (* 1 = 6.92283 loss)
I0124 13:02:06.683118 27399 sgd_solver.cpp:106] Iteration 3140, lr = 0.01
I0124 13:02:14.102020 27399 solver.cpp:237] Iteration 3160, loss = 6.91656
I0124 13:02:14.102052 27399 solver.cpp:253]     Train net output #0: loss = 6.91492 (* 1 = 6.91492 loss)
I0124 13:02:14.102063 27399 sgd_solver.cpp:106] Iteration 3160, lr = 0.01
I0124 13:02:21.635193 27399 solver.cpp:237] Iteration 3180, loss = 6.914
I0124 13:02:21.635231 27399 solver.cpp:253]     Train net output #0: loss = 6.91513 (* 1 = 6.91513 loss)
I0124 13:02:21.635237 27399 sgd_solver.cpp:106] Iteration 3180, lr = 0.01
I0124 13:02:29.173144 27399 solver.cpp:237] Iteration 3200, loss = 6.91458
I0124 13:02:29.173239 27399 solver.cpp:253]     Train net output #0: loss = 6.914 (* 1 = 6.914 loss)
I0124 13:02:29.173261 27399 sgd_solver.cpp:106] Iteration 3200, lr = 0.01
I0124 13:02:36.643874 27399 solver.cpp:237] Iteration 3220, loss = 6.91332
I0124 13:02:36.643913 27399 solver.cpp:253]     Train net output #0: loss = 6.89438 (* 1 = 6.89438 loss)
I0124 13:02:36.643923 27399 sgd_solver.cpp:106] Iteration 3220, lr = 0.01
I0124 13:02:44.161036 27399 solver.cpp:237] Iteration 3240, loss = 6.91816
I0124 13:02:44.161156 27399 solver.cpp:253]     Train net output #0: loss = 6.92742 (* 1 = 6.92742 loss)
I0124 13:02:44.161167 27399 sgd_solver.cpp:106] Iteration 3240, lr = 0.01
I0124 13:02:51.929918 27399 solver.cpp:237] Iteration 3260, loss = 6.91747
I0124 13:02:51.929955 27399 solver.cpp:253]     Train net output #0: loss = 6.92174 (* 1 = 6.92174 loss)
I0124 13:02:51.929962 27399 sgd_solver.cpp:106] Iteration 3260, lr = 0.01
I0124 13:02:59.809695 27399 solver.cpp:237] Iteration 3280, loss = 6.91734
I0124 13:02:59.809738 27399 solver.cpp:253]     Train net output #0: loss = 6.90853 (* 1 = 6.90853 loss)
I0124 13:02:59.809748 27399 sgd_solver.cpp:106] Iteration 3280, lr = 0.01
I0124 13:03:07.458490 27399 solver.cpp:237] Iteration 3300, loss = 6.91666
I0124 13:03:07.458526 27399 solver.cpp:253]     Train net output #0: loss = 6.90557 (* 1 = 6.90557 loss)
I0124 13:03:07.458534 27399 sgd_solver.cpp:106] Iteration 3300, lr = 0.01
I0124 13:03:14.922376 27399 solver.cpp:237] Iteration 3320, loss = 6.91434
I0124 13:03:14.922580 27399 solver.cpp:253]     Train net output #0: loss = 6.91129 (* 1 = 6.91129 loss)
I0124 13:03:14.922607 27399 sgd_solver.cpp:106] Iteration 3320, lr = 0.01
I0124 13:03:22.360010 27399 solver.cpp:237] Iteration 3340, loss = 6.91534
I0124 13:03:22.360047 27399 solver.cpp:253]     Train net output #0: loss = 6.89089 (* 1 = 6.89089 loss)
I0124 13:03:22.360056 27399 sgd_solver.cpp:106] Iteration 3340, lr = 0.01
I0124 13:03:29.737218 27399 solver.cpp:237] Iteration 3360, loss = 6.91521
I0124 13:03:29.737334 27399 solver.cpp:253]     Train net output #0: loss = 6.90044 (* 1 = 6.90044 loss)
I0124 13:03:29.737367 27399 sgd_solver.cpp:106] Iteration 3360, lr = 0.01
I0124 13:03:37.154639 27399 solver.cpp:237] Iteration 3380, loss = 6.91614
I0124 13:03:37.154677 27399 solver.cpp:253]     Train net output #0: loss = 6.9161 (* 1 = 6.9161 loss)
I0124 13:03:37.154687 27399 sgd_solver.cpp:106] Iteration 3380, lr = 0.01
I0124 13:03:44.597101 27399 solver.cpp:237] Iteration 3400, loss = 6.91441
I0124 13:03:44.597139 27399 solver.cpp:253]     Train net output #0: loss = 6.91267 (* 1 = 6.91267 loss)
I0124 13:03:44.597149 27399 sgd_solver.cpp:106] Iteration 3400, lr = 0.01
I0124 13:03:52.149503 27399 solver.cpp:237] Iteration 3420, loss = 6.91695
I0124 13:03:52.149580 27399 solver.cpp:253]     Train net output #0: loss = 6.92817 (* 1 = 6.92817 loss)
I0124 13:03:52.149662 27399 sgd_solver.cpp:106] Iteration 3420, lr = 0.01
I0124 13:03:59.599694 27399 solver.cpp:237] Iteration 3440, loss = 6.91336
I0124 13:03:59.599733 27399 solver.cpp:253]     Train net output #0: loss = 6.9261 (* 1 = 6.9261 loss)
I0124 13:03:59.599743 27399 sgd_solver.cpp:106] Iteration 3440, lr = 0.01
I0124 13:04:07.059038 27399 solver.cpp:237] Iteration 3460, loss = 6.91559
I0124 13:04:07.059074 27399 solver.cpp:253]     Train net output #0: loss = 6.91798 (* 1 = 6.91798 loss)
I0124 13:04:07.059082 27399 sgd_solver.cpp:106] Iteration 3460, lr = 0.01
I0124 13:04:14.395246 27399 solver.cpp:237] Iteration 3480, loss = 6.91868
I0124 13:04:14.395285 27399 solver.cpp:253]     Train net output #0: loss = 6.92213 (* 1 = 6.92213 loss)
I0124 13:04:14.395297 27399 sgd_solver.cpp:106] Iteration 3480, lr = 0.01
I0124 13:04:21.895449 27399 solver.cpp:237] Iteration 3500, loss = 6.91756
I0124 13:04:21.895485 27399 solver.cpp:253]     Train net output #0: loss = 6.91691 (* 1 = 6.91691 loss)
I0124 13:04:21.895496 27399 sgd_solver.cpp:106] Iteration 3500, lr = 0.01
I0124 13:04:29.436601 27399 solver.cpp:237] Iteration 3520, loss = 6.91572
I0124 13:04:29.436743 27399 solver.cpp:253]     Train net output #0: loss = 6.9278 (* 1 = 6.9278 loss)
I0124 13:04:29.436751 27399 sgd_solver.cpp:106] Iteration 3520, lr = 0.01
I0124 13:04:37.061405 27399 solver.cpp:237] Iteration 3540, loss = 6.91251
I0124 13:04:37.061507 27399 solver.cpp:253]     Train net output #0: loss = 6.90055 (* 1 = 6.90055 loss)
I0124 13:04:37.061542 27399 sgd_solver.cpp:106] Iteration 3540, lr = 0.01
I0124 13:04:44.480312 27399 solver.cpp:237] Iteration 3560, loss = 6.91695
I0124 13:04:44.480348 27399 solver.cpp:253]     Train net output #0: loss = 6.91987 (* 1 = 6.91987 loss)
I0124 13:04:44.480355 27399 sgd_solver.cpp:106] Iteration 3560, lr = 0.01
I0124 13:04:51.948809 27399 solver.cpp:237] Iteration 3580, loss = 6.91702
I0124 13:04:51.948846 27399 solver.cpp:253]     Train net output #0: loss = 6.92603 (* 1 = 6.92603 loss)
I0124 13:04:51.948856 27399 sgd_solver.cpp:106] Iteration 3580, lr = 0.01
I0124 13:04:59.558805 27399 solver.cpp:237] Iteration 3600, loss = 6.9187
I0124 13:04:59.558928 27399 solver.cpp:253]     Train net output #0: loss = 6.92961 (* 1 = 6.92961 loss)
I0124 13:04:59.558939 27399 sgd_solver.cpp:106] Iteration 3600, lr = 0.01
I0124 13:05:07.292282 27399 solver.cpp:237] Iteration 3620, loss = 6.9157
I0124 13:05:07.292321 27399 solver.cpp:253]     Train net output #0: loss = 6.9106 (* 1 = 6.9106 loss)
I0124 13:05:07.292330 27399 sgd_solver.cpp:106] Iteration 3620, lr = 0.01
I0124 13:05:14.940404 27399 solver.cpp:237] Iteration 3640, loss = 6.91838
I0124 13:05:14.940443 27399 solver.cpp:253]     Train net output #0: loss = 6.92114 (* 1 = 6.92114 loss)
I0124 13:05:14.940450 27399 sgd_solver.cpp:106] Iteration 3640, lr = 0.01
I0124 13:05:22.675801 27399 solver.cpp:237] Iteration 3660, loss = 6.91508
I0124 13:05:22.675839 27399 solver.cpp:253]     Train net output #0: loss = 6.92081 (* 1 = 6.92081 loss)
I0124 13:05:22.675848 27399 sgd_solver.cpp:106] Iteration 3660, lr = 0.01
I0124 13:05:30.496973 27399 solver.cpp:237] Iteration 3680, loss = 6.91815
I0124 13:05:30.497704 27399 solver.cpp:253]     Train net output #0: loss = 6.9348 (* 1 = 6.9348 loss)
I0124 13:05:30.497743 27399 sgd_solver.cpp:106] Iteration 3680, lr = 0.01
I0124 13:05:38.085844 27399 solver.cpp:237] Iteration 3700, loss = 6.91392
I0124 13:05:38.085942 27399 solver.cpp:253]     Train net output #0: loss = 6.92799 (* 1 = 6.92799 loss)
I0124 13:05:38.085973 27399 sgd_solver.cpp:106] Iteration 3700, lr = 0.01
I0124 13:05:45.486054 27399 solver.cpp:237] Iteration 3720, loss = 6.9133
I0124 13:05:45.486094 27399 solver.cpp:253]     Train net output #0: loss = 6.90887 (* 1 = 6.90887 loss)
I0124 13:05:45.486105 27399 sgd_solver.cpp:106] Iteration 3720, lr = 0.01
I0124 13:05:53.041676 27399 solver.cpp:237] Iteration 3740, loss = 6.91619
I0124 13:05:53.041715 27399 solver.cpp:253]     Train net output #0: loss = 6.92549 (* 1 = 6.92549 loss)
I0124 13:05:53.041721 27399 sgd_solver.cpp:106] Iteration 3740, lr = 0.01
I0124 13:06:00.540611 27399 solver.cpp:237] Iteration 3760, loss = 6.9113
I0124 13:06:00.540686 27399 solver.cpp:253]     Train net output #0: loss = 6.91625 (* 1 = 6.91625 loss)
I0124 13:06:00.540694 27399 sgd_solver.cpp:106] Iteration 3760, lr = 0.01
I0124 13:06:08.057060 27399 solver.cpp:237] Iteration 3780, loss = 6.9125
I0124 13:06:08.057097 27399 solver.cpp:253]     Train net output #0: loss = 6.91938 (* 1 = 6.91938 loss)
I0124 13:06:08.057104 27399 sgd_solver.cpp:106] Iteration 3780, lr = 0.01
I0124 13:06:15.574060 27399 solver.cpp:237] Iteration 3800, loss = 6.91988
I0124 13:06:15.574100 27399 solver.cpp:253]     Train net output #0: loss = 6.91616 (* 1 = 6.91616 loss)
I0124 13:06:15.574110 27399 sgd_solver.cpp:106] Iteration 3800, lr = 0.01
I0124 13:06:18.636618 27399 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 13:06:23.201062 27399 solver.cpp:237] Iteration 3820, loss = 6.9164
I0124 13:06:23.201160 27399 solver.cpp:253]     Train net output #0: loss = 6.9002 (* 1 = 6.9002 loss)
I0124 13:06:23.201190 27399 sgd_solver.cpp:106] Iteration 3820, lr = 0.01
I0124 13:06:30.746388 27399 solver.cpp:237] Iteration 3840, loss = 6.92034
I0124 13:06:30.746459 27399 solver.cpp:253]     Train net output #0: loss = 6.93323 (* 1 = 6.93323 loss)
I0124 13:06:30.746533 27399 sgd_solver.cpp:106] Iteration 3840, lr = 0.01
I0124 13:06:38.185925 27399 solver.cpp:237] Iteration 3860, loss = 6.9158
I0124 13:06:38.185963 27399 solver.cpp:253]     Train net output #0: loss = 6.9162 (* 1 = 6.9162 loss)
I0124 13:06:38.185973 27399 sgd_solver.cpp:106] Iteration 3860, lr = 0.01
I0124 13:06:45.707979 27399 solver.cpp:237] Iteration 3880, loss = 6.91572
I0124 13:06:45.708017 27399 solver.cpp:253]     Train net output #0: loss = 6.90266 (* 1 = 6.90266 loss)
I0124 13:06:45.708099 27399 sgd_solver.cpp:106] Iteration 3880, lr = 0.01
I0124 13:06:53.289403 27399 solver.cpp:237] Iteration 3900, loss = 6.91474
I0124 13:06:53.289441 27399 solver.cpp:253]     Train net output #0: loss = 6.91872 (* 1 = 6.91872 loss)
I0124 13:06:53.289450 27399 sgd_solver.cpp:106] Iteration 3900, lr = 0.01
I0124 13:07:00.845755 27399 solver.cpp:237] Iteration 3920, loss = 6.91954
I0124 13:07:00.845877 27399 solver.cpp:253]     Train net output #0: loss = 6.93421 (* 1 = 6.93421 loss)
I0124 13:07:00.845890 27399 sgd_solver.cpp:106] Iteration 3920, lr = 0.01
I0124 13:07:08.303376 27399 solver.cpp:237] Iteration 3940, loss = 6.91362
I0124 13:07:08.303412 27399 solver.cpp:253]     Train net output #0: loss = 6.89796 (* 1 = 6.89796 loss)
I0124 13:07:08.303419 27399 sgd_solver.cpp:106] Iteration 3940, lr = 0.01
I0124 13:07:15.802943 27399 solver.cpp:237] Iteration 3960, loss = 6.91822
I0124 13:07:15.802979 27399 solver.cpp:253]     Train net output #0: loss = 6.90606 (* 1 = 6.90606 loss)
I0124 13:07:15.802988 27399 sgd_solver.cpp:106] Iteration 3960, lr = 0.01
I0124 13:07:23.188535 27399 solver.cpp:237] Iteration 3980, loss = 6.91778
I0124 13:07:23.188576 27399 solver.cpp:253]     Train net output #0: loss = 6.91601 (* 1 = 6.91601 loss)
I0124 13:07:23.188586 27399 sgd_solver.cpp:106] Iteration 3980, lr = 0.01
I0124 13:07:30.327103 27399 solver.cpp:341] Iteration 4000, Testing net (#0)
I0124 13:08:45.497620 27399 solver.cpp:409]     Test net output #0: accuracy = 0.001
I0124 13:08:45.497807 27399 solver.cpp:409]     Test net output #1: loss = 6.91849 (* 1 = 6.91849 loss)
I0124 13:08:45.536306 27399 solver.cpp:237] Iteration 4000, loss = 6.91772
I0124 13:08:45.536340 27399 solver.cpp:253]     Train net output #0: loss = 6.92422 (* 1 = 6.92422 loss)
I0124 13:08:45.536347 27399 sgd_solver.cpp:106] Iteration 4000, lr = 0.01
I0124 13:08:52.857173 27399 solver.cpp:237] Iteration 4020, loss = 6.91823
I0124 13:08:52.857213 27399 solver.cpp:253]     Train net output #0: loss = 6.91242 (* 1 = 6.91242 loss)
I0124 13:08:52.857223 27399 sgd_solver.cpp:106] Iteration 4020, lr = 0.01
I0124 13:09:00.370342 27399 solver.cpp:237] Iteration 4040, loss = 6.91651
I0124 13:09:00.370384 27399 solver.cpp:253]     Train net output #0: loss = 6.91345 (* 1 = 6.91345 loss)
I0124 13:09:00.370394 27399 sgd_solver.cpp:106] Iteration 4040, lr = 0.01
I0124 13:09:08.004561 27399 solver.cpp:237] Iteration 4060, loss = 6.9168
I0124 13:09:08.004602 27399 solver.cpp:253]     Train net output #0: loss = 6.92359 (* 1 = 6.92359 loss)
I0124 13:09:08.004616 27399 sgd_solver.cpp:106] Iteration 4060, lr = 0.01
I0124 13:09:15.727010 27399 solver.cpp:237] Iteration 4080, loss = 6.91669
I0124 13:09:15.727087 27399 solver.cpp:253]     Train net output #0: loss = 6.92302 (* 1 = 6.92302 loss)
I0124 13:09:15.727097 27399 sgd_solver.cpp:106] Iteration 4080, lr = 0.01
I0124 13:09:23.243405 27399 solver.cpp:237] Iteration 4100, loss = 6.91811
I0124 13:09:23.243443 27399 solver.cpp:253]     Train net output #0: loss = 6.91005 (* 1 = 6.91005 loss)
I0124 13:09:23.243453 27399 sgd_solver.cpp:106] Iteration 4100, lr = 0.01
I0124 13:09:30.659837 27399 solver.cpp:237] Iteration 4120, loss = 6.9147
I0124 13:09:30.659939 27399 solver.cpp:253]     Train net output #0: loss = 6.91383 (* 1 = 6.91383 loss)
I0124 13:09:30.659970 27399 sgd_solver.cpp:106] Iteration 4120, lr = 0.01
I0124 13:09:38.110411 27399 solver.cpp:237] Iteration 4140, loss = 6.91328
I0124 13:09:38.110450 27399 solver.cpp:253]     Train net output #0: loss = 6.91195 (* 1 = 6.91195 loss)
I0124 13:09:38.110460 27399 sgd_solver.cpp:106] Iteration 4140, lr = 0.01
I0124 13:09:45.538715 27399 solver.cpp:237] Iteration 4160, loss = 6.91725
I0124 13:09:45.538751 27399 solver.cpp:253]     Train net output #0: loss = 6.92308 (* 1 = 6.92308 loss)
I0124 13:09:45.538761 27399 sgd_solver.cpp:106] Iteration 4160, lr = 0.01
I0124 13:09:52.956025 27399 solver.cpp:237] Iteration 4180, loss = 6.91931
I0124 13:09:52.956117 27399 solver.cpp:253]     Train net output #0: loss = 6.91304 (* 1 = 6.91304 loss)
I0124 13:09:52.956126 27399 sgd_solver.cpp:106] Iteration 4180, lr = 0.01
I0124 13:10:00.399458 27399 solver.cpp:237] Iteration 4200, loss = 6.91863
I0124 13:10:00.399497 27399 solver.cpp:253]     Train net output #0: loss = 6.90965 (* 1 = 6.90965 loss)
I0124 13:10:00.399507 27399 sgd_solver.cpp:106] Iteration 4200, lr = 0.01
I0124 13:10:07.936293 27399 solver.cpp:237] Iteration 4220, loss = 6.91603
I0124 13:10:07.936328 27399 solver.cpp:253]     Train net output #0: loss = 6.9168 (* 1 = 6.9168 loss)
I0124 13:10:07.936400 27399 sgd_solver.cpp:106] Iteration 4220, lr = 0.01
I0124 13:10:15.380942 27399 solver.cpp:237] Iteration 4240, loss = 6.9152
I0124 13:10:15.380980 27399 solver.cpp:253]     Train net output #0: loss = 6.91839 (* 1 = 6.91839 loss)
I0124 13:10:15.380986 27399 sgd_solver.cpp:106] Iteration 4240, lr = 0.01
I0124 13:10:22.991489 27399 solver.cpp:237] Iteration 4260, loss = 6.9196
I0124 13:10:22.991583 27399 solver.cpp:253]     Train net output #0: loss = 6.92398 (* 1 = 6.92398 loss)
I0124 13:10:22.991593 27399 sgd_solver.cpp:106] Iteration 4260, lr = 0.01
I0124 13:10:30.669163 27399 solver.cpp:237] Iteration 4280, loss = 6.91568
I0124 13:10:30.669200 27399 solver.cpp:253]     Train net output #0: loss = 6.90679 (* 1 = 6.90679 loss)
I0124 13:10:30.669209 27399 sgd_solver.cpp:106] Iteration 4280, lr = 0.01
I0124 13:10:38.147753 27399 solver.cpp:237] Iteration 4300, loss = 6.91699
I0124 13:10:38.147790 27399 solver.cpp:253]     Train net output #0: loss = 6.92091 (* 1 = 6.92091 loss)
I0124 13:10:38.147799 27399 sgd_solver.cpp:106] Iteration 4300, lr = 0.01
I0124 13:10:45.616508 27399 solver.cpp:237] Iteration 4320, loss = 6.91466
I0124 13:10:45.616546 27399 solver.cpp:253]     Train net output #0: loss = 6.90932 (* 1 = 6.90932 loss)
I0124 13:10:45.616555 27399 sgd_solver.cpp:106] Iteration 4320, lr = 0.01
I0124 13:10:53.107872 27399 solver.cpp:237] Iteration 4340, loss = 6.91731
I0124 13:10:53.108096 27399 solver.cpp:253]     Train net output #0: loss = 6.92666 (* 1 = 6.92666 loss)
I0124 13:10:53.108116 27399 sgd_solver.cpp:106] Iteration 4340, lr = 0.01
I0124 13:11:00.775123 27399 solver.cpp:237] Iteration 4360, loss = 6.91695
I0124 13:11:00.775172 27399 solver.cpp:253]     Train net output #0: loss = 6.91691 (* 1 = 6.91691 loss)
I0124 13:11:00.775179 27399 sgd_solver.cpp:106] Iteration 4360, lr = 0.01
I0124 13:11:08.345307 27399 solver.cpp:237] Iteration 4380, loss = 6.91893
I0124 13:11:08.345341 27399 solver.cpp:253]     Train net output #0: loss = 6.93867 (* 1 = 6.93867 loss)
I0124 13:11:08.345348 27399 sgd_solver.cpp:106] Iteration 4380, lr = 0.01
I0124 13:11:15.821589 27399 solver.cpp:237] Iteration 4400, loss = 6.91369
I0124 13:11:15.821622 27399 solver.cpp:253]     Train net output #0: loss = 6.91075 (* 1 = 6.91075 loss)
I0124 13:11:15.821630 27399 sgd_solver.cpp:106] Iteration 4400, lr = 0.01
I0124 13:11:23.308599 27399 solver.cpp:237] Iteration 4420, loss = 6.91544
I0124 13:11:23.308707 27399 solver.cpp:253]     Train net output #0: loss = 6.91147 (* 1 = 6.91147 loss)
I0124 13:11:23.308717 27399 sgd_solver.cpp:106] Iteration 4420, lr = 0.01
I0124 13:11:30.820482 27399 solver.cpp:237] Iteration 4440, loss = 6.91887
I0124 13:11:30.820516 27399 solver.cpp:253]     Train net output #0: loss = 6.91465 (* 1 = 6.91465 loss)
I0124 13:11:30.820523 27399 sgd_solver.cpp:106] Iteration 4440, lr = 0.01
I0124 13:11:38.439234 27399 solver.cpp:237] Iteration 4460, loss = 6.91537
I0124 13:11:38.439272 27399 solver.cpp:253]     Train net output #0: loss = 6.9294 (* 1 = 6.9294 loss)
I0124 13:11:38.439280 27399 sgd_solver.cpp:106] Iteration 4460, lr = 0.01
I0124 13:11:46.091811 27399 solver.cpp:237] Iteration 4480, loss = 6.91578
I0124 13:11:46.091856 27399 solver.cpp:253]     Train net output #0: loss = 6.89099 (* 1 = 6.89099 loss)
I0124 13:11:46.091866 27399 sgd_solver.cpp:106] Iteration 4480, lr = 0.01
I0124 13:11:53.782644 27399 solver.cpp:237] Iteration 4500, loss = 6.91141
I0124 13:11:53.782771 27399 solver.cpp:253]     Train net output #0: loss = 6.9244 (* 1 = 6.9244 loss)
I0124 13:11:53.782781 27399 sgd_solver.cpp:106] Iteration 4500, lr = 0.01
I0124 13:12:01.431581 27399 solver.cpp:237] Iteration 4520, loss = 6.91744
I0124 13:12:01.431617 27399 solver.cpp:253]     Train net output #0: loss = 6.90648 (* 1 = 6.90648 loss)
I0124 13:12:01.431624 27399 sgd_solver.cpp:106] Iteration 4520, lr = 0.01
I0124 13:12:09.082402 27399 solver.cpp:237] Iteration 4540, loss = 6.91944
I0124 13:12:09.082442 27399 solver.cpp:253]     Train net output #0: loss = 6.91642 (* 1 = 6.91642 loss)
I0124 13:12:09.082451 27399 sgd_solver.cpp:106] Iteration 4540, lr = 0.01
I0124 13:12:16.828923 27399 solver.cpp:237] Iteration 4560, loss = 6.91411
I0124 13:12:16.828965 27399 solver.cpp:253]     Train net output #0: loss = 6.9345 (* 1 = 6.9345 loss)
I0124 13:12:16.828974 27399 sgd_solver.cpp:106] Iteration 4560, lr = 0.01
I0124 13:12:24.363852 27399 solver.cpp:237] Iteration 4580, loss = 6.9184
I0124 13:12:24.363931 27399 solver.cpp:253]     Train net output #0: loss = 6.9077 (* 1 = 6.9077 loss)
I0124 13:12:24.364017 27399 sgd_solver.cpp:106] Iteration 4580, lr = 0.01
I0124 13:12:32.294605 27399 solver.cpp:237] Iteration 4600, loss = 6.91384
I0124 13:12:32.294641 27399 solver.cpp:253]     Train net output #0: loss = 6.9077 (* 1 = 6.9077 loss)
I0124 13:12:32.294647 27399 sgd_solver.cpp:106] Iteration 4600, lr = 0.01
