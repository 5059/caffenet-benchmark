I0124 14:11:19.258646 16439 caffe.cpp:184] Using GPUs 0
I0124 14:11:20.019423 16439 solver.cpp:47] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.01
display: 20
max_iter: 320000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "snapshots/caffenet128_lsuv_adam"
solver_mode: GPU
device_id: 0
net_param {
  name: "CaffeNet"
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TRAIN
    }
    transform_param {
      scale: 0.04
      mirror: true
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 124
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/local/temporary/imagenet_clc_orig/ilsvrc12_train_shuffle_lmdb"
      batch_size: 128
      backend: LMDB
    }
  }
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TEST
    }
    transform_param {
      scale: 0.04
      mirror: false
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 123
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/local/temporary/imagenet_clc_orig/ilsvrc12_val_lmdb"
      batch_size: 50
      backend: LMDB
    }
  }
  layer {
    name: "conv1"
    type: "Convolution"
    bottom: "data"
    top: "conv1"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 96
      kernel_size: 11
      stride: 4
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu1"
    type: "ReLU"
    bottom: "conv1"
    top: "relu1"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "relu1"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv2"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 2
      kernel_size: 5
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu2"
    type: "ReLU"
    bottom: "conv2"
    top: "relu2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "relu2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv3"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu3"
    type: "ReLU"
    bottom: "conv3"
    top: "relu3"
  }
  layer {
    name: "conv4"
    type: "Convolution"
    bottom: "relu3"
    top: "conv4"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu4"
    type: "ReLU"
    bottom: "conv4"
    top: "relu4"
  }
  layer {
    name: "conv5"
    type: "Convolution"
    bottom: "relu4"
    top: "conv5"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu5"
    type: "ReLU"
    bottom: "conv5"
    top: "relu5"
  }
  layer {
    name: "pool5"
    type: "Pooling"
    bottom: "relu5"
    top: "pool5"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "pool5"
    top: "fc6"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu6"
    type: "ReLU"
    bottom: "fc6"
    top: "relu6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "relu6"
    top: "drop6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "drop6"
    top: "fc7"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu7"
    type: "ReLU"
    bottom: "fc7"
    top: "relu7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "relu7"
    top: "drop7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc8"
    type: "InnerProduct"
    bottom: "drop7"
    top: "fc8"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 1000
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "accuracy"
    type: "Accuracy"
    bottom: "fc8"
    bottom: "label"
    top: "accuracy"
    include {
      phase: TEST
    }
  }
  layer {
    name: "loss"
    type: "SoftmaxWithLoss"
    bottom: "fc8"
    bottom: "label"
    top: "loss"
  }
}
delta: 1e-09
test_initialization: false
iter_size: 1
momentum2: 0.999
type: "Adam"
I0124 14:11:20.066087 16439 solver.cpp:85] Creating training net specified in net_param.
I0124 14:11:20.066258 16439 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0124 14:11:20.066308 16439 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0124 14:11:20.066679 16439 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.04
    mirror: true
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 124
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/local/temporary/imagenet_clc_orig/ilsvrc12_train_shuffle_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 14:11:20.066903 16439 layer_factory.hpp:76] Creating layer data
I0124 14:11:20.067641 16439 net.cpp:106] Creating Layer data
I0124 14:11:20.067675 16439 net.cpp:411] data -> data
I0124 14:11:20.067736 16439 net.cpp:411] data -> label
I0124 14:11:20.069280 16529 db_lmdb.cpp:38] Opened lmdb /local/temporary/imagenet_clc_orig/ilsvrc12_train_shuffle_lmdb
I0124 14:11:20.085129 16439 data_layer.cpp:41] output data size: 128,3,128,128
I0124 14:11:20.134747 16439 net.cpp:150] Setting up data
I0124 14:11:20.134786 16439 net.cpp:157] Top shape: 128 3 128 128 (6291456)
I0124 14:11:20.134793 16439 net.cpp:157] Top shape: 128 (128)
I0124 14:11:20.134795 16439 net.cpp:165] Memory required for data: 25166336
I0124 14:11:20.134807 16439 layer_factory.hpp:76] Creating layer conv1
I0124 14:11:20.134835 16439 net.cpp:106] Creating Layer conv1
I0124 14:11:20.134842 16439 net.cpp:454] conv1 <- data
I0124 14:11:20.134860 16439 net.cpp:411] conv1 -> conv1
I0124 14:11:20.262903 16439 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 4356
I0124 14:11:20.263047 16439 net.cpp:150] Setting up conv1
I0124 14:11:20.263062 16439 net.cpp:157] Top shape: 128 96 30 30 (11059200)
I0124 14:11:20.263068 16439 net.cpp:165] Memory required for data: 69403136
I0124 14:11:20.263087 16439 layer_factory.hpp:76] Creating layer relu1
I0124 14:11:20.263100 16439 net.cpp:106] Creating Layer relu1
I0124 14:11:20.263106 16439 net.cpp:454] relu1 <- conv1
I0124 14:11:20.263113 16439 net.cpp:411] relu1 -> relu1
I0124 14:11:20.263309 16439 net.cpp:150] Setting up relu1
I0124 14:11:20.263319 16439 net.cpp:157] Top shape: 128 96 30 30 (11059200)
I0124 14:11:20.263324 16439 net.cpp:165] Memory required for data: 113639936
I0124 14:11:20.263327 16439 layer_factory.hpp:76] Creating layer pool1
I0124 14:11:20.263334 16439 net.cpp:106] Creating Layer pool1
I0124 14:11:20.263339 16439 net.cpp:454] pool1 <- relu1
I0124 14:11:20.263345 16439 net.cpp:411] pool1 -> pool1
I0124 14:11:20.263658 16439 net.cpp:150] Setting up pool1
I0124 14:11:20.263669 16439 net.cpp:157] Top shape: 128 96 15 15 (2764800)
I0124 14:11:20.263674 16439 net.cpp:165] Memory required for data: 124699136
I0124 14:11:20.263677 16439 layer_factory.hpp:76] Creating layer conv2
I0124 14:11:20.263689 16439 net.cpp:106] Creating Layer conv2
I0124 14:11:20.263694 16439 net.cpp:454] conv2 <- pool1
I0124 14:11:20.263698 16439 net.cpp:411] conv2 -> conv2
I0124 14:11:20.274854 16439 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 28800
I0124 14:11:20.274876 16439 net.cpp:150] Setting up conv2
I0124 14:11:20.274884 16439 net.cpp:157] Top shape: 128 256 15 15 (7372800)
I0124 14:11:20.274889 16439 net.cpp:165] Memory required for data: 154190336
I0124 14:11:20.274899 16439 layer_factory.hpp:76] Creating layer relu2
I0124 14:11:20.274907 16439 net.cpp:106] Creating Layer relu2
I0124 14:11:20.274911 16439 net.cpp:454] relu2 <- conv2
I0124 14:11:20.274917 16439 net.cpp:411] relu2 -> relu2
I0124 14:11:20.275096 16439 net.cpp:150] Setting up relu2
I0124 14:11:20.275105 16439 net.cpp:157] Top shape: 128 256 15 15 (7372800)
I0124 14:11:20.275110 16439 net.cpp:165] Memory required for data: 183681536
I0124 14:11:20.275113 16439 layer_factory.hpp:76] Creating layer pool2
I0124 14:11:20.275121 16439 net.cpp:106] Creating Layer pool2
I0124 14:11:20.275125 16439 net.cpp:454] pool2 <- relu2
I0124 14:11:20.275130 16439 net.cpp:411] pool2 -> pool2
I0124 14:11:20.275434 16439 net.cpp:150] Setting up pool2
I0124 14:11:20.275444 16439 net.cpp:157] Top shape: 128 256 7 7 (1605632)
I0124 14:11:20.275449 16439 net.cpp:165] Memory required for data: 190104064
I0124 14:11:20.275454 16439 layer_factory.hpp:76] Creating layer conv3
I0124 14:11:20.275461 16439 net.cpp:106] Creating Layer conv3
I0124 14:11:20.275465 16439 net.cpp:454] conv3 <- pool2
I0124 14:11:20.275471 16439 net.cpp:411] conv3 -> conv3
I0124 14:11:20.303560 16439 net.cpp:150] Setting up conv3
I0124 14:11:20.303594 16439 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I0124 14:11:20.303598 16439 net.cpp:165] Memory required for data: 199737856
I0124 14:11:20.303613 16439 layer_factory.hpp:76] Creating layer relu3
I0124 14:11:20.303627 16439 net.cpp:106] Creating Layer relu3
I0124 14:11:20.303630 16439 net.cpp:454] relu3 <- conv3
I0124 14:11:20.303642 16439 net.cpp:411] relu3 -> relu3
I0124 14:11:20.303993 16439 net.cpp:150] Setting up relu3
I0124 14:11:20.304007 16439 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I0124 14:11:20.304010 16439 net.cpp:165] Memory required for data: 209371648
I0124 14:11:20.304013 16439 layer_factory.hpp:76] Creating layer conv4
I0124 14:11:20.304026 16439 net.cpp:106] Creating Layer conv4
I0124 14:11:20.304031 16439 net.cpp:454] conv4 <- relu3
I0124 14:11:20.304038 16439 net.cpp:411] conv4 -> conv4
I0124 14:11:20.326176 16439 net.cpp:150] Setting up conv4
I0124 14:11:20.326206 16439 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I0124 14:11:20.326210 16439 net.cpp:165] Memory required for data: 219005440
I0124 14:11:20.326221 16439 layer_factory.hpp:76] Creating layer relu4
I0124 14:11:20.326231 16439 net.cpp:106] Creating Layer relu4
I0124 14:11:20.326236 16439 net.cpp:454] relu4 <- conv4
I0124 14:11:20.326241 16439 net.cpp:411] relu4 -> relu4
I0124 14:11:20.326454 16439 net.cpp:150] Setting up relu4
I0124 14:11:20.326464 16439 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I0124 14:11:20.326469 16439 net.cpp:165] Memory required for data: 228639232
I0124 14:11:20.326472 16439 layer_factory.hpp:76] Creating layer conv5
I0124 14:11:20.326485 16439 net.cpp:106] Creating Layer conv5
I0124 14:11:20.326489 16439 net.cpp:454] conv5 <- relu4
I0124 14:11:20.326495 16439 net.cpp:411] conv5 -> conv5
I0124 14:11:20.342036 16439 net.cpp:150] Setting up conv5
I0124 14:11:20.342051 16439 net.cpp:157] Top shape: 128 256 7 7 (1605632)
I0124 14:11:20.342054 16439 net.cpp:165] Memory required for data: 235061760
I0124 14:11:20.342064 16439 layer_factory.hpp:76] Creating layer relu5
I0124 14:11:20.342073 16439 net.cpp:106] Creating Layer relu5
I0124 14:11:20.342077 16439 net.cpp:454] relu5 <- conv5
I0124 14:11:20.342087 16439 net.cpp:411] relu5 -> relu5
I0124 14:11:20.342291 16439 net.cpp:150] Setting up relu5
I0124 14:11:20.342306 16439 net.cpp:157] Top shape: 128 256 7 7 (1605632)
I0124 14:11:20.342314 16439 net.cpp:165] Memory required for data: 241484288
I0124 14:11:20.342319 16439 layer_factory.hpp:76] Creating layer pool5
I0124 14:11:20.342330 16439 net.cpp:106] Creating Layer pool5
I0124 14:11:20.342337 16439 net.cpp:454] pool5 <- relu5
I0124 14:11:20.342349 16439 net.cpp:411] pool5 -> pool5
I0124 14:11:20.342679 16439 net.cpp:150] Setting up pool5
I0124 14:11:20.342691 16439 net.cpp:157] Top shape: 128 256 3 3 (294912)
I0124 14:11:20.342694 16439 net.cpp:165] Memory required for data: 242663936
I0124 14:11:20.342697 16439 layer_factory.hpp:76] Creating layer fc6
I0124 14:11:20.342708 16439 net.cpp:106] Creating Layer fc6
I0124 14:11:20.342715 16439 net.cpp:454] fc6 <- pool5
I0124 14:11:20.342723 16439 net.cpp:411] fc6 -> fc6
I0124 14:11:20.489109 16439 net.cpp:150] Setting up fc6
I0124 14:11:20.489138 16439 net.cpp:157] Top shape: 128 2048 (262144)
I0124 14:11:20.489143 16439 net.cpp:165] Memory required for data: 243712512
I0124 14:11:20.489157 16439 layer_factory.hpp:76] Creating layer relu6
I0124 14:11:20.489173 16439 net.cpp:106] Creating Layer relu6
I0124 14:11:20.489182 16439 net.cpp:454] relu6 <- fc6
I0124 14:11:20.489192 16439 net.cpp:411] relu6 -> relu6
I0124 14:11:20.489445 16439 net.cpp:150] Setting up relu6
I0124 14:11:20.489457 16439 net.cpp:157] Top shape: 128 2048 (262144)
I0124 14:11:20.489462 16439 net.cpp:165] Memory required for data: 244761088
I0124 14:11:20.489466 16439 layer_factory.hpp:76] Creating layer drop6
I0124 14:11:20.489475 16439 net.cpp:106] Creating Layer drop6
I0124 14:11:20.489478 16439 net.cpp:454] drop6 <- relu6
I0124 14:11:20.489485 16439 net.cpp:411] drop6 -> drop6
I0124 14:11:20.489528 16439 net.cpp:150] Setting up drop6
I0124 14:11:20.489536 16439 net.cpp:157] Top shape: 128 2048 (262144)
I0124 14:11:20.489538 16439 net.cpp:165] Memory required for data: 245809664
I0124 14:11:20.489542 16439 layer_factory.hpp:76] Creating layer fc7
I0124 14:11:20.489550 16439 net.cpp:106] Creating Layer fc7
I0124 14:11:20.489555 16439 net.cpp:454] fc7 <- drop6
I0124 14:11:20.489562 16439 net.cpp:411] fc7 -> fc7
I0124 14:11:20.619869 16439 net.cpp:150] Setting up fc7
I0124 14:11:20.619943 16439 net.cpp:157] Top shape: 128 2048 (262144)
I0124 14:11:20.619947 16439 net.cpp:165] Memory required for data: 246858240
I0124 14:11:20.619957 16439 layer_factory.hpp:76] Creating layer relu7
I0124 14:11:20.619967 16439 net.cpp:106] Creating Layer relu7
I0124 14:11:20.619971 16439 net.cpp:454] relu7 <- fc7
I0124 14:11:20.619978 16439 net.cpp:411] relu7 -> relu7
I0124 14:11:20.620465 16439 net.cpp:150] Setting up relu7
I0124 14:11:20.620477 16439 net.cpp:157] Top shape: 128 2048 (262144)
I0124 14:11:20.620481 16439 net.cpp:165] Memory required for data: 247906816
I0124 14:11:20.620484 16439 layer_factory.hpp:76] Creating layer drop7
I0124 14:11:20.620492 16439 net.cpp:106] Creating Layer drop7
I0124 14:11:20.620496 16439 net.cpp:454] drop7 <- relu7
I0124 14:11:20.620503 16439 net.cpp:411] drop7 -> drop7
I0124 14:11:20.620543 16439 net.cpp:150] Setting up drop7
I0124 14:11:20.620549 16439 net.cpp:157] Top shape: 128 2048 (262144)
I0124 14:11:20.620553 16439 net.cpp:165] Memory required for data: 248955392
I0124 14:11:20.620555 16439 layer_factory.hpp:76] Creating layer fc8
I0124 14:11:20.620568 16439 net.cpp:106] Creating Layer fc8
I0124 14:11:20.620573 16439 net.cpp:454] fc8 <- drop7
I0124 14:11:20.620578 16439 net.cpp:411] fc8 -> fc8
I0124 14:11:20.684037 16439 net.cpp:150] Setting up fc8
I0124 14:11:20.684063 16439 net.cpp:157] Top shape: 128 1000 (128000)
I0124 14:11:20.684067 16439 net.cpp:165] Memory required for data: 249467392
I0124 14:11:20.684077 16439 layer_factory.hpp:76] Creating layer loss
I0124 14:11:20.684089 16439 net.cpp:106] Creating Layer loss
I0124 14:11:20.684094 16439 net.cpp:454] loss <- fc8
I0124 14:11:20.684099 16439 net.cpp:454] loss <- label
I0124 14:11:20.684106 16439 net.cpp:411] loss -> loss
I0124 14:11:20.684120 16439 layer_factory.hpp:76] Creating layer loss
I0124 14:11:20.685256 16439 net.cpp:150] Setting up loss
I0124 14:11:20.685268 16439 net.cpp:157] Top shape: (1)
I0124 14:11:20.685272 16439 net.cpp:160]     with loss weight 1
I0124 14:11:20.685298 16439 net.cpp:165] Memory required for data: 249467396
I0124 14:11:20.685302 16439 net.cpp:226] loss needs backward computation.
I0124 14:11:20.685307 16439 net.cpp:226] fc8 needs backward computation.
I0124 14:11:20.685309 16439 net.cpp:226] drop7 needs backward computation.
I0124 14:11:20.685313 16439 net.cpp:226] relu7 needs backward computation.
I0124 14:11:20.685317 16439 net.cpp:226] fc7 needs backward computation.
I0124 14:11:20.685319 16439 net.cpp:226] drop6 needs backward computation.
I0124 14:11:20.685333 16439 net.cpp:226] relu6 needs backward computation.
I0124 14:11:20.685339 16439 net.cpp:226] fc6 needs backward computation.
I0124 14:11:20.685343 16439 net.cpp:226] pool5 needs backward computation.
I0124 14:11:20.685345 16439 net.cpp:226] relu5 needs backward computation.
I0124 14:11:20.685349 16439 net.cpp:226] conv5 needs backward computation.
I0124 14:11:20.685353 16439 net.cpp:226] relu4 needs backward computation.
I0124 14:11:20.685355 16439 net.cpp:226] conv4 needs backward computation.
I0124 14:11:20.685359 16439 net.cpp:226] relu3 needs backward computation.
I0124 14:11:20.685362 16439 net.cpp:226] conv3 needs backward computation.
I0124 14:11:20.685365 16439 net.cpp:226] pool2 needs backward computation.
I0124 14:11:20.685374 16439 net.cpp:226] relu2 needs backward computation.
I0124 14:11:20.685377 16439 net.cpp:226] conv2 needs backward computation.
I0124 14:11:20.685381 16439 net.cpp:226] pool1 needs backward computation.
I0124 14:11:20.685384 16439 net.cpp:226] relu1 needs backward computation.
I0124 14:11:20.685396 16439 net.cpp:226] conv1 needs backward computation.
I0124 14:11:20.685400 16439 net.cpp:228] data does not need backward computation.
I0124 14:11:20.685403 16439 net.cpp:270] This network produces output loss
I0124 14:11:20.685423 16439 net.cpp:283] Network initialization done.
I0124 14:11:20.685571 16439 solver.cpp:180] Creating test net (#0) specified by net_param
I0124 14:11:20.685619 16439 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0124 14:11:20.686074 16439 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.04
    mirror: false
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 123
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/local/temporary/imagenet_clc_orig/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 14:11:20.686219 16439 layer_factory.hpp:76] Creating layer data
I0124 14:11:20.686313 16439 net.cpp:106] Creating Layer data
I0124 14:11:20.686331 16439 net.cpp:411] data -> data
I0124 14:11:20.686341 16439 net.cpp:411] data -> label
I0124 14:11:20.687978 16569 db_lmdb.cpp:38] Opened lmdb /local/temporary/imagenet_clc_orig/ilsvrc12_val_lmdb
I0124 14:11:20.690785 16439 data_layer.cpp:41] output data size: 50,3,128,128
I0124 14:11:20.710222 16439 net.cpp:150] Setting up data
I0124 14:11:20.710250 16439 net.cpp:157] Top shape: 50 3 128 128 (2457600)
I0124 14:11:20.710255 16439 net.cpp:157] Top shape: 50 (50)
I0124 14:11:20.710258 16439 net.cpp:165] Memory required for data: 9830600
I0124 14:11:20.710264 16439 layer_factory.hpp:76] Creating layer label_data_1_split
I0124 14:11:20.710278 16439 net.cpp:106] Creating Layer label_data_1_split
I0124 14:11:20.710283 16439 net.cpp:454] label_data_1_split <- label
I0124 14:11:20.710290 16439 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0124 14:11:20.710301 16439 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0124 14:11:20.710356 16439 net.cpp:150] Setting up label_data_1_split
I0124 14:11:20.710363 16439 net.cpp:157] Top shape: 50 (50)
I0124 14:11:20.710367 16439 net.cpp:157] Top shape: 50 (50)
I0124 14:11:20.710371 16439 net.cpp:165] Memory required for data: 9831000
I0124 14:11:20.710373 16439 layer_factory.hpp:76] Creating layer conv1
I0124 14:11:20.710386 16439 net.cpp:106] Creating Layer conv1
I0124 14:11:20.710391 16439 net.cpp:454] conv1 <- data
I0124 14:11:20.710397 16439 net.cpp:411] conv1 -> conv1
I0124 14:11:20.712811 16439 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 4356
I0124 14:11:20.712848 16439 net.cpp:150] Setting up conv1
I0124 14:11:20.712857 16439 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I0124 14:11:20.712862 16439 net.cpp:165] Memory required for data: 27111000
I0124 14:11:20.712877 16439 layer_factory.hpp:76] Creating layer relu1
I0124 14:11:20.712887 16439 net.cpp:106] Creating Layer relu1
I0124 14:11:20.712891 16439 net.cpp:454] relu1 <- conv1
I0124 14:11:20.712898 16439 net.cpp:411] relu1 -> relu1
I0124 14:11:20.713212 16439 net.cpp:150] Setting up relu1
I0124 14:11:20.713237 16439 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I0124 14:11:20.713240 16439 net.cpp:165] Memory required for data: 44391000
I0124 14:11:20.713243 16439 layer_factory.hpp:76] Creating layer pool1
I0124 14:11:20.713253 16439 net.cpp:106] Creating Layer pool1
I0124 14:11:20.713258 16439 net.cpp:454] pool1 <- relu1
I0124 14:11:20.713264 16439 net.cpp:411] pool1 -> pool1
I0124 14:11:20.713479 16439 net.cpp:150] Setting up pool1
I0124 14:11:20.713490 16439 net.cpp:157] Top shape: 50 96 15 15 (1080000)
I0124 14:11:20.713492 16439 net.cpp:165] Memory required for data: 48711000
I0124 14:11:20.713496 16439 layer_factory.hpp:76] Creating layer conv2
I0124 14:11:20.713507 16439 net.cpp:106] Creating Layer conv2
I0124 14:11:20.713510 16439 net.cpp:454] conv2 <- pool1
I0124 14:11:20.713516 16439 net.cpp:411] conv2 -> conv2
I0124 14:11:20.724822 16439 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 28800
I0124 14:11:20.724875 16439 net.cpp:150] Setting up conv2
I0124 14:11:20.724882 16439 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I0124 14:11:20.724885 16439 net.cpp:165] Memory required for data: 60231000
I0124 14:11:20.724900 16439 layer_factory.hpp:76] Creating layer relu2
I0124 14:11:20.724907 16439 net.cpp:106] Creating Layer relu2
I0124 14:11:20.724911 16439 net.cpp:454] relu2 <- conv2
I0124 14:11:20.724916 16439 net.cpp:411] relu2 -> relu2
I0124 14:11:20.725134 16439 net.cpp:150] Setting up relu2
I0124 14:11:20.725145 16439 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I0124 14:11:20.725148 16439 net.cpp:165] Memory required for data: 71751000
I0124 14:11:20.725152 16439 layer_factory.hpp:76] Creating layer pool2
I0124 14:11:20.725159 16439 net.cpp:106] Creating Layer pool2
I0124 14:11:20.725164 16439 net.cpp:454] pool2 <- relu2
I0124 14:11:20.725169 16439 net.cpp:411] pool2 -> pool2
I0124 14:11:20.725531 16439 net.cpp:150] Setting up pool2
I0124 14:11:20.725543 16439 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0124 14:11:20.725546 16439 net.cpp:165] Memory required for data: 74259800
I0124 14:11:20.725549 16439 layer_factory.hpp:76] Creating layer conv3
I0124 14:11:20.725564 16439 net.cpp:106] Creating Layer conv3
I0124 14:11:20.725569 16439 net.cpp:454] conv3 <- pool2
I0124 14:11:20.725575 16439 net.cpp:411] conv3 -> conv3
I0124 14:11:20.754081 16439 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 27648
I0124 14:11:20.754113 16439 net.cpp:150] Setting up conv3
I0124 14:11:20.754122 16439 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0124 14:11:20.754124 16439 net.cpp:165] Memory required for data: 78023000
I0124 14:11:20.754137 16439 layer_factory.hpp:76] Creating layer relu3
I0124 14:11:20.754150 16439 net.cpp:106] Creating Layer relu3
I0124 14:11:20.754154 16439 net.cpp:454] relu3 <- conv3
I0124 14:11:20.754160 16439 net.cpp:411] relu3 -> relu3
I0124 14:11:20.754379 16439 net.cpp:150] Setting up relu3
I0124 14:11:20.754390 16439 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0124 14:11:20.754393 16439 net.cpp:165] Memory required for data: 81786200
I0124 14:11:20.754396 16439 layer_factory.hpp:76] Creating layer conv4
I0124 14:11:20.754408 16439 net.cpp:106] Creating Layer conv4
I0124 14:11:20.754413 16439 net.cpp:454] conv4 <- relu3
I0124 14:11:20.754421 16439 net.cpp:411] conv4 -> conv4
I0124 14:11:20.777197 16439 net.cpp:150] Setting up conv4
I0124 14:11:20.777222 16439 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0124 14:11:20.777226 16439 net.cpp:165] Memory required for data: 85549400
I0124 14:11:20.777236 16439 layer_factory.hpp:76] Creating layer relu4
I0124 14:11:20.777245 16439 net.cpp:106] Creating Layer relu4
I0124 14:11:20.777251 16439 net.cpp:454] relu4 <- conv4
I0124 14:11:20.777257 16439 net.cpp:411] relu4 -> relu4
I0124 14:11:20.777482 16439 net.cpp:150] Setting up relu4
I0124 14:11:20.777493 16439 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0124 14:11:20.777497 16439 net.cpp:165] Memory required for data: 89312600
I0124 14:11:20.777500 16439 layer_factory.hpp:76] Creating layer conv5
I0124 14:11:20.777513 16439 net.cpp:106] Creating Layer conv5
I0124 14:11:20.777518 16439 net.cpp:454] conv5 <- relu4
I0124 14:11:20.777525 16439 net.cpp:411] conv5 -> conv5
I0124 14:11:20.793279 16439 net.cpp:150] Setting up conv5
I0124 14:11:20.793297 16439 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0124 14:11:20.793299 16439 net.cpp:165] Memory required for data: 91821400
I0124 14:11:20.793313 16439 layer_factory.hpp:76] Creating layer relu5
I0124 14:11:20.793320 16439 net.cpp:106] Creating Layer relu5
I0124 14:11:20.793328 16439 net.cpp:454] relu5 <- conv5
I0124 14:11:20.793334 16439 net.cpp:411] relu5 -> relu5
I0124 14:11:20.793666 16439 net.cpp:150] Setting up relu5
I0124 14:11:20.793678 16439 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0124 14:11:20.793681 16439 net.cpp:165] Memory required for data: 94330200
I0124 14:11:20.793685 16439 layer_factory.hpp:76] Creating layer pool5
I0124 14:11:20.793692 16439 net.cpp:106] Creating Layer pool5
I0124 14:11:20.793695 16439 net.cpp:454] pool5 <- relu5
I0124 14:11:20.793730 16439 net.cpp:411] pool5 -> pool5
I0124 14:11:20.793980 16439 net.cpp:150] Setting up pool5
I0124 14:11:20.793992 16439 net.cpp:157] Top shape: 50 256 3 3 (115200)
I0124 14:11:20.793995 16439 net.cpp:165] Memory required for data: 94791000
I0124 14:11:20.793998 16439 layer_factory.hpp:76] Creating layer fc6
I0124 14:11:20.794008 16439 net.cpp:106] Creating Layer fc6
I0124 14:11:20.794010 16439 net.cpp:454] fc6 <- pool5
I0124 14:11:20.794018 16439 net.cpp:411] fc6 -> fc6
I0124 14:11:20.940387 16439 net.cpp:150] Setting up fc6
I0124 14:11:20.940414 16439 net.cpp:157] Top shape: 50 2048 (102400)
I0124 14:11:20.940418 16439 net.cpp:165] Memory required for data: 95200600
I0124 14:11:20.940428 16439 layer_factory.hpp:76] Creating layer relu6
I0124 14:11:20.940440 16439 net.cpp:106] Creating Layer relu6
I0124 14:11:20.940444 16439 net.cpp:454] relu6 <- fc6
I0124 14:11:20.940454 16439 net.cpp:411] relu6 -> relu6
I0124 14:11:20.940958 16439 net.cpp:150] Setting up relu6
I0124 14:11:20.940969 16439 net.cpp:157] Top shape: 50 2048 (102400)
I0124 14:11:20.940973 16439 net.cpp:165] Memory required for data: 95610200
I0124 14:11:20.940976 16439 layer_factory.hpp:76] Creating layer drop6
I0124 14:11:20.940984 16439 net.cpp:106] Creating Layer drop6
I0124 14:11:20.940987 16439 net.cpp:454] drop6 <- relu6
I0124 14:11:20.940994 16439 net.cpp:411] drop6 -> drop6
I0124 14:11:20.941037 16439 net.cpp:150] Setting up drop6
I0124 14:11:20.941045 16439 net.cpp:157] Top shape: 50 2048 (102400)
I0124 14:11:20.941047 16439 net.cpp:165] Memory required for data: 96019800
I0124 14:11:20.941051 16439 layer_factory.hpp:76] Creating layer fc7
I0124 14:11:20.941059 16439 net.cpp:106] Creating Layer fc7
I0124 14:11:20.941066 16439 net.cpp:454] fc7 <- drop6
I0124 14:11:20.941073 16439 net.cpp:411] fc7 -> fc7
I0124 14:11:21.071173 16439 net.cpp:150] Setting up fc7
I0124 14:11:21.071199 16439 net.cpp:157] Top shape: 50 2048 (102400)
I0124 14:11:21.071203 16439 net.cpp:165] Memory required for data: 96429400
I0124 14:11:21.071213 16439 layer_factory.hpp:76] Creating layer relu7
I0124 14:11:21.071224 16439 net.cpp:106] Creating Layer relu7
I0124 14:11:21.071229 16439 net.cpp:454] relu7 <- fc7
I0124 14:11:21.071236 16439 net.cpp:411] relu7 -> relu7
I0124 14:11:21.071511 16439 net.cpp:150] Setting up relu7
I0124 14:11:21.071519 16439 net.cpp:157] Top shape: 50 2048 (102400)
I0124 14:11:21.071523 16439 net.cpp:165] Memory required for data: 96839000
I0124 14:11:21.071527 16439 layer_factory.hpp:76] Creating layer drop7
I0124 14:11:21.071534 16439 net.cpp:106] Creating Layer drop7
I0124 14:11:21.071539 16439 net.cpp:454] drop7 <- relu7
I0124 14:11:21.071547 16439 net.cpp:411] drop7 -> drop7
I0124 14:11:21.071588 16439 net.cpp:150] Setting up drop7
I0124 14:11:21.071593 16439 net.cpp:157] Top shape: 50 2048 (102400)
I0124 14:11:21.071596 16439 net.cpp:165] Memory required for data: 97248600
I0124 14:11:21.071599 16439 layer_factory.hpp:76] Creating layer fc8
I0124 14:11:21.071609 16439 net.cpp:106] Creating Layer fc8
I0124 14:11:21.071614 16439 net.cpp:454] fc8 <- drop7
I0124 14:11:21.071621 16439 net.cpp:411] fc8 -> fc8
I0124 14:11:21.135375 16439 net.cpp:150] Setting up fc8
I0124 14:11:21.135402 16439 net.cpp:157] Top shape: 50 1000 (50000)
I0124 14:11:21.135406 16439 net.cpp:165] Memory required for data: 97448600
I0124 14:11:21.135417 16439 layer_factory.hpp:76] Creating layer fc8_fc8_0_split
I0124 14:11:21.135426 16439 net.cpp:106] Creating Layer fc8_fc8_0_split
I0124 14:11:21.135432 16439 net.cpp:454] fc8_fc8_0_split <- fc8
I0124 14:11:21.135457 16439 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0124 14:11:21.135481 16439 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0124 14:11:21.135529 16439 net.cpp:150] Setting up fc8_fc8_0_split
I0124 14:11:21.135535 16439 net.cpp:157] Top shape: 50 1000 (50000)
I0124 14:11:21.135538 16439 net.cpp:157] Top shape: 50 1000 (50000)
I0124 14:11:21.135542 16439 net.cpp:165] Memory required for data: 97848600
I0124 14:11:21.135545 16439 layer_factory.hpp:76] Creating layer accuracy
I0124 14:11:21.135588 16439 net.cpp:106] Creating Layer accuracy
I0124 14:11:21.135592 16439 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0124 14:11:21.135596 16439 net.cpp:454] accuracy <- label_data_1_split_0
I0124 14:11:21.135603 16439 net.cpp:411] accuracy -> accuracy
I0124 14:11:21.135612 16439 net.cpp:150] Setting up accuracy
I0124 14:11:21.135617 16439 net.cpp:157] Top shape: (1)
I0124 14:11:21.135620 16439 net.cpp:165] Memory required for data: 97848604
I0124 14:11:21.135624 16439 layer_factory.hpp:76] Creating layer loss
I0124 14:11:21.135629 16439 net.cpp:106] Creating Layer loss
I0124 14:11:21.135634 16439 net.cpp:454] loss <- fc8_fc8_0_split_1
I0124 14:11:21.135637 16439 net.cpp:454] loss <- label_data_1_split_1
I0124 14:11:21.135643 16439 net.cpp:411] loss -> loss
I0124 14:11:21.135650 16439 layer_factory.hpp:76] Creating layer loss
I0124 14:11:21.136251 16439 net.cpp:150] Setting up loss
I0124 14:11:21.136265 16439 net.cpp:157] Top shape: (1)
I0124 14:11:21.136281 16439 net.cpp:160]     with loss weight 1
I0124 14:11:21.136303 16439 net.cpp:165] Memory required for data: 97848608
I0124 14:11:21.136307 16439 net.cpp:226] loss needs backward computation.
I0124 14:11:21.136312 16439 net.cpp:228] accuracy does not need backward computation.
I0124 14:11:21.136317 16439 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0124 14:11:21.136320 16439 net.cpp:226] fc8 needs backward computation.
I0124 14:11:21.136323 16439 net.cpp:226] drop7 needs backward computation.
I0124 14:11:21.136327 16439 net.cpp:226] relu7 needs backward computation.
I0124 14:11:21.136330 16439 net.cpp:226] fc7 needs backward computation.
I0124 14:11:21.136333 16439 net.cpp:226] drop6 needs backward computation.
I0124 14:11:21.136337 16439 net.cpp:226] relu6 needs backward computation.
I0124 14:11:21.136340 16439 net.cpp:226] fc6 needs backward computation.
I0124 14:11:21.136343 16439 net.cpp:226] pool5 needs backward computation.
I0124 14:11:21.136346 16439 net.cpp:226] relu5 needs backward computation.
I0124 14:11:21.136350 16439 net.cpp:226] conv5 needs backward computation.
I0124 14:11:21.136353 16439 net.cpp:226] relu4 needs backward computation.
I0124 14:11:21.136356 16439 net.cpp:226] conv4 needs backward computation.
I0124 14:11:21.136359 16439 net.cpp:226] relu3 needs backward computation.
I0124 14:11:21.136371 16439 net.cpp:226] conv3 needs backward computation.
I0124 14:11:21.136374 16439 net.cpp:226] pool2 needs backward computation.
I0124 14:11:21.136378 16439 net.cpp:226] relu2 needs backward computation.
I0124 14:11:21.136380 16439 net.cpp:226] conv2 needs backward computation.
I0124 14:11:21.136384 16439 net.cpp:226] pool1 needs backward computation.
I0124 14:11:21.136389 16439 net.cpp:226] relu1 needs backward computation.
I0124 14:11:21.136391 16439 net.cpp:226] conv1 needs backward computation.
I0124 14:11:21.136395 16439 net.cpp:228] label_data_1_split does not need backward computation.
I0124 14:11:21.136399 16439 net.cpp:228] data does not need backward computation.
I0124 14:11:21.136401 16439 net.cpp:270] This network produces output accuracy
I0124 14:11:21.136405 16439 net.cpp:270] This network produces output loss
I0124 14:11:21.136426 16439 net.cpp:283] Network initialization done.
I0124 14:11:21.136555 16439 solver.cpp:59] Solver scaffolding done.
I0124 14:11:21.137447 16439 caffe.cpp:128] Finetuning from caffenet128_lsuv_adagrad.prototxt.caffemodel
I0124 14:11:21.261750 16439 caffe.cpp:212] Starting Optimization
I0124 14:11:21.261775 16439 solver.cpp:287] Solving CaffeNet
I0124 14:11:21.261780 16439 solver.cpp:288] Learning Rate Policy: fixed
I0124 14:11:21.328245 16439 solver.cpp:236] Iteration 0, loss = 7.46656
I0124 14:11:21.328291 16439 solver.cpp:252]     Train net output #0: loss = 7.46656 (* 1 = 7.46656 loss)
I0124 14:11:21.328300 16439 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0124 14:11:21.724174 16439 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 14:11:25.832970 16439 solver.cpp:236] Iteration 20, loss = 6.89763
I0124 14:11:25.833044 16439 solver.cpp:252]     Train net output #0: loss = 6.89763 (* 1 = 6.89763 loss)
I0124 14:11:25.833117 16439 sgd_solver.cpp:106] Iteration 20, lr = 0.01
I0124 14:11:30.523062 16439 solver.cpp:236] Iteration 40, loss = 6.93131
I0124 14:11:30.523123 16439 solver.cpp:252]     Train net output #0: loss = 6.93131 (* 1 = 6.93131 loss)
I0124 14:11:30.523135 16439 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I0124 14:11:35.145519 16439 solver.cpp:236] Iteration 60, loss = 6.91199
I0124 14:11:35.145581 16439 solver.cpp:252]     Train net output #0: loss = 6.91199 (* 1 = 6.91199 loss)
I0124 14:11:35.145599 16439 sgd_solver.cpp:106] Iteration 60, lr = 0.01
I0124 14:11:39.701196 16439 solver.cpp:236] Iteration 80, loss = 6.9069
I0124 14:11:39.701262 16439 solver.cpp:252]     Train net output #0: loss = 6.9069 (* 1 = 6.9069 loss)
I0124 14:11:39.701274 16439 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I0124 14:11:44.242369 16439 solver.cpp:236] Iteration 100, loss = 6.91497
I0124 14:11:44.242430 16439 solver.cpp:252]     Train net output #0: loss = 6.91498 (* 1 = 6.91498 loss)
I0124 14:11:44.242442 16439 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I0124 14:11:48.841401 16439 solver.cpp:236] Iteration 120, loss = 6.92696
I0124 14:11:48.841471 16439 solver.cpp:252]     Train net output #0: loss = 6.92696 (* 1 = 6.92696 loss)
I0124 14:11:48.841483 16439 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I0124 14:11:53.470268 16439 solver.cpp:236] Iteration 140, loss = 6.95639
I0124 14:11:53.470407 16439 solver.cpp:252]     Train net output #0: loss = 6.9564 (* 1 = 6.9564 loss)
I0124 14:11:53.470419 16439 sgd_solver.cpp:106] Iteration 140, lr = 0.01
I0124 14:11:58.658367 16439 solver.cpp:236] Iteration 160, loss = 6.961
I0124 14:11:58.658661 16439 solver.cpp:252]     Train net output #0: loss = 6.961 (* 1 = 6.961 loss)
I0124 14:11:58.658710 16439 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I0124 14:12:03.948536 16439 solver.cpp:236] Iteration 180, loss = 6.94947
I0124 14:12:03.948595 16439 solver.cpp:252]     Train net output #0: loss = 6.94948 (* 1 = 6.94948 loss)
I0124 14:12:03.948606 16439 sgd_solver.cpp:106] Iteration 180, lr = 0.01
I0124 14:12:08.826077 16439 solver.cpp:236] Iteration 200, loss = 6.91475
I0124 14:12:08.826138 16439 solver.cpp:252]     Train net output #0: loss = 6.91476 (* 1 = 6.91476 loss)
I0124 14:12:08.826149 16439 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I0124 14:12:13.510099 16439 solver.cpp:236] Iteration 220, loss = 6.93873
I0124 14:12:13.510177 16439 solver.cpp:252]     Train net output #0: loss = 6.93874 (* 1 = 6.93874 loss)
I0124 14:12:13.510190 16439 sgd_solver.cpp:106] Iteration 220, lr = 0.01
I0124 14:12:18.174170 16439 solver.cpp:236] Iteration 240, loss = 6.93695
I0124 14:12:18.174240 16439 solver.cpp:252]     Train net output #0: loss = 6.93696 (* 1 = 6.93696 loss)
I0124 14:12:18.174253 16439 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I0124 14:12:22.861238 16439 solver.cpp:236] Iteration 260, loss = 6.90891
I0124 14:12:22.861302 16439 solver.cpp:252]     Train net output #0: loss = 6.90891 (* 1 = 6.90891 loss)
I0124 14:12:22.861313 16439 sgd_solver.cpp:106] Iteration 260, lr = 0.01
I0124 14:12:27.583827 16439 solver.cpp:236] Iteration 280, loss = 6.91739
I0124 14:12:27.584025 16439 solver.cpp:252]     Train net output #0: loss = 6.9174 (* 1 = 6.9174 loss)
I0124 14:12:27.584038 16439 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I0124 14:12:32.473613 16439 solver.cpp:236] Iteration 300, loss = 6.92419
I0124 14:12:32.473682 16439 solver.cpp:252]     Train net output #0: loss = 6.9242 (* 1 = 6.9242 loss)
I0124 14:12:32.473695 16439 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I0124 14:12:37.193045 16439 solver.cpp:236] Iteration 320, loss = 6.93144
I0124 14:12:37.193112 16439 solver.cpp:252]     Train net output #0: loss = 6.93145 (* 1 = 6.93145 loss)
I0124 14:12:37.193123 16439 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I0124 14:12:42.704164 16439 solver.cpp:236] Iteration 340, loss = 6.90261
I0124 14:12:42.704228 16439 solver.cpp:252]     Train net output #0: loss = 6.90262 (* 1 = 6.90262 loss)
I0124 14:12:42.704241 16439 sgd_solver.cpp:106] Iteration 340, lr = 0.01
I0124 14:12:47.405411 16439 solver.cpp:236] Iteration 360, loss = 6.93445
I0124 14:12:47.405475 16439 solver.cpp:252]     Train net output #0: loss = 6.93446 (* 1 = 6.93446 loss)
I0124 14:12:47.405488 16439 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I0124 14:12:51.787616 16439 solver.cpp:236] Iteration 380, loss = 6.91054
I0124 14:12:51.787693 16439 solver.cpp:252]     Train net output #0: loss = 6.91055 (* 1 = 6.91055 loss)
I0124 14:12:51.787705 16439 sgd_solver.cpp:106] Iteration 380, lr = 0.01
I0124 14:12:56.222539 16439 solver.cpp:236] Iteration 400, loss = 6.90363
I0124 14:12:56.222612 16439 solver.cpp:252]     Train net output #0: loss = 6.90364 (* 1 = 6.90364 loss)
I0124 14:12:56.222625 16439 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I0124 14:13:00.932662 16439 solver.cpp:236] Iteration 420, loss = 6.92533
I0124 14:13:00.932857 16439 solver.cpp:252]     Train net output #0: loss = 6.92534 (* 1 = 6.92534 loss)
I0124 14:13:00.932868 16439 sgd_solver.cpp:106] Iteration 420, lr = 0.01
I0124 14:13:05.644340 16439 solver.cpp:236] Iteration 440, loss = 85.4545
I0124 14:13:05.644403 16439 solver.cpp:252]     Train net output #0: loss = 85.4545 (* 1 = 85.4545 loss)
I0124 14:13:05.644417 16439 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I0124 14:13:10.345881 16439 solver.cpp:236] Iteration 460, loss = 6.94829
I0124 14:13:10.345957 16439 solver.cpp:252]     Train net output #0: loss = 6.9483 (* 1 = 6.9483 loss)
I0124 14:13:10.345969 16439 sgd_solver.cpp:106] Iteration 460, lr = 0.01
I0124 14:13:15.140652 16439 solver.cpp:236] Iteration 480, loss = 6.91407
I0124 14:13:15.140712 16439 solver.cpp:252]     Train net output #0: loss = 6.91408 (* 1 = 6.91408 loss)
I0124 14:13:15.140724 16439 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I0124 14:13:20.034554 16439 solver.cpp:236] Iteration 500, loss = 6.91831
I0124 14:13:20.034618 16439 solver.cpp:252]     Train net output #0: loss = 6.91832 (* 1 = 6.91832 loss)
I0124 14:13:20.034631 16439 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I0124 14:13:25.100630 16439 solver.cpp:236] Iteration 520, loss = 6.89728
I0124 14:13:25.100683 16439 solver.cpp:252]     Train net output #0: loss = 6.89729 (* 1 = 6.89729 loss)
I0124 14:13:25.100692 16439 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I0124 14:13:30.192185 16439 solver.cpp:236] Iteration 540, loss = 6.90872
I0124 14:13:30.192268 16439 solver.cpp:252]     Train net output #0: loss = 6.90873 (* 1 = 6.90873 loss)
I0124 14:13:30.192281 16439 sgd_solver.cpp:106] Iteration 540, lr = 0.01
I0124 14:13:35.049384 16439 solver.cpp:236] Iteration 560, loss = 6.9312
I0124 14:13:35.049598 16439 solver.cpp:252]     Train net output #0: loss = 6.93121 (* 1 = 6.93121 loss)
I0124 14:13:35.049614 16439 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I0124 14:13:39.866438 16439 solver.cpp:236] Iteration 580, loss = 6.92041
I0124 14:13:39.866505 16439 solver.cpp:252]     Train net output #0: loss = 6.92042 (* 1 = 6.92042 loss)
I0124 14:13:39.866518 16439 sgd_solver.cpp:106] Iteration 580, lr = 0.01
I0124 14:13:44.670676 16439 solver.cpp:236] Iteration 600, loss = 6.92589
I0124 14:13:44.670743 16439 solver.cpp:252]     Train net output #0: loss = 6.9259 (* 1 = 6.9259 loss)
I0124 14:13:44.670754 16439 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I0124 14:13:49.478515 16439 solver.cpp:236] Iteration 620, loss = 6.92044
I0124 14:13:49.478571 16439 solver.cpp:252]     Train net output #0: loss = 6.92045 (* 1 = 6.92045 loss)
I0124 14:13:49.478582 16439 sgd_solver.cpp:106] Iteration 620, lr = 0.01
I0124 14:13:54.321882 16439 solver.cpp:236] Iteration 640, loss = 6.92481
I0124 14:13:54.321964 16439 solver.cpp:252]     Train net output #0: loss = 6.92482 (* 1 = 6.92482 loss)
I0124 14:13:54.321976 16439 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I0124 14:13:59.159814 16439 solver.cpp:236] Iteration 660, loss = 6.91037
I0124 14:13:59.159880 16439 solver.cpp:252]     Train net output #0: loss = 6.91038 (* 1 = 6.91038 loss)
I0124 14:13:59.159899 16439 sgd_solver.cpp:106] Iteration 660, lr = 0.01
I0124 14:14:04.008739 16439 solver.cpp:236] Iteration 680, loss = 6.93952
I0124 14:14:04.008792 16439 solver.cpp:252]     Train net output #0: loss = 6.93953 (* 1 = 6.93953 loss)
I0124 14:14:04.008800 16439 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I0124 14:14:09.195883 16439 solver.cpp:236] Iteration 700, loss = 6.92945
I0124 14:14:09.196069 16439 solver.cpp:252]     Train net output #0: loss = 6.92946 (* 1 = 6.92946 loss)
I0124 14:14:09.196079 16439 sgd_solver.cpp:106] Iteration 700, lr = 0.01
