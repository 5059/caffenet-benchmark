I0124 14:23:24.511813 31536 caffe.cpp:184] Using GPUs 0
I0124 14:23:24.663559 31536 solver.cpp:48] Initializing solver from parameters: 
test_iter: 200
test_interval: 2000
base_lr: 0.01
display: 20
max_iter: 320000
lr_policy: "step"
weight_decay: 0.0005
stepsize: 100000
snapshot: 10000
snapshot_prefix: "snapshots1/caffenet128_rmsprop"
solver_mode: GPU
device_id: 0
net_param {
  name: "CaffeNet"
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TRAIN
    }
    transform_param {
      scale: 0.04
      mirror: true
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 124
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/home/old-ufo/datasets/imagenet/ilsvrc12_train_shuffle_lmdb"
      batch_size: 256
      backend: LMDB
    }
  }
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TEST
    }
    transform_param {
      scale: 0.04
      mirror: false
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 123
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/home/old-ufo/datasets/imagenet/ilsvrc12_val_lmdb"
      batch_size: 250
      backend: LMDB
    }
  }
  layer {
    name: "conv1"
    type: "Convolution"
    bottom: "data"
    top: "conv1"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 96
      kernel_size: 11
      stride: 4
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu1"
    type: "ReLU"
    bottom: "conv1"
    top: "relu1"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "relu1"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv2"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 2
      kernel_size: 5
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu2"
    type: "ReLU"
    bottom: "conv2"
    top: "conv2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "conv2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv3"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu3"
    type: "ReLU"
    bottom: "conv3"
    top: "conv3"
  }
  layer {
    name: "conv4"
    type: "Convolution"
    bottom: "conv3"
    top: "conv4"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu4"
    type: "ReLU"
    bottom: "conv4"
    top: "conv4"
  }
  layer {
    name: "conv5"
    type: "Convolution"
    bottom: "conv4"
    top: "conv5"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu5"
    type: "ReLU"
    bottom: "conv5"
    top: "conv5"
  }
  layer {
    name: "pool5"
    type: "Pooling"
    bottom: "conv5"
    top: "pool5"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "pool5"
    top: "fc6"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu6"
    type: "ReLU"
    bottom: "fc6"
    top: "fc6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "fc6"
    top: "fc6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "fc6"
    top: "fc7"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu7"
    type: "ReLU"
    bottom: "fc7"
    top: "fc7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "fc7"
    top: "fc7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc8"
    type: "InnerProduct"
    bottom: "fc7"
    top: "fc8"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 1000
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "accuracy"
    type: "Accuracy"
    bottom: "fc8"
    bottom: "label"
    top: "accuracy"
    include {
      phase: TEST
    }
  }
  layer {
    name: "loss"
    type: "SoftmaxWithLoss"
    bottom: "fc8"
    bottom: "label"
    top: "loss"
  }
}
test_initialization: false
average_loss: 20
iter_size: 1
rms_decay: 0.95
type: "RMSProp"
I0124 14:23:25.079885 31536 solver.cpp:86] Creating training net specified in net_param.
I0124 14:23:25.080036 31536 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0124 14:23:25.080075 31536 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0124 14:23:25.080246 31536 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.04
    mirror: true
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 124
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/old-ufo/datasets/imagenet/ilsvrc12_train_shuffle_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 14:23:25.080375 31536 layer_factory.hpp:76] Creating layer data
I0124 14:23:25.080929 31536 net.cpp:106] Creating Layer data
I0124 14:23:25.080946 31536 net.cpp:411] data -> data
I0124 14:23:25.080986 31536 net.cpp:411] data -> label
I0124 14:23:25.081626 31540 db_lmdb.cpp:38] Opened lmdb /home/old-ufo/datasets/imagenet/ilsvrc12_train_shuffle_lmdb
I0124 14:23:25.099112 31536 data_layer.cpp:41] output data size: 256,3,128,128
I0124 14:23:25.172317 31536 net.cpp:150] Setting up data
I0124 14:23:25.172343 31536 net.cpp:157] Top shape: 256 3 128 128 (12582912)
I0124 14:23:25.172349 31536 net.cpp:157] Top shape: 256 (256)
I0124 14:23:25.172353 31536 net.cpp:165] Memory required for data: 50332672
I0124 14:23:25.172364 31536 layer_factory.hpp:76] Creating layer conv1
I0124 14:23:25.172386 31536 net.cpp:106] Creating Layer conv1
I0124 14:23:25.172392 31536 net.cpp:454] conv1 <- data
I0124 14:23:25.172406 31536 net.cpp:411] conv1 -> conv1
I0124 14:23:25.320933 31536 net.cpp:150] Setting up conv1
I0124 14:23:25.320960 31536 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I0124 14:23:25.320965 31536 net.cpp:165] Memory required for data: 138806272
I0124 14:23:25.320982 31536 layer_factory.hpp:76] Creating layer relu1
I0124 14:23:25.320993 31536 net.cpp:106] Creating Layer relu1
I0124 14:23:25.320997 31536 net.cpp:454] relu1 <- conv1
I0124 14:23:25.321002 31536 net.cpp:411] relu1 -> relu1
I0124 14:23:25.321626 31536 net.cpp:150] Setting up relu1
I0124 14:23:25.321638 31536 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I0124 14:23:25.321641 31536 net.cpp:165] Memory required for data: 227279872
I0124 14:23:25.321645 31536 layer_factory.hpp:76] Creating layer pool1
I0124 14:23:25.321651 31536 net.cpp:106] Creating Layer pool1
I0124 14:23:25.321655 31536 net.cpp:454] pool1 <- relu1
I0124 14:23:25.321660 31536 net.cpp:411] pool1 -> pool1
I0124 14:23:25.322201 31536 net.cpp:150] Setting up pool1
I0124 14:23:25.322211 31536 net.cpp:157] Top shape: 256 96 15 15 (5529600)
I0124 14:23:25.322213 31536 net.cpp:165] Memory required for data: 249398272
I0124 14:23:25.322216 31536 layer_factory.hpp:76] Creating layer conv2
I0124 14:23:25.322224 31536 net.cpp:106] Creating Layer conv2
I0124 14:23:25.322227 31536 net.cpp:454] conv2 <- pool1
I0124 14:23:25.322232 31536 net.cpp:411] conv2 -> conv2
I0124 14:23:25.333742 31536 net.cpp:150] Setting up conv2
I0124 14:23:25.333763 31536 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I0124 14:23:25.333766 31536 net.cpp:165] Memory required for data: 308380672
I0124 14:23:25.333776 31536 layer_factory.hpp:76] Creating layer relu2
I0124 14:23:25.333784 31536 net.cpp:106] Creating Layer relu2
I0124 14:23:25.333787 31536 net.cpp:454] relu2 <- conv2
I0124 14:23:25.333793 31536 net.cpp:397] relu2 -> conv2 (in-place)
I0124 14:23:25.334384 31536 net.cpp:150] Setting up relu2
I0124 14:23:25.334396 31536 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I0124 14:23:25.334399 31536 net.cpp:165] Memory required for data: 367363072
I0124 14:23:25.334403 31536 layer_factory.hpp:76] Creating layer pool2
I0124 14:23:25.334413 31536 net.cpp:106] Creating Layer pool2
I0124 14:23:25.334415 31536 net.cpp:454] pool2 <- conv2
I0124 14:23:25.334422 31536 net.cpp:411] pool2 -> pool2
I0124 14:23:25.335271 31536 net.cpp:150] Setting up pool2
I0124 14:23:25.335286 31536 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I0124 14:23:25.335290 31536 net.cpp:165] Memory required for data: 380208128
I0124 14:23:25.335294 31536 layer_factory.hpp:76] Creating layer conv3
I0124 14:23:25.335305 31536 net.cpp:106] Creating Layer conv3
I0124 14:23:25.335310 31536 net.cpp:454] conv3 <- pool2
I0124 14:23:25.335317 31536 net.cpp:411] conv3 -> conv3
I0124 14:23:25.362766 31536 net.cpp:150] Setting up conv3
I0124 14:23:25.362789 31536 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 14:23:25.362792 31536 net.cpp:165] Memory required for data: 399475712
I0124 14:23:25.362803 31536 layer_factory.hpp:76] Creating layer relu3
I0124 14:23:25.362812 31536 net.cpp:106] Creating Layer relu3
I0124 14:23:25.362815 31536 net.cpp:454] relu3 <- conv3
I0124 14:23:25.362821 31536 net.cpp:397] relu3 -> conv3 (in-place)
I0124 14:23:25.363351 31536 net.cpp:150] Setting up relu3
I0124 14:23:25.363360 31536 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 14:23:25.363363 31536 net.cpp:165] Memory required for data: 418743296
I0124 14:23:25.363381 31536 layer_factory.hpp:76] Creating layer conv4
I0124 14:23:25.363389 31536 net.cpp:106] Creating Layer conv4
I0124 14:23:25.363391 31536 net.cpp:454] conv4 <- conv3
I0124 14:23:25.363396 31536 net.cpp:411] conv4 -> conv4
I0124 14:23:25.384536 31536 net.cpp:150] Setting up conv4
I0124 14:23:25.384559 31536 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 14:23:25.384562 31536 net.cpp:165] Memory required for data: 438010880
I0124 14:23:25.384570 31536 layer_factory.hpp:76] Creating layer relu4
I0124 14:23:25.384578 31536 net.cpp:106] Creating Layer relu4
I0124 14:23:25.384582 31536 net.cpp:454] relu4 <- conv4
I0124 14:23:25.384587 31536 net.cpp:397] relu4 -> conv4 (in-place)
I0124 14:23:25.385116 31536 net.cpp:150] Setting up relu4
I0124 14:23:25.385125 31536 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 14:23:25.385128 31536 net.cpp:165] Memory required for data: 457278464
I0124 14:23:25.385130 31536 layer_factory.hpp:76] Creating layer conv5
I0124 14:23:25.385139 31536 net.cpp:106] Creating Layer conv5
I0124 14:23:25.385141 31536 net.cpp:454] conv5 <- conv4
I0124 14:23:25.385146 31536 net.cpp:411] conv5 -> conv5
I0124 14:23:25.400138 31536 net.cpp:150] Setting up conv5
I0124 14:23:25.400159 31536 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I0124 14:23:25.400162 31536 net.cpp:165] Memory required for data: 470123520
I0124 14:23:25.400172 31536 layer_factory.hpp:76] Creating layer relu5
I0124 14:23:25.400180 31536 net.cpp:106] Creating Layer relu5
I0124 14:23:25.400184 31536 net.cpp:454] relu5 <- conv5
I0124 14:23:25.400189 31536 net.cpp:397] relu5 -> conv5 (in-place)
I0124 14:23:25.400743 31536 net.cpp:150] Setting up relu5
I0124 14:23:25.400753 31536 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I0124 14:23:25.400755 31536 net.cpp:165] Memory required for data: 482968576
I0124 14:23:25.400758 31536 layer_factory.hpp:76] Creating layer pool5
I0124 14:23:25.400764 31536 net.cpp:106] Creating Layer pool5
I0124 14:23:25.400766 31536 net.cpp:454] pool5 <- conv5
I0124 14:23:25.400770 31536 net.cpp:411] pool5 -> pool5
I0124 14:23:25.401350 31536 net.cpp:150] Setting up pool5
I0124 14:23:25.401358 31536 net.cpp:157] Top shape: 256 256 3 3 (589824)
I0124 14:23:25.401360 31536 net.cpp:165] Memory required for data: 485327872
I0124 14:23:25.401363 31536 layer_factory.hpp:76] Creating layer fc6
I0124 14:23:25.401370 31536 net.cpp:106] Creating Layer fc6
I0124 14:23:25.401371 31536 net.cpp:454] fc6 <- pool5
I0124 14:23:25.401377 31536 net.cpp:411] fc6 -> fc6
I0124 14:23:25.525099 31536 net.cpp:150] Setting up fc6
I0124 14:23:25.525122 31536 net.cpp:157] Top shape: 256 2048 (524288)
I0124 14:23:25.525126 31536 net.cpp:165] Memory required for data: 487425024
I0124 14:23:25.525135 31536 layer_factory.hpp:76] Creating layer relu6
I0124 14:23:25.525143 31536 net.cpp:106] Creating Layer relu6
I0124 14:23:25.525147 31536 net.cpp:454] relu6 <- fc6
I0124 14:23:25.525153 31536 net.cpp:397] relu6 -> fc6 (in-place)
I0124 14:23:25.525868 31536 net.cpp:150] Setting up relu6
I0124 14:23:25.525881 31536 net.cpp:157] Top shape: 256 2048 (524288)
I0124 14:23:25.525883 31536 net.cpp:165] Memory required for data: 489522176
I0124 14:23:25.525887 31536 layer_factory.hpp:76] Creating layer drop6
I0124 14:23:25.525897 31536 net.cpp:106] Creating Layer drop6
I0124 14:23:25.525902 31536 net.cpp:454] drop6 <- fc6
I0124 14:23:25.525905 31536 net.cpp:397] drop6 -> fc6 (in-place)
I0124 14:23:25.525936 31536 net.cpp:150] Setting up drop6
I0124 14:23:25.525941 31536 net.cpp:157] Top shape: 256 2048 (524288)
I0124 14:23:25.525943 31536 net.cpp:165] Memory required for data: 491619328
I0124 14:23:25.525945 31536 layer_factory.hpp:76] Creating layer fc7
I0124 14:23:25.525952 31536 net.cpp:106] Creating Layer fc7
I0124 14:23:25.525954 31536 net.cpp:454] fc7 <- fc6
I0124 14:23:25.525959 31536 net.cpp:411] fc7 -> fc7
I0124 14:23:25.635411 31536 net.cpp:150] Setting up fc7
I0124 14:23:25.635434 31536 net.cpp:157] Top shape: 256 2048 (524288)
I0124 14:23:25.635437 31536 net.cpp:165] Memory required for data: 493716480
I0124 14:23:25.635468 31536 layer_factory.hpp:76] Creating layer relu7
I0124 14:23:25.635476 31536 net.cpp:106] Creating Layer relu7
I0124 14:23:25.635485 31536 net.cpp:454] relu7 <- fc7
I0124 14:23:25.635490 31536 net.cpp:397] relu7 -> fc7 (in-place)
I0124 14:23:25.636257 31536 net.cpp:150] Setting up relu7
I0124 14:23:25.636268 31536 net.cpp:157] Top shape: 256 2048 (524288)
I0124 14:23:25.636271 31536 net.cpp:165] Memory required for data: 495813632
I0124 14:23:25.636275 31536 layer_factory.hpp:76] Creating layer drop7
I0124 14:23:25.636281 31536 net.cpp:106] Creating Layer drop7
I0124 14:23:25.636284 31536 net.cpp:454] drop7 <- fc7
I0124 14:23:25.636288 31536 net.cpp:397] drop7 -> fc7 (in-place)
I0124 14:23:25.636313 31536 net.cpp:150] Setting up drop7
I0124 14:23:25.636317 31536 net.cpp:157] Top shape: 256 2048 (524288)
I0124 14:23:25.636319 31536 net.cpp:165] Memory required for data: 497910784
I0124 14:23:25.636322 31536 layer_factory.hpp:76] Creating layer fc8
I0124 14:23:25.636332 31536 net.cpp:106] Creating Layer fc8
I0124 14:23:25.636335 31536 net.cpp:454] fc8 <- fc7
I0124 14:23:25.636340 31536 net.cpp:411] fc8 -> fc8
I0124 14:23:25.690111 31536 net.cpp:150] Setting up fc8
I0124 14:23:25.690134 31536 net.cpp:157] Top shape: 256 1000 (256000)
I0124 14:23:25.690135 31536 net.cpp:165] Memory required for data: 498934784
I0124 14:23:25.690143 31536 layer_factory.hpp:76] Creating layer loss
I0124 14:23:25.690152 31536 net.cpp:106] Creating Layer loss
I0124 14:23:25.690157 31536 net.cpp:454] loss <- fc8
I0124 14:23:25.690163 31536 net.cpp:454] loss <- label
I0124 14:23:25.690171 31536 net.cpp:411] loss -> loss
I0124 14:23:25.690184 31536 layer_factory.hpp:76] Creating layer loss
I0124 14:23:25.691552 31536 net.cpp:150] Setting up loss
I0124 14:23:25.691566 31536 net.cpp:157] Top shape: (1)
I0124 14:23:25.691570 31536 net.cpp:160]     with loss weight 1
I0124 14:23:25.691584 31536 net.cpp:165] Memory required for data: 498934788
I0124 14:23:25.691589 31536 net.cpp:226] loss needs backward computation.
I0124 14:23:25.691593 31536 net.cpp:226] fc8 needs backward computation.
I0124 14:23:25.691596 31536 net.cpp:226] drop7 needs backward computation.
I0124 14:23:25.691601 31536 net.cpp:226] relu7 needs backward computation.
I0124 14:23:25.691603 31536 net.cpp:226] fc7 needs backward computation.
I0124 14:23:25.691606 31536 net.cpp:226] drop6 needs backward computation.
I0124 14:23:25.691609 31536 net.cpp:226] relu6 needs backward computation.
I0124 14:23:25.691612 31536 net.cpp:226] fc6 needs backward computation.
I0124 14:23:25.691617 31536 net.cpp:226] pool5 needs backward computation.
I0124 14:23:25.691622 31536 net.cpp:226] relu5 needs backward computation.
I0124 14:23:25.691624 31536 net.cpp:226] conv5 needs backward computation.
I0124 14:23:25.691628 31536 net.cpp:226] relu4 needs backward computation.
I0124 14:23:25.691632 31536 net.cpp:226] conv4 needs backward computation.
I0124 14:23:25.691635 31536 net.cpp:226] relu3 needs backward computation.
I0124 14:23:25.691639 31536 net.cpp:226] conv3 needs backward computation.
I0124 14:23:25.691643 31536 net.cpp:226] pool2 needs backward computation.
I0124 14:23:25.691648 31536 net.cpp:226] relu2 needs backward computation.
I0124 14:23:25.691650 31536 net.cpp:226] conv2 needs backward computation.
I0124 14:23:25.691654 31536 net.cpp:226] pool1 needs backward computation.
I0124 14:23:25.691658 31536 net.cpp:226] relu1 needs backward computation.
I0124 14:23:25.691663 31536 net.cpp:226] conv1 needs backward computation.
I0124 14:23:25.691669 31536 net.cpp:228] data does not need backward computation.
I0124 14:23:25.691673 31536 net.cpp:270] This network produces output loss
I0124 14:23:25.691689 31536 net.cpp:283] Network initialization done.
I0124 14:23:25.691779 31536 solver.cpp:181] Creating test net (#0) specified by net_param
I0124 14:23:25.691819 31536 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0124 14:23:25.691970 31536 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.04
    mirror: false
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 123
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/old-ufo/datasets/imagenet/ilsvrc12_val_lmdb"
    batch_size: 250
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 14:23:25.692101 31536 layer_factory.hpp:76] Creating layer data
I0124 14:23:25.692293 31536 net.cpp:106] Creating Layer data
I0124 14:23:25.692303 31536 net.cpp:411] data -> data
I0124 14:23:25.692312 31536 net.cpp:411] data -> label
I0124 14:23:25.692945 31549 db_lmdb.cpp:38] Opened lmdb /home/old-ufo/datasets/imagenet/ilsvrc12_val_lmdb
I0124 14:23:25.695750 31536 data_layer.cpp:41] output data size: 250,3,128,128
I0124 14:23:25.770270 31536 net.cpp:150] Setting up data
I0124 14:23:25.770308 31536 net.cpp:157] Top shape: 250 3 128 128 (12288000)
I0124 14:23:25.770318 31536 net.cpp:157] Top shape: 250 (250)
I0124 14:23:25.770323 31536 net.cpp:165] Memory required for data: 49153000
I0124 14:23:25.770330 31536 layer_factory.hpp:76] Creating layer label_data_1_split
I0124 14:23:25.770345 31536 net.cpp:106] Creating Layer label_data_1_split
I0124 14:23:25.770351 31536 net.cpp:454] label_data_1_split <- label
I0124 14:23:25.770361 31536 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0124 14:23:25.770373 31536 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0124 14:23:25.770534 31536 net.cpp:150] Setting up label_data_1_split
I0124 14:23:25.770546 31536 net.cpp:157] Top shape: 250 (250)
I0124 14:23:25.770551 31536 net.cpp:157] Top shape: 250 (250)
I0124 14:23:25.770555 31536 net.cpp:165] Memory required for data: 49155000
I0124 14:23:25.770558 31536 layer_factory.hpp:76] Creating layer conv1
I0124 14:23:25.770571 31536 net.cpp:106] Creating Layer conv1
I0124 14:23:25.770576 31536 net.cpp:454] conv1 <- data
I0124 14:23:25.770583 31536 net.cpp:411] conv1 -> conv1
I0124 14:23:25.776177 31536 net.cpp:150] Setting up conv1
I0124 14:23:25.776203 31536 net.cpp:157] Top shape: 250 96 30 30 (21600000)
I0124 14:23:25.776208 31536 net.cpp:165] Memory required for data: 135555000
I0124 14:23:25.776226 31536 layer_factory.hpp:76] Creating layer relu1
I0124 14:23:25.776238 31536 net.cpp:106] Creating Layer relu1
I0124 14:23:25.776244 31536 net.cpp:454] relu1 <- conv1
I0124 14:23:25.776253 31536 net.cpp:411] relu1 -> relu1
I0124 14:23:25.777204 31536 net.cpp:150] Setting up relu1
I0124 14:23:25.777218 31536 net.cpp:157] Top shape: 250 96 30 30 (21600000)
I0124 14:23:25.777221 31536 net.cpp:165] Memory required for data: 221955000
I0124 14:23:25.777225 31536 layer_factory.hpp:76] Creating layer pool1
I0124 14:23:25.777235 31536 net.cpp:106] Creating Layer pool1
I0124 14:23:25.777237 31536 net.cpp:454] pool1 <- relu1
I0124 14:23:25.777241 31536 net.cpp:411] pool1 -> pool1
I0124 14:23:25.777956 31536 net.cpp:150] Setting up pool1
I0124 14:23:25.777969 31536 net.cpp:157] Top shape: 250 96 15 15 (5400000)
I0124 14:23:25.777972 31536 net.cpp:165] Memory required for data: 243555000
I0124 14:23:25.777976 31536 layer_factory.hpp:76] Creating layer conv2
I0124 14:23:25.777987 31536 net.cpp:106] Creating Layer conv2
I0124 14:23:25.777989 31536 net.cpp:454] conv2 <- pool1
I0124 14:23:25.777997 31536 net.cpp:411] conv2 -> conv2
I0124 14:23:25.794147 31536 net.cpp:150] Setting up conv2
I0124 14:23:25.794174 31536 net.cpp:157] Top shape: 250 256 15 15 (14400000)
I0124 14:23:25.794180 31536 net.cpp:165] Memory required for data: 301155000
I0124 14:23:25.794194 31536 layer_factory.hpp:76] Creating layer relu2
I0124 14:23:25.794205 31536 net.cpp:106] Creating Layer relu2
I0124 14:23:25.794229 31536 net.cpp:454] relu2 <- conv2
I0124 14:23:25.794301 31536 net.cpp:397] relu2 -> conv2 (in-place)
I0124 14:23:25.796939 31536 net.cpp:150] Setting up relu2
I0124 14:23:25.796969 31536 net.cpp:157] Top shape: 250 256 15 15 (14400000)
I0124 14:23:25.796973 31536 net.cpp:165] Memory required for data: 358755000
I0124 14:23:25.796979 31536 layer_factory.hpp:76] Creating layer pool2
I0124 14:23:25.796993 31536 net.cpp:106] Creating Layer pool2
I0124 14:23:25.796998 31536 net.cpp:454] pool2 <- conv2
I0124 14:23:25.797006 31536 net.cpp:411] pool2 -> pool2
I0124 14:23:25.798135 31536 net.cpp:150] Setting up pool2
I0124 14:23:25.798238 31536 net.cpp:157] Top shape: 250 256 7 7 (3136000)
I0124 14:23:25.798248 31536 net.cpp:165] Memory required for data: 371299000
I0124 14:23:25.798281 31536 layer_factory.hpp:76] Creating layer conv3
I0124 14:23:25.798328 31536 net.cpp:106] Creating Layer conv3
I0124 14:23:25.798337 31536 net.cpp:454] conv3 <- pool2
I0124 14:23:25.798398 31536 net.cpp:411] conv3 -> conv3
I0124 14:23:25.831223 31536 net.cpp:150] Setting up conv3
I0124 14:23:25.831246 31536 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 14:23:25.831249 31536 net.cpp:165] Memory required for data: 390115000
I0124 14:23:25.831262 31536 layer_factory.hpp:76] Creating layer relu3
I0124 14:23:25.831272 31536 net.cpp:106] Creating Layer relu3
I0124 14:23:25.831277 31536 net.cpp:454] relu3 <- conv3
I0124 14:23:25.831285 31536 net.cpp:397] relu3 -> conv3 (in-place)
I0124 14:23:25.832067 31536 net.cpp:150] Setting up relu3
I0124 14:23:25.832083 31536 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 14:23:25.832088 31536 net.cpp:165] Memory required for data: 408931000
I0124 14:23:25.832093 31536 layer_factory.hpp:76] Creating layer conv4
I0124 14:23:25.832105 31536 net.cpp:106] Creating Layer conv4
I0124 14:23:25.832113 31536 net.cpp:454] conv4 <- conv3
I0124 14:23:25.832121 31536 net.cpp:411] conv4 -> conv4
I0124 14:23:25.855010 31536 net.cpp:150] Setting up conv4
I0124 14:23:25.855033 31536 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 14:23:25.855038 31536 net.cpp:165] Memory required for data: 427747000
I0124 14:23:25.855048 31536 layer_factory.hpp:76] Creating layer relu4
I0124 14:23:25.855059 31536 net.cpp:106] Creating Layer relu4
I0124 14:23:25.855064 31536 net.cpp:454] relu4 <- conv4
I0124 14:23:25.855072 31536 net.cpp:397] relu4 -> conv4 (in-place)
I0124 14:23:25.855659 31536 net.cpp:150] Setting up relu4
I0124 14:23:25.855670 31536 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 14:23:25.855674 31536 net.cpp:165] Memory required for data: 446563000
I0124 14:23:25.855679 31536 layer_factory.hpp:76] Creating layer conv5
I0124 14:23:25.855690 31536 net.cpp:106] Creating Layer conv5
I0124 14:23:25.855697 31536 net.cpp:454] conv5 <- conv4
I0124 14:23:25.855710 31536 net.cpp:411] conv5 -> conv5
I0124 14:23:25.872773 31536 net.cpp:150] Setting up conv5
I0124 14:23:25.872803 31536 net.cpp:157] Top shape: 250 256 7 7 (3136000)
I0124 14:23:25.872807 31536 net.cpp:165] Memory required for data: 459107000
I0124 14:23:25.872824 31536 layer_factory.hpp:76] Creating layer relu5
I0124 14:23:25.872838 31536 net.cpp:106] Creating Layer relu5
I0124 14:23:25.872845 31536 net.cpp:454] relu5 <- conv5
I0124 14:23:25.872853 31536 net.cpp:397] relu5 -> conv5 (in-place)
I0124 14:23:25.873451 31536 net.cpp:150] Setting up relu5
I0124 14:23:25.873464 31536 net.cpp:157] Top shape: 250 256 7 7 (3136000)
I0124 14:23:25.873467 31536 net.cpp:165] Memory required for data: 471651000
I0124 14:23:25.873471 31536 layer_factory.hpp:76] Creating layer pool5
I0124 14:23:25.873481 31536 net.cpp:106] Creating Layer pool5
I0124 14:23:25.873484 31536 net.cpp:454] pool5 <- conv5
I0124 14:23:25.873493 31536 net.cpp:411] pool5 -> pool5
I0124 14:23:25.874138 31536 net.cpp:150] Setting up pool5
I0124 14:23:25.874150 31536 net.cpp:157] Top shape: 250 256 3 3 (576000)
I0124 14:23:25.874152 31536 net.cpp:165] Memory required for data: 473955000
I0124 14:23:25.874156 31536 layer_factory.hpp:76] Creating layer fc6
I0124 14:23:25.874187 31536 net.cpp:106] Creating Layer fc6
I0124 14:23:25.874194 31536 net.cpp:454] fc6 <- pool5
I0124 14:23:25.874203 31536 net.cpp:411] fc6 -> fc6
I0124 14:23:26.004297 31536 net.cpp:150] Setting up fc6
I0124 14:23:26.004319 31536 net.cpp:157] Top shape: 250 2048 (512000)
I0124 14:23:26.004323 31536 net.cpp:165] Memory required for data: 476003000
I0124 14:23:26.004330 31536 layer_factory.hpp:76] Creating layer relu6
I0124 14:23:26.004343 31536 net.cpp:106] Creating Layer relu6
I0124 14:23:26.004348 31536 net.cpp:454] relu6 <- fc6
I0124 14:23:26.004354 31536 net.cpp:397] relu6 -> fc6 (in-place)
I0124 14:23:26.005187 31536 net.cpp:150] Setting up relu6
I0124 14:23:26.005198 31536 net.cpp:157] Top shape: 250 2048 (512000)
I0124 14:23:26.005200 31536 net.cpp:165] Memory required for data: 478051000
I0124 14:23:26.005203 31536 layer_factory.hpp:76] Creating layer drop6
I0124 14:23:26.005213 31536 net.cpp:106] Creating Layer drop6
I0124 14:23:26.005215 31536 net.cpp:454] drop6 <- fc6
I0124 14:23:26.005223 31536 net.cpp:397] drop6 -> fc6 (in-place)
I0124 14:23:26.005264 31536 net.cpp:150] Setting up drop6
I0124 14:23:26.005270 31536 net.cpp:157] Top shape: 250 2048 (512000)
I0124 14:23:26.005272 31536 net.cpp:165] Memory required for data: 480099000
I0124 14:23:26.005276 31536 layer_factory.hpp:76] Creating layer fc7
I0124 14:23:26.005286 31536 net.cpp:106] Creating Layer fc7
I0124 14:23:26.005291 31536 net.cpp:454] fc7 <- fc6
I0124 14:23:26.005300 31536 net.cpp:411] fc7 -> fc7
I0124 14:23:26.137691 31536 net.cpp:150] Setting up fc7
I0124 14:23:26.137720 31536 net.cpp:157] Top shape: 250 2048 (512000)
I0124 14:23:26.137723 31536 net.cpp:165] Memory required for data: 482147000
I0124 14:23:26.137732 31536 layer_factory.hpp:76] Creating layer relu7
I0124 14:23:26.137742 31536 net.cpp:106] Creating Layer relu7
I0124 14:23:26.137747 31536 net.cpp:454] relu7 <- fc7
I0124 14:23:26.137753 31536 net.cpp:397] relu7 -> fc7 (in-place)
I0124 14:23:26.138494 31536 net.cpp:150] Setting up relu7
I0124 14:23:26.138504 31536 net.cpp:157] Top shape: 250 2048 (512000)
I0124 14:23:26.138506 31536 net.cpp:165] Memory required for data: 484195000
I0124 14:23:26.138511 31536 layer_factory.hpp:76] Creating layer drop7
I0124 14:23:26.138519 31536 net.cpp:106] Creating Layer drop7
I0124 14:23:26.138522 31536 net.cpp:454] drop7 <- fc7
I0124 14:23:26.138530 31536 net.cpp:397] drop7 -> fc7 (in-place)
I0124 14:23:26.138566 31536 net.cpp:150] Setting up drop7
I0124 14:23:26.138576 31536 net.cpp:157] Top shape: 250 2048 (512000)
I0124 14:23:26.138578 31536 net.cpp:165] Memory required for data: 486243000
I0124 14:23:26.138582 31536 layer_factory.hpp:76] Creating layer fc8
I0124 14:23:26.138592 31536 net.cpp:106] Creating Layer fc8
I0124 14:23:26.138595 31536 net.cpp:454] fc8 <- fc7
I0124 14:23:26.138603 31536 net.cpp:411] fc8 -> fc8
I0124 14:23:26.209441 31536 net.cpp:150] Setting up fc8
I0124 14:23:26.209466 31536 net.cpp:157] Top shape: 250 1000 (250000)
I0124 14:23:26.209471 31536 net.cpp:165] Memory required for data: 487243000
I0124 14:23:26.209482 31536 layer_factory.hpp:76] Creating layer fc8_fc8_0_split
I0124 14:23:26.209491 31536 net.cpp:106] Creating Layer fc8_fc8_0_split
I0124 14:23:26.209496 31536 net.cpp:454] fc8_fc8_0_split <- fc8
I0124 14:23:26.209506 31536 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0124 14:23:26.209517 31536 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0124 14:23:26.209568 31536 net.cpp:150] Setting up fc8_fc8_0_split
I0124 14:23:26.209576 31536 net.cpp:157] Top shape: 250 1000 (250000)
I0124 14:23:26.209580 31536 net.cpp:157] Top shape: 250 1000 (250000)
I0124 14:23:26.209584 31536 net.cpp:165] Memory required for data: 489243000
I0124 14:23:26.209588 31536 layer_factory.hpp:76] Creating layer accuracy
I0124 14:23:26.209601 31536 net.cpp:106] Creating Layer accuracy
I0124 14:23:26.209605 31536 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0124 14:23:26.209610 31536 net.cpp:454] accuracy <- label_data_1_split_0
I0124 14:23:26.209615 31536 net.cpp:411] accuracy -> accuracy
I0124 14:23:26.209648 31536 net.cpp:150] Setting up accuracy
I0124 14:23:26.209655 31536 net.cpp:157] Top shape: (1)
I0124 14:23:26.209657 31536 net.cpp:165] Memory required for data: 489243004
I0124 14:23:26.209661 31536 layer_factory.hpp:76] Creating layer loss
I0124 14:23:26.209669 31536 net.cpp:106] Creating Layer loss
I0124 14:23:26.209673 31536 net.cpp:454] loss <- fc8_fc8_0_split_1
I0124 14:23:26.209678 31536 net.cpp:454] loss <- label_data_1_split_1
I0124 14:23:26.209683 31536 net.cpp:411] loss -> loss
I0124 14:23:26.209692 31536 layer_factory.hpp:76] Creating layer loss
I0124 14:23:26.211498 31536 net.cpp:150] Setting up loss
I0124 14:23:26.211521 31536 net.cpp:157] Top shape: (1)
I0124 14:23:26.211525 31536 net.cpp:160]     with loss weight 1
I0124 14:23:26.211536 31536 net.cpp:165] Memory required for data: 489243008
I0124 14:23:26.211541 31536 net.cpp:226] loss needs backward computation.
I0124 14:23:26.211547 31536 net.cpp:228] accuracy does not need backward computation.
I0124 14:23:26.211552 31536 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0124 14:23:26.211556 31536 net.cpp:226] fc8 needs backward computation.
I0124 14:23:26.211560 31536 net.cpp:226] drop7 needs backward computation.
I0124 14:23:26.211563 31536 net.cpp:226] relu7 needs backward computation.
I0124 14:23:26.211567 31536 net.cpp:226] fc7 needs backward computation.
I0124 14:23:26.211570 31536 net.cpp:226] drop6 needs backward computation.
I0124 14:23:26.211575 31536 net.cpp:226] relu6 needs backward computation.
I0124 14:23:26.211577 31536 net.cpp:226] fc6 needs backward computation.
I0124 14:23:26.211582 31536 net.cpp:226] pool5 needs backward computation.
I0124 14:23:26.211585 31536 net.cpp:226] relu5 needs backward computation.
I0124 14:23:26.211590 31536 net.cpp:226] conv5 needs backward computation.
I0124 14:23:26.211593 31536 net.cpp:226] relu4 needs backward computation.
I0124 14:23:26.211596 31536 net.cpp:226] conv4 needs backward computation.
I0124 14:23:26.211601 31536 net.cpp:226] relu3 needs backward computation.
I0124 14:23:26.211604 31536 net.cpp:226] conv3 needs backward computation.
I0124 14:23:26.211608 31536 net.cpp:226] pool2 needs backward computation.
I0124 14:23:26.211612 31536 net.cpp:226] relu2 needs backward computation.
I0124 14:23:26.211616 31536 net.cpp:226] conv2 needs backward computation.
I0124 14:23:26.211619 31536 net.cpp:226] pool1 needs backward computation.
I0124 14:23:26.211622 31536 net.cpp:226] relu1 needs backward computation.
I0124 14:23:26.211627 31536 net.cpp:226] conv1 needs backward computation.
I0124 14:23:26.211630 31536 net.cpp:228] label_data_1_split does not need backward computation.
I0124 14:23:26.211635 31536 net.cpp:228] data does not need backward computation.
I0124 14:23:26.211639 31536 net.cpp:270] This network produces output accuracy
I0124 14:23:26.211644 31536 net.cpp:270] This network produces output loss
I0124 14:23:26.211663 31536 net.cpp:283] Network initialization done.
I0124 14:23:26.211765 31536 solver.cpp:60] Solver scaffolding done.
I0124 14:23:26.212640 31536 caffe.cpp:128] Finetuning from ./caffenet_lsuv_rmsprop_095_lr001.prototxt.caffemodel
I0124 14:23:26.495707 31536 caffe.cpp:212] Starting Optimization
I0124 14:23:26.495735 31536 solver.cpp:288] Solving CaffeNet
I0124 14:23:26.495739 31536 solver.cpp:289] Learning Rate Policy: step
I0124 14:23:26.546339 31536 solver.cpp:237] Iteration 0, loss = 7.3986
I0124 14:23:26.546377 31536 solver.cpp:253]     Train net output #0: loss = 7.3986 (* 1 = 7.3986 loss)
I0124 14:23:26.546397 31536 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0124 14:23:26.638941 31536 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 14:23:33.433430 31536 solver.cpp:237] Iteration 20, loss = 27.0068
I0124 14:23:33.433466 31536 solver.cpp:253]     Train net output #0: loss = 6.93278 (* 1 = 6.93278 loss)
I0124 14:23:33.433476 31536 sgd_solver.cpp:106] Iteration 20, lr = 0.01
I0124 14:23:40.908213 31536 solver.cpp:237] Iteration 40, loss = 14.9706
I0124 14:23:40.908247 31536 solver.cpp:253]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0124 14:23:40.908368 31536 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I0124 14:23:48.635073 31536 solver.cpp:237] Iteration 60, loss = 14.5181
I0124 14:23:48.635107 31536 solver.cpp:253]     Train net output #0: loss = 6.91875 (* 1 = 6.91875 loss)
I0124 14:23:48.635115 31536 sgd_solver.cpp:106] Iteration 60, lr = 0.01
I0124 14:23:56.174072 31536 solver.cpp:237] Iteration 80, loss = 11.646
I0124 14:23:56.174142 31536 solver.cpp:253]     Train net output #0: loss = 6.93466 (* 1 = 6.93466 loss)
I0124 14:23:56.174152 31536 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I0124 14:24:03.599442 31536 solver.cpp:237] Iteration 100, loss = 34.9502
I0124 14:24:03.599479 31536 solver.cpp:253]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0124 14:24:03.599491 31536 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I0124 14:24:11.075537 31536 solver.cpp:237] Iteration 120, loss = 57.6049
I0124 14:24:11.075572 31536 solver.cpp:253]     Train net output #0: loss = 6.92092 (* 1 = 6.92092 loss)
I0124 14:24:11.075582 31536 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I0124 14:24:18.628129 31536 solver.cpp:237] Iteration 140, loss = 17.1431
I0124 14:24:18.628167 31536 solver.cpp:253]     Train net output #0: loss = 6.91485 (* 1 = 6.91485 loss)
I0124 14:24:18.628176 31536 sgd_solver.cpp:106] Iteration 140, lr = 0.01
I0124 14:24:26.302116 31536 solver.cpp:237] Iteration 160, loss = 17.7784
I0124 14:24:26.302182 31536 solver.cpp:253]     Train net output #0: loss = 9.40993 (* 1 = 9.40993 loss)
I0124 14:24:26.302193 31536 sgd_solver.cpp:106] Iteration 160, lr = 0.01
I0124 14:24:34.167325 31536 solver.cpp:237] Iteration 180, loss = 7.3024
I0124 14:24:34.167409 31536 solver.cpp:253]     Train net output #0: loss = 6.92553 (* 1 = 6.92553 loss)
I0124 14:24:34.167431 31536 sgd_solver.cpp:106] Iteration 180, lr = 0.01
I0124 14:24:42.029202 31536 solver.cpp:237] Iteration 200, loss = 7.16765
I0124 14:24:42.029233 31536 solver.cpp:253]     Train net output #0: loss = 6.90847 (* 1 = 6.90847 loss)
I0124 14:24:42.029239 31536 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I0124 14:24:49.602576 31536 solver.cpp:237] Iteration 220, loss = 6.96233
I0124 14:24:49.602605 31536 solver.cpp:253]     Train net output #0: loss = 6.9184 (* 1 = 6.9184 loss)
I0124 14:24:49.602612 31536 sgd_solver.cpp:106] Iteration 220, lr = 0.01
I0124 14:24:57.133332 31536 solver.cpp:237] Iteration 240, loss = 42.0973
I0124 14:24:57.133463 31536 solver.cpp:253]     Train net output #0: loss = 6.92012 (* 1 = 6.92012 loss)
I0124 14:24:57.133496 31536 sgd_solver.cpp:106] Iteration 240, lr = 0.01
I0124 14:25:04.778306 31536 solver.cpp:237] Iteration 260, loss = 6.92214
I0124 14:25:04.778347 31536 solver.cpp:253]     Train net output #0: loss = 6.90867 (* 1 = 6.90867 loss)
I0124 14:25:04.778357 31536 sgd_solver.cpp:106] Iteration 260, lr = 0.01
I0124 14:25:12.332052 31536 solver.cpp:237] Iteration 280, loss = 6.91987
I0124 14:25:12.332087 31536 solver.cpp:253]     Train net output #0: loss = 6.92346 (* 1 = 6.92346 loss)
I0124 14:25:12.332096 31536 sgd_solver.cpp:106] Iteration 280, lr = 0.01
I0124 14:25:19.776765 31536 solver.cpp:237] Iteration 300, loss = 17.855
I0124 14:25:19.776799 31536 solver.cpp:253]     Train net output #0: loss = 6.9196 (* 1 = 6.9196 loss)
I0124 14:25:19.776808 31536 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I0124 14:25:27.190855 31536 solver.cpp:237] Iteration 320, loss = 6.92146
I0124 14:25:27.191002 31536 solver.cpp:253]     Train net output #0: loss = 6.92681 (* 1 = 6.92681 loss)
I0124 14:25:27.191033 31536 sgd_solver.cpp:106] Iteration 320, lr = 0.01
I0124 14:25:35.047943 31536 solver.cpp:237] Iteration 340, loss = 37.4352
I0124 14:25:35.047977 31536 solver.cpp:253]     Train net output #0: loss = 68.4243 (* 1 = 68.4243 loss)
I0124 14:25:35.047987 31536 sgd_solver.cpp:106] Iteration 340, lr = 0.01
I0124 14:25:42.807291 31536 solver.cpp:237] Iteration 360, loss = 25.8865
I0124 14:25:42.807325 31536 solver.cpp:253]     Train net output #0: loss = 6.93192 (* 1 = 6.93192 loss)
I0124 14:25:42.807334 31536 sgd_solver.cpp:106] Iteration 360, lr = 0.01
I0124 14:25:50.563992 31536 solver.cpp:237] Iteration 380, loss = 10.9402
I0124 14:25:50.564080 31536 solver.cpp:253]     Train net output #0: loss = 6.92057 (* 1 = 6.92057 loss)
I0124 14:25:50.564105 31536 sgd_solver.cpp:106] Iteration 380, lr = 0.01
I0124 14:25:58.112354 31536 solver.cpp:237] Iteration 400, loss = 32.1932
I0124 14:25:58.112475 31536 solver.cpp:253]     Train net output #0: loss = 6.92649 (* 1 = 6.92649 loss)
I0124 14:25:58.112488 31536 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I0124 14:26:05.732470 31536 solver.cpp:237] Iteration 420, loss = 11.827
I0124 14:26:05.732503 31536 solver.cpp:253]     Train net output #0: loss = 7.17359 (* 1 = 7.17359 loss)
I0124 14:26:05.732513 31536 sgd_solver.cpp:106] Iteration 420, lr = 0.01
I0124 14:26:13.344265 31536 solver.cpp:237] Iteration 440, loss = 19.5298
I0124 14:26:13.344349 31536 solver.cpp:253]     Train net output #0: loss = 39.0992 (* 1 = 39.0992 loss)
I0124 14:26:13.344360 31536 sgd_solver.cpp:106] Iteration 440, lr = 0.01
I0124 14:26:20.910009 31536 solver.cpp:237] Iteration 460, loss = 16.6736
I0124 14:26:20.910043 31536 solver.cpp:253]     Train net output #0: loss = 6.92713 (* 1 = 6.92713 loss)
I0124 14:26:20.910051 31536 sgd_solver.cpp:106] Iteration 460, lr = 0.01
I0124 14:26:28.515501 31536 solver.cpp:237] Iteration 480, loss = 8.23734
I0124 14:26:28.515583 31536 solver.cpp:253]     Train net output #0: loss = 6.9218 (* 1 = 6.9218 loss)
I0124 14:26:28.515658 31536 sgd_solver.cpp:106] Iteration 480, lr = 0.01
I0124 14:26:36.073236 31536 solver.cpp:237] Iteration 500, loss = 29.9255
I0124 14:26:36.073272 31536 solver.cpp:253]     Train net output #0: loss = 6.91572 (* 1 = 6.91572 loss)
I0124 14:26:36.073282 31536 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I0124 14:26:43.553180 31536 solver.cpp:237] Iteration 520, loss = 6.9241
I0124 14:26:43.553215 31536 solver.cpp:253]     Train net output #0: loss = 6.91921 (* 1 = 6.91921 loss)
I0124 14:26:43.553225 31536 sgd_solver.cpp:106] Iteration 520, lr = 0.01
I0124 14:26:51.075316 31536 solver.cpp:237] Iteration 540, loss = 17.0991
I0124 14:26:51.075361 31536 solver.cpp:253]     Train net output #0: loss = 6.92017 (* 1 = 6.92017 loss)
I0124 14:26:51.075371 31536 sgd_solver.cpp:106] Iteration 540, lr = 0.01
I0124 14:26:58.939795 31536 solver.cpp:237] Iteration 560, loss = 6.96322
I0124 14:26:58.939872 31536 solver.cpp:253]     Train net output #0: loss = 6.91549 (* 1 = 6.91549 loss)
I0124 14:26:58.939936 31536 sgd_solver.cpp:106] Iteration 560, lr = 0.01
I0124 14:27:07.042821 31536 solver.cpp:237] Iteration 580, loss = 34.386
I0124 14:27:07.042857 31536 solver.cpp:253]     Train net output #0: loss = 6.90284 (* 1 = 6.90284 loss)
I0124 14:27:07.042867 31536 sgd_solver.cpp:106] Iteration 580, lr = 0.01
I0124 14:27:14.577102 31536 solver.cpp:237] Iteration 600, loss = 6.93475
I0124 14:27:14.577147 31536 solver.cpp:253]     Train net output #0: loss = 6.90706 (* 1 = 6.90706 loss)
I0124 14:27:14.577157 31536 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I0124 14:27:22.345840 31536 solver.cpp:237] Iteration 620, loss = 18.5466
I0124 14:27:22.345881 31536 solver.cpp:253]     Train net output #0: loss = 6.92998 (* 1 = 6.92998 loss)
I0124 14:27:22.345892 31536 sgd_solver.cpp:106] Iteration 620, lr = 0.01
I0124 14:27:29.962093 31536 solver.cpp:237] Iteration 640, loss = 36.0346
I0124 14:27:29.962173 31536 solver.cpp:253]     Train net output #0: loss = 6.97029 (* 1 = 6.97029 loss)
I0124 14:27:29.962184 31536 sgd_solver.cpp:106] Iteration 640, lr = 0.01
I0124 14:27:37.467686 31536 solver.cpp:237] Iteration 660, loss = 33.4911
I0124 14:27:37.467725 31536 solver.cpp:253]     Train net output #0: loss = 6.93515 (* 1 = 6.93515 loss)
I0124 14:27:37.467736 31536 sgd_solver.cpp:106] Iteration 660, lr = 0.01
I0124 14:27:45.172062 31536 solver.cpp:237] Iteration 680, loss = 12.9501
I0124 14:27:45.172096 31536 solver.cpp:253]     Train net output #0: loss = 6.90909 (* 1 = 6.90909 loss)
I0124 14:27:45.172106 31536 sgd_solver.cpp:106] Iteration 680, lr = 0.01
I0124 14:27:52.811733 31536 solver.cpp:237] Iteration 700, loss = 33.2188
I0124 14:27:52.811846 31536 solver.cpp:253]     Train net output #0: loss = 15.9594 (* 1 = 15.9594 loss)
I0124 14:27:52.811885 31536 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I0124 14:28:00.630477 31536 solver.cpp:237] Iteration 720, loss = 27.5508
I0124 14:28:00.633838 31536 solver.cpp:253]     Train net output #0: loss = 6.91147 (* 1 = 6.91147 loss)
I0124 14:28:00.633862 31536 sgd_solver.cpp:106] Iteration 720, lr = 0.01
I0124 14:28:08.591106 31536 solver.cpp:237] Iteration 740, loss = 61.3882
I0124 14:28:08.591159 31536 solver.cpp:253]     Train net output #0: loss = 74.6344 (* 1 = 74.6344 loss)
I0124 14:28:08.591171 31536 sgd_solver.cpp:106] Iteration 740, lr = 0.01
I0124 14:28:16.335268 31536 solver.cpp:237] Iteration 760, loss = 33.7212
I0124 14:28:16.335305 31536 solver.cpp:253]     Train net output #0: loss = 24.0301 (* 1 = 24.0301 loss)
I0124 14:28:16.335314 31536 sgd_solver.cpp:106] Iteration 760, lr = 0.01
I0124 14:28:24.421687 31536 solver.cpp:237] Iteration 780, loss = 12.6348
I0124 14:28:24.421730 31536 solver.cpp:253]     Train net output #0: loss = 6.9779 (* 1 = 6.9779 loss)
I0124 14:28:24.421741 31536 sgd_solver.cpp:106] Iteration 780, lr = 0.01
I0124 14:28:32.070741 31536 solver.cpp:237] Iteration 800, loss = 11.1949
I0124 14:28:32.070986 31536 solver.cpp:253]     Train net output #0: loss = 6.92717 (* 1 = 6.92717 loss)
I0124 14:28:32.071001 31536 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I0124 14:28:39.611814 31536 solver.cpp:237] Iteration 820, loss = 7.17169
I0124 14:28:39.611851 31536 solver.cpp:253]     Train net output #0: loss = 6.93449 (* 1 = 6.93449 loss)
I0124 14:28:39.611868 31536 sgd_solver.cpp:106] Iteration 820, lr = 0.01
I0124 14:28:47.427459 31536 solver.cpp:237] Iteration 840, loss = 7.58536
I0124 14:28:47.427492 31536 solver.cpp:253]     Train net output #0: loss = 6.93389 (* 1 = 6.93389 loss)
I0124 14:28:47.427503 31536 sgd_solver.cpp:106] Iteration 840, lr = 0.01
I0124 14:28:55.199314 31536 solver.cpp:237] Iteration 860, loss = 40.5877
I0124 14:28:55.199352 31536 solver.cpp:253]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0124 14:28:55.199362 31536 sgd_solver.cpp:106] Iteration 860, lr = 0.01
I0124 14:29:03.001898 31536 solver.cpp:237] Iteration 880, loss = 60.3516
I0124 14:29:03.001971 31536 solver.cpp:253]     Train net output #0: loss = 6.96597 (* 1 = 6.96597 loss)
I0124 14:29:03.001982 31536 sgd_solver.cpp:106] Iteration 880, lr = 0.01
I0124 14:29:10.720182 31536 solver.cpp:237] Iteration 900, loss = 11.5928
I0124 14:29:10.720217 31536 solver.cpp:253]     Train net output #0: loss = 6.94498 (* 1 = 6.94498 loss)
I0124 14:29:10.720227 31536 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I0124 14:29:18.883494 31536 solver.cpp:237] Iteration 920, loss = 11.8972
I0124 14:29:18.883668 31536 solver.cpp:253]     Train net output #0: loss = 7.69764 (* 1 = 7.69764 loss)
I0124 14:29:18.883744 31536 sgd_solver.cpp:106] Iteration 920, lr = 0.01
I0124 14:29:27.032196 31536 solver.cpp:237] Iteration 940, loss = 19.2666
I0124 14:29:27.032230 31536 solver.cpp:253]     Train net output #0: loss = 8.23942 (* 1 = 8.23942 loss)
I0124 14:29:27.032239 31536 sgd_solver.cpp:106] Iteration 940, lr = 0.01
I0124 14:29:34.852849 31536 solver.cpp:237] Iteration 960, loss = 21.5579
I0124 14:29:34.853770 31536 solver.cpp:253]     Train net output #0: loss = 15.2428 (* 1 = 15.2428 loss)
I0124 14:29:34.853796 31536 sgd_solver.cpp:106] Iteration 960, lr = 0.01
I0124 14:29:42.787626 31536 solver.cpp:237] Iteration 980, loss = 39.8561
I0124 14:29:42.787662 31536 solver.cpp:253]     Train net output #0: loss = 78.5125 (* 1 = 78.5125 loss)
I0124 14:29:42.787672 31536 sgd_solver.cpp:106] Iteration 980, lr = 0.01
I0124 14:29:50.468431 31536 solver.cpp:237] Iteration 1000, loss = 65.9771
I0124 14:29:50.468471 31536 solver.cpp:253]     Train net output #0: loss = 17.5998 (* 1 = 17.5998 loss)
I0124 14:29:50.468480 31536 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I0124 14:29:50.852104 31536 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 14:29:58.327433 31536 solver.cpp:237] Iteration 1020, loss = 11.1042
I0124 14:29:58.327474 31536 solver.cpp:253]     Train net output #0: loss = 14.1254 (* 1 = 14.1254 loss)
I0124 14:29:58.327486 31536 sgd_solver.cpp:106] Iteration 1020, lr = 0.01
I0124 14:30:06.153254 31536 solver.cpp:237] Iteration 1040, loss = 21.5529
I0124 14:30:06.153350 31536 solver.cpp:253]     Train net output #0: loss = 12.3226 (* 1 = 12.3226 loss)
I0124 14:30:06.153362 31536 sgd_solver.cpp:106] Iteration 1040, lr = 0.01
I0124 14:30:14.074343 31536 solver.cpp:237] Iteration 1060, loss = 14.7055
I0124 14:30:14.074379 31536 solver.cpp:253]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0124 14:30:14.074390 31536 sgd_solver.cpp:106] Iteration 1060, lr = 0.01
I0124 14:30:22.189615 31536 solver.cpp:237] Iteration 1080, loss = 71.8717
I0124 14:30:22.189703 31536 solver.cpp:253]     Train net output #0: loss = 50.0401 (* 1 = 50.0401 loss)
I0124 14:30:22.189729 31536 sgd_solver.cpp:106] Iteration 1080, lr = 0.01
I0124 14:30:30.240861 31536 solver.cpp:237] Iteration 1100, loss = 10.8072
I0124 14:30:30.240910 31536 solver.cpp:253]     Train net output #0: loss = 6.96221 (* 1 = 6.96221 loss)
I0124 14:30:30.240922 31536 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I0124 14:30:38.557953 31536 solver.cpp:237] Iteration 1120, loss = 11.8606
I0124 14:30:38.558295 31536 solver.cpp:253]     Train net output #0: loss = 7.11036 (* 1 = 7.11036 loss)
I0124 14:30:38.558317 31536 sgd_solver.cpp:106] Iteration 1120, lr = 0.01
I0124 14:30:46.421761 31536 solver.cpp:237] Iteration 1140, loss = 13.8715
I0124 14:30:46.421792 31536 solver.cpp:253]     Train net output #0: loss = 6.92997 (* 1 = 6.92997 loss)
I0124 14:30:46.421799 31536 sgd_solver.cpp:106] Iteration 1140, lr = 0.01
I0124 14:30:54.468904 31536 solver.cpp:237] Iteration 1160, loss = 32.91
I0124 14:30:54.468943 31536 solver.cpp:253]     Train net output #0: loss = 6.9017 (* 1 = 6.9017 loss)
I0124 14:30:54.468955 31536 sgd_solver.cpp:106] Iteration 1160, lr = 0.01
I0124 14:31:02.544875 31536 solver.cpp:237] Iteration 1180, loss = 46.7578
I0124 14:31:02.544909 31536 solver.cpp:253]     Train net output #0: loss = 10.4906 (* 1 = 10.4906 loss)
I0124 14:31:02.544921 31536 sgd_solver.cpp:106] Iteration 1180, lr = 0.01
I0124 14:31:10.088657 31536 solver.cpp:237] Iteration 1200, loss = 10.8908
I0124 14:31:10.088735 31536 solver.cpp:253]     Train net output #0: loss = 7.02409 (* 1 = 7.02409 loss)
I0124 14:31:10.088747 31536 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I0124 14:31:18.034116 31536 solver.cpp:237] Iteration 1220, loss = 19.9133
I0124 14:31:18.034212 31536 solver.cpp:253]     Train net output #0: loss = 64.0908 (* 1 = 64.0908 loss)
I0124 14:31:18.034245 31536 sgd_solver.cpp:106] Iteration 1220, lr = 0.01
I0124 14:31:26.308729 31536 solver.cpp:237] Iteration 1240, loss = 21.0287
I0124 14:31:26.308828 31536 solver.cpp:253]     Train net output #0: loss = 7.00158 (* 1 = 7.00158 loss)
I0124 14:31:26.308869 31536 sgd_solver.cpp:106] Iteration 1240, lr = 0.01
I0124 14:31:33.883699 31536 solver.cpp:237] Iteration 1260, loss = 8.99226
I0124 14:31:33.883731 31536 solver.cpp:253]     Train net output #0: loss = 7.70041 (* 1 = 7.70041 loss)
I0124 14:31:33.883740 31536 sgd_solver.cpp:106] Iteration 1260, lr = 0.01
I0124 14:31:41.524157 31536 solver.cpp:237] Iteration 1280, loss = 20.9746
I0124 14:31:41.524301 31536 solver.cpp:253]     Train net output #0: loss = 28.5779 (* 1 = 28.5779 loss)
I0124 14:31:41.524335 31536 sgd_solver.cpp:106] Iteration 1280, lr = 0.01
I0124 14:31:49.311455 31536 solver.cpp:237] Iteration 1300, loss = 14.4842
I0124 14:31:49.311542 31536 solver.cpp:253]     Train net output #0: loss = 18.1419 (* 1 = 18.1419 loss)
I0124 14:31:49.311568 31536 sgd_solver.cpp:106] Iteration 1300, lr = 0.01
I0124 14:31:56.984447 31536 solver.cpp:237] Iteration 1320, loss = 28.0015
I0124 14:31:56.984479 31536 solver.cpp:253]     Train net output #0: loss = 7.34214 (* 1 = 7.34214 loss)
I0124 14:31:56.984488 31536 sgd_solver.cpp:106] Iteration 1320, lr = 0.01
I0124 14:32:04.653720 31536 solver.cpp:237] Iteration 1340, loss = 16.245
I0124 14:32:04.653756 31536 solver.cpp:253]     Train net output #0: loss = 6.91632 (* 1 = 6.91632 loss)
I0124 14:32:04.653841 31536 sgd_solver.cpp:106] Iteration 1340, lr = 0.01
I0124 14:32:12.440068 31536 solver.cpp:237] Iteration 1360, loss = 39.5674
I0124 14:32:12.440177 31536 solver.cpp:253]     Train net output #0: loss = 73.2347 (* 1 = 73.2347 loss)
I0124 14:32:12.440250 31536 sgd_solver.cpp:106] Iteration 1360, lr = 0.01
I0124 14:32:20.364635 31536 solver.cpp:237] Iteration 1380, loss = 19.1848
I0124 14:32:20.364727 31536 solver.cpp:253]     Train net output #0: loss = 6.98463 (* 1 = 6.98463 loss)
I0124 14:32:20.364753 31536 sgd_solver.cpp:106] Iteration 1380, lr = 0.01
I0124 14:32:28.220310 31536 solver.cpp:237] Iteration 1400, loss = 21.7148
I0124 14:32:28.220405 31536 solver.cpp:253]     Train net output #0: loss = 32.7284 (* 1 = 32.7284 loss)
I0124 14:32:28.220438 31536 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I0124 14:32:36.036217 31536 solver.cpp:237] Iteration 1420, loss = 56.2536
I0124 14:32:36.036311 31536 solver.cpp:253]     Train net output #0: loss = 47.9087 (* 1 = 47.9087 loss)
I0124 14:32:36.036336 31536 sgd_solver.cpp:106] Iteration 1420, lr = 0.01
I0124 14:32:43.720366 31536 solver.cpp:237] Iteration 1440, loss = 20.9847
I0124 14:32:43.720499 31536 solver.cpp:253]     Train net output #0: loss = 22.3414 (* 1 = 22.3414 loss)
I0124 14:32:43.720530 31536 sgd_solver.cpp:106] Iteration 1440, lr = 0.01
I0124 14:32:51.478322 31536 solver.cpp:237] Iteration 1460, loss = 13.7815
I0124 14:32:51.478363 31536 solver.cpp:253]     Train net output #0: loss = 7.14273 (* 1 = 7.14273 loss)
I0124 14:32:51.478371 31536 sgd_solver.cpp:106] Iteration 1460, lr = 0.01
I0124 14:32:59.198941 31536 solver.cpp:237] Iteration 1480, loss = 18.1355
I0124 14:32:59.198982 31536 solver.cpp:253]     Train net output #0: loss = 16.6043 (* 1 = 16.6043 loss)
I0124 14:32:59.198992 31536 sgd_solver.cpp:106] Iteration 1480, lr = 0.01
I0124 14:33:07.001809 31536 solver.cpp:237] Iteration 1500, loss = 36.7342
I0124 14:33:07.001850 31536 solver.cpp:253]     Train net output #0: loss = 9.73132 (* 1 = 9.73132 loss)
I0124 14:33:07.001860 31536 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I0124 14:33:14.746904 31536 solver.cpp:237] Iteration 1520, loss = 19.7899
I0124 14:33:14.746980 31536 solver.cpp:253]     Train net output #0: loss = 7.0477 (* 1 = 7.0477 loss)
I0124 14:33:14.746991 31536 sgd_solver.cpp:106] Iteration 1520, lr = 0.01
I0124 14:33:22.438433 31536 solver.cpp:237] Iteration 1540, loss = 15.8415
I0124 14:33:22.438467 31536 solver.cpp:253]     Train net output #0: loss = 10.1263 (* 1 = 10.1263 loss)
I0124 14:33:22.438478 31536 sgd_solver.cpp:106] Iteration 1540, lr = 0.01
I0124 14:33:30.094434 31536 solver.cpp:237] Iteration 1560, loss = 37.1226
I0124 14:33:30.094473 31536 solver.cpp:253]     Train net output #0: loss = 57.2955 (* 1 = 57.2955 loss)
I0124 14:33:30.094483 31536 sgd_solver.cpp:106] Iteration 1560, lr = 0.01
I0124 14:33:37.576158 31536 solver.cpp:237] Iteration 1580, loss = 25.9126
I0124 14:33:37.576254 31536 solver.cpp:253]     Train net output #0: loss = 7.18268 (* 1 = 7.18268 loss)
I0124 14:33:37.576287 31536 sgd_solver.cpp:106] Iteration 1580, lr = 0.01
I0124 14:33:45.065089 31536 solver.cpp:237] Iteration 1600, loss = 43.4135
I0124 14:33:45.065163 31536 solver.cpp:253]     Train net output #0: loss = 10.6227 (* 1 = 10.6227 loss)
I0124 14:33:45.065174 31536 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I0124 14:33:52.752502 31536 solver.cpp:237] Iteration 1620, loss = 21.8182
I0124 14:33:52.752537 31536 solver.cpp:253]     Train net output #0: loss = 13.8756 (* 1 = 13.8756 loss)
I0124 14:33:52.752547 31536 sgd_solver.cpp:106] Iteration 1620, lr = 0.01
I0124 14:34:00.551348 31536 solver.cpp:237] Iteration 1640, loss = 32.5791
I0124 14:34:00.551443 31536 solver.cpp:253]     Train net output #0: loss = 10.0448 (* 1 = 10.0448 loss)
I0124 14:34:00.551470 31536 sgd_solver.cpp:106] Iteration 1640, lr = 0.01
I0124 14:34:08.026151 31536 solver.cpp:237] Iteration 1660, loss = 14.9047
I0124 14:34:08.026188 31536 solver.cpp:253]     Train net output #0: loss = 17.9459 (* 1 = 17.9459 loss)
I0124 14:34:08.026199 31536 sgd_solver.cpp:106] Iteration 1660, lr = 0.01
I0124 14:34:15.613597 31536 solver.cpp:237] Iteration 1680, loss = 34.6511
I0124 14:34:15.613685 31536 solver.cpp:253]     Train net output #0: loss = 8.12995 (* 1 = 8.12995 loss)
I0124 14:34:15.613695 31536 sgd_solver.cpp:106] Iteration 1680, lr = 0.01
I0124 14:34:23.324996 31536 solver.cpp:237] Iteration 1700, loss = 24.918
I0124 14:34:23.325031 31536 solver.cpp:253]     Train net output #0: loss = 56.4301 (* 1 = 56.4301 loss)
I0124 14:34:23.325042 31536 sgd_solver.cpp:106] Iteration 1700, lr = 0.01
I0124 14:34:30.940706 31536 solver.cpp:237] Iteration 1720, loss = 16.2538
I0124 14:34:30.940740 31536 solver.cpp:253]     Train net output #0: loss = 31.6879 (* 1 = 31.6879 loss)
I0124 14:34:30.940752 31536 sgd_solver.cpp:106] Iteration 1720, lr = 0.01
I0124 14:34:38.701640 31536 solver.cpp:237] Iteration 1740, loss = 32.2626
I0124 14:34:38.701743 31536 solver.cpp:253]     Train net output #0: loss = 6.98249 (* 1 = 6.98249 loss)
I0124 14:34:38.701766 31536 sgd_solver.cpp:106] Iteration 1740, lr = 0.01
I0124 14:34:46.497052 31536 solver.cpp:237] Iteration 1760, loss = 14.8231
I0124 14:34:46.497128 31536 solver.cpp:253]     Train net output #0: loss = 6.93196 (* 1 = 6.93196 loss)
I0124 14:34:46.497138 31536 sgd_solver.cpp:106] Iteration 1760, lr = 0.01
I0124 14:34:54.209919 31536 solver.cpp:237] Iteration 1780, loss = 34.5152
I0124 14:34:54.210012 31536 solver.cpp:253]     Train net output #0: loss = 86.5523 (* 1 = 86.5523 loss)
I0124 14:34:54.210037 31536 sgd_solver.cpp:106] Iteration 1780, lr = 0.01
I0124 14:35:01.959779 31536 solver.cpp:237] Iteration 1800, loss = 34.6274
I0124 14:35:01.959820 31536 solver.cpp:253]     Train net output #0: loss = 22.2729 (* 1 = 22.2729 loss)
I0124 14:35:01.959830 31536 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I0124 14:35:09.666069 31536 solver.cpp:237] Iteration 1820, loss = 18.1542
I0124 14:35:09.666107 31536 solver.cpp:253]     Train net output #0: loss = 40.6445 (* 1 = 40.6445 loss)
I0124 14:35:09.666118 31536 sgd_solver.cpp:106] Iteration 1820, lr = 0.01
I0124 14:35:17.300943 31536 solver.cpp:237] Iteration 1840, loss = 27.053
I0124 14:35:17.301033 31536 solver.cpp:253]     Train net output #0: loss = 12.328 (* 1 = 12.328 loss)
I0124 14:35:17.301044 31536 sgd_solver.cpp:106] Iteration 1840, lr = 0.01
I0124 14:35:25.975486 31536 solver.cpp:237] Iteration 1860, loss = 21.7008
I0124 14:35:25.975522 31536 solver.cpp:253]     Train net output #0: loss = 12.2741 (* 1 = 12.2741 loss)
I0124 14:35:25.975531 31536 sgd_solver.cpp:106] Iteration 1860, lr = 0.01
I0124 14:35:34.241477 31536 solver.cpp:237] Iteration 1880, loss = 22.1187
I0124 14:35:34.241513 31536 solver.cpp:253]     Train net output #0: loss = 36.4837 (* 1 = 36.4837 loss)
I0124 14:35:34.241523 31536 sgd_solver.cpp:106] Iteration 1880, lr = 0.01
I0124 14:35:42.718075 31536 solver.cpp:237] Iteration 1900, loss = 11.7019
I0124 14:35:42.718112 31536 solver.cpp:253]     Train net output #0: loss = 10.2719 (* 1 = 10.2719 loss)
I0124 14:35:42.718124 31536 sgd_solver.cpp:106] Iteration 1900, lr = 0.01
I0124 14:35:50.559226 31536 solver.cpp:237] Iteration 1920, loss = 24.1403
I0124 14:35:50.559301 31536 solver.cpp:253]     Train net output #0: loss = 6.95141 (* 1 = 6.95141 loss)
I0124 14:35:50.559311 31536 sgd_solver.cpp:106] Iteration 1920, lr = 0.01
I0124 14:35:58.246150 31536 solver.cpp:237] Iteration 1940, loss = 17.9032
I0124 14:35:58.246333 31536 solver.cpp:253]     Train net output #0: loss = 13.4208 (* 1 = 13.4208 loss)
I0124 14:35:58.246402 31536 sgd_solver.cpp:106] Iteration 1940, lr = 0.01
I0124 14:36:05.814090 31536 solver.cpp:237] Iteration 1960, loss = 12.8118
I0124 14:36:05.814126 31536 solver.cpp:253]     Train net output #0: loss = 7.68371 (* 1 = 7.68371 loss)
I0124 14:36:05.814139 31536 sgd_solver.cpp:106] Iteration 1960, lr = 0.01
I0124 14:36:13.311858 31536 solver.cpp:237] Iteration 1980, loss = 7.41714
I0124 14:36:13.311892 31536 solver.cpp:253]     Train net output #0: loss = 6.93343 (* 1 = 6.93343 loss)
I0124 14:36:13.311902 31536 sgd_solver.cpp:106] Iteration 1980, lr = 0.01
I0124 14:36:20.547675 31536 solver.cpp:341] Iteration 2000, Testing net (#0)
I0124 14:36:21.457288 31536 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 14:37:36.563305 31536 solver.cpp:409]     Test net output #0: accuracy = 0.001
I0124 14:37:36.563380 31536 solver.cpp:409]     Test net output #1: loss = 64.8491 (* 1 = 64.8491 loss)
I0124 14:37:36.600172 31536 solver.cpp:237] Iteration 2000, loss = 25.0649
I0124 14:37:36.600209 31536 solver.cpp:253]     Train net output #0: loss = 50.1927 (* 1 = 50.1927 loss)
I0124 14:37:36.600222 31536 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I0124 14:37:44.052814 31536 solver.cpp:237] Iteration 2020, loss = 19.0641
I0124 14:37:44.052855 31536 solver.cpp:253]     Train net output #0: loss = 7.46116 (* 1 = 7.46116 loss)
I0124 14:37:44.052867 31536 sgd_solver.cpp:106] Iteration 2020, lr = 0.01
I0124 14:37:51.574378 31536 solver.cpp:237] Iteration 2040, loss = 19.1062
I0124 14:37:51.574411 31536 solver.cpp:253]     Train net output #0: loss = 7.06354 (* 1 = 7.06354 loss)
I0124 14:37:51.574422 31536 sgd_solver.cpp:106] Iteration 2040, lr = 0.01
I0124 14:37:59.197422 31536 solver.cpp:237] Iteration 2060, loss = 13.2617
I0124 14:37:59.197618 31536 solver.cpp:253]     Train net output #0: loss = 64.5126 (* 1 = 64.5126 loss)
I0124 14:37:59.197713 31536 sgd_solver.cpp:106] Iteration 2060, lr = 0.01
I0124 14:38:06.749003 31536 solver.cpp:237] Iteration 2080, loss = 41.8779
I0124 14:38:06.749083 31536 solver.cpp:253]     Train net output #0: loss = 33.0912 (* 1 = 33.0912 loss)
I0124 14:38:06.749094 31536 sgd_solver.cpp:106] Iteration 2080, lr = 0.01
I0124 14:38:14.350049 31536 solver.cpp:237] Iteration 2100, loss = 10.8323
I0124 14:38:14.350085 31536 solver.cpp:253]     Train net output #0: loss = 6.98392 (* 1 = 6.98392 loss)
I0124 14:38:14.350093 31536 sgd_solver.cpp:106] Iteration 2100, lr = 0.01
I0124 14:38:21.987810 31536 solver.cpp:237] Iteration 2120, loss = 34.6295
I0124 14:38:21.987844 31536 solver.cpp:253]     Train net output #0: loss = 8.29491 (* 1 = 8.29491 loss)
I0124 14:38:21.987854 31536 sgd_solver.cpp:106] Iteration 2120, lr = 0.01
I0124 14:38:29.578104 31536 solver.cpp:237] Iteration 2140, loss = 17.1904
I0124 14:38:29.578141 31536 solver.cpp:253]     Train net output #0: loss = 7.50184 (* 1 = 7.50184 loss)
I0124 14:38:29.578150 31536 sgd_solver.cpp:106] Iteration 2140, lr = 0.01
I0124 14:38:37.067663 31536 solver.cpp:237] Iteration 2160, loss = 11.0728
I0124 14:38:37.067750 31536 solver.cpp:253]     Train net output #0: loss = 72.008 (* 1 = 72.008 loss)
I0124 14:38:37.067764 31536 sgd_solver.cpp:106] Iteration 2160, lr = 0.01
I0124 14:38:44.553776 31536 solver.cpp:237] Iteration 2180, loss = 32.5632
I0124 14:38:44.553987 31536 solver.cpp:253]     Train net output #0: loss = 7.7114 (* 1 = 7.7114 loss)
I0124 14:38:44.554004 31536 sgd_solver.cpp:106] Iteration 2180, lr = 0.01
I0124 14:38:52.235183 31536 solver.cpp:237] Iteration 2200, loss = 21.9929
I0124 14:38:52.235216 31536 solver.cpp:253]     Train net output #0: loss = 52.7938 (* 1 = 52.7938 loss)
I0124 14:38:52.235225 31536 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I0124 14:38:59.885871 31536 solver.cpp:237] Iteration 2220, loss = 16.8392
I0124 14:38:59.885910 31536 solver.cpp:253]     Train net output #0: loss = 7.19081 (* 1 = 7.19081 loss)
I0124 14:38:59.885921 31536 sgd_solver.cpp:106] Iteration 2220, lr = 0.01
I0124 14:39:07.632390 31536 solver.cpp:237] Iteration 2240, loss = 47.593
I0124 14:39:07.632477 31536 solver.cpp:253]     Train net output #0: loss = 83.7122 (* 1 = 83.7122 loss)
I0124 14:39:07.632488 31536 sgd_solver.cpp:106] Iteration 2240, lr = 0.01
I0124 14:39:15.298099 31536 solver.cpp:237] Iteration 2260, loss = 25.6054
I0124 14:39:15.298138 31536 solver.cpp:253]     Train net output #0: loss = 10.4417 (* 1 = 10.4417 loss)
I0124 14:39:15.298148 31536 sgd_solver.cpp:106] Iteration 2260, lr = 0.01
I0124 14:39:22.915127 31536 solver.cpp:237] Iteration 2280, loss = 16.597
I0124 14:39:22.915168 31536 solver.cpp:253]     Train net output #0: loss = 40.7915 (* 1 = 40.7915 loss)
I0124 14:39:22.915177 31536 sgd_solver.cpp:106] Iteration 2280, lr = 0.01
I0124 14:39:30.747330 31536 solver.cpp:237] Iteration 2300, loss = 34.0356
I0124 14:39:30.747369 31536 solver.cpp:253]     Train net output #0: loss = 8.7764 (* 1 = 8.7764 loss)
I0124 14:39:30.747380 31536 sgd_solver.cpp:106] Iteration 2300, lr = 0.01
I0124 14:39:38.408942 31536 solver.cpp:237] Iteration 2320, loss = 42.0051
I0124 14:39:38.409212 31536 solver.cpp:253]     Train net output #0: loss = 9.47898 (* 1 = 9.47898 loss)
I0124 14:39:38.409286 31536 sgd_solver.cpp:106] Iteration 2320, lr = 0.01
I0124 14:39:45.911811 31536 solver.cpp:237] Iteration 2340, loss = 33.0509
I0124 14:39:45.911984 31536 solver.cpp:253]     Train net output #0: loss = 9.96131 (* 1 = 9.96131 loss)
I0124 14:39:45.912053 31536 sgd_solver.cpp:106] Iteration 2340, lr = 0.01
I0124 14:39:53.435161 31536 solver.cpp:237] Iteration 2360, loss = 23.925
I0124 14:39:53.435199 31536 solver.cpp:253]     Train net output #0: loss = 21.3409 (* 1 = 21.3409 loss)
I0124 14:39:53.435209 31536 sgd_solver.cpp:106] Iteration 2360, lr = 0.01
I0124 14:40:00.913007 31536 solver.cpp:237] Iteration 2380, loss = 30.0706
I0124 14:40:00.913044 31536 solver.cpp:253]     Train net output #0: loss = 74.4993 (* 1 = 74.4993 loss)
I0124 14:40:00.913055 31536 sgd_solver.cpp:106] Iteration 2380, lr = 0.01
I0124 14:40:08.386502 31536 solver.cpp:237] Iteration 2400, loss = 26.2781
I0124 14:40:08.386538 31536 solver.cpp:253]     Train net output #0: loss = 8.05377 (* 1 = 8.05377 loss)
I0124 14:40:08.386548 31536 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
I0124 14:40:15.995944 31536 solver.cpp:237] Iteration 2420, loss = 16.223
I0124 14:40:15.996143 31536 solver.cpp:253]     Train net output #0: loss = 7.85102 (* 1 = 7.85102 loss)
I0124 14:40:15.996155 31536 sgd_solver.cpp:106] Iteration 2420, lr = 0.01
I0124 14:40:23.556108 31536 solver.cpp:237] Iteration 2440, loss = 22.9109
I0124 14:40:23.556141 31536 solver.cpp:253]     Train net output #0: loss = 60.2255 (* 1 = 60.2255 loss)
I0124 14:40:23.556150 31536 sgd_solver.cpp:106] Iteration 2440, lr = 0.01
I0124 14:40:31.156963 31536 solver.cpp:237] Iteration 2460, loss = 18.9112
I0124 14:40:31.157047 31536 solver.cpp:253]     Train net output #0: loss = 7.73639 (* 1 = 7.73639 loss)
I0124 14:40:31.157058 31536 sgd_solver.cpp:106] Iteration 2460, lr = 0.01
I0124 14:40:38.765092 31536 solver.cpp:237] Iteration 2480, loss = 44.5489
I0124 14:40:38.765126 31536 solver.cpp:253]     Train net output #0: loss = 11.5977 (* 1 = 11.5977 loss)
I0124 14:40:38.765136 31536 sgd_solver.cpp:106] Iteration 2480, lr = 0.01
I0124 14:40:46.266396 31536 solver.cpp:237] Iteration 2500, loss = 11.8543
I0124 14:40:46.266475 31536 solver.cpp:253]     Train net output #0: loss = 17.4866 (* 1 = 17.4866 loss)
I0124 14:40:46.266486 31536 sgd_solver.cpp:106] Iteration 2500, lr = 0.01
I0124 14:40:54.216985 31536 solver.cpp:237] Iteration 2520, loss = 28.5599
I0124 14:40:54.217025 31536 solver.cpp:253]     Train net output #0: loss = 47.4335 (* 1 = 47.4335 loss)
I0124 14:40:54.217036 31536 sgd_solver.cpp:106] Iteration 2520, lr = 0.01
I0124 14:41:02.104750 31536 solver.cpp:237] Iteration 2540, loss = 16.7264
I0124 14:41:02.104787 31536 solver.cpp:253]     Train net output #0: loss = 18.0422 (* 1 = 18.0422 loss)
I0124 14:41:02.104799 31536 sgd_solver.cpp:106] Iteration 2540, lr = 0.01
I0124 14:41:09.874528 31536 solver.cpp:237] Iteration 2560, loss = 27.5546
I0124 14:41:09.874572 31536 solver.cpp:253]     Train net output #0: loss = 82.4318 (* 1 = 82.4318 loss)
I0124 14:41:09.874582 31536 sgd_solver.cpp:106] Iteration 2560, lr = 0.01
I0124 14:41:17.548846 31536 solver.cpp:237] Iteration 2580, loss = 16.3003
I0124 14:41:17.549068 31536 solver.cpp:253]     Train net output #0: loss = 7.12092 (* 1 = 7.12092 loss)
I0124 14:41:17.549144 31536 sgd_solver.cpp:106] Iteration 2580, lr = 0.01
I0124 14:41:25.352727 31536 solver.cpp:237] Iteration 2600, loss = 23.4138
I0124 14:41:25.352820 31536 solver.cpp:253]     Train net output #0: loss = 15.4087 (* 1 = 15.4087 loss)
I0124 14:41:25.352854 31536 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I0124 14:41:28.054152 31541 blocking_queue.cpp:50] Waiting for data
I0124 14:41:33.429743 31536 solver.cpp:237] Iteration 2620, loss = 20.2619
I0124 14:41:33.429777 31536 solver.cpp:253]     Train net output #0: loss = 12.2825 (* 1 = 12.2825 loss)
I0124 14:41:33.429853 31536 sgd_solver.cpp:106] Iteration 2620, lr = 0.01
I0124 14:41:41.378989 31536 solver.cpp:237] Iteration 2640, loss = 24.7594
I0124 14:41:41.379029 31536 solver.cpp:253]     Train net output #0: loss = 7.49429 (* 1 = 7.49429 loss)
I0124 14:41:41.379040 31536 sgd_solver.cpp:106] Iteration 2640, lr = 0.01
I0124 14:41:49.109580 31536 solver.cpp:237] Iteration 2660, loss = 18.6592
I0124 14:41:49.109673 31536 solver.cpp:253]     Train net output #0: loss = 10.5608 (* 1 = 10.5608 loss)
I0124 14:41:49.109791 31536 sgd_solver.cpp:106] Iteration 2660, lr = 0.01
I0124 14:41:56.774369 31536 solver.cpp:237] Iteration 2680, loss = 15.8676
I0124 14:41:56.774404 31536 solver.cpp:253]     Train net output #0: loss = 20.6761 (* 1 = 20.6761 loss)
I0124 14:41:56.774411 31536 sgd_solver.cpp:106] Iteration 2680, lr = 0.01
I0124 14:42:04.598260 31536 solver.cpp:237] Iteration 2700, loss = 12.5281
I0124 14:42:04.598296 31536 solver.cpp:253]     Train net output #0: loss = 7.4031 (* 1 = 7.4031 loss)
I0124 14:42:04.598307 31536 sgd_solver.cpp:106] Iteration 2700, lr = 0.01
I0124 14:42:12.366547 31536 solver.cpp:237] Iteration 2720, loss = 27.5411
I0124 14:42:12.366582 31536 solver.cpp:253]     Train net output #0: loss = 23.967 (* 1 = 23.967 loss)
I0124 14:42:12.366591 31536 sgd_solver.cpp:106] Iteration 2720, lr = 0.01
I0124 14:42:20.380698 31536 solver.cpp:237] Iteration 2740, loss = 32.7412
I0124 14:42:20.380782 31536 solver.cpp:253]     Train net output #0: loss = 8.05722 (* 1 = 8.05722 loss)
I0124 14:42:20.380825 31536 sgd_solver.cpp:106] Iteration 2740, lr = 0.01
I0124 14:42:28.059044 31536 solver.cpp:237] Iteration 2760, loss = 24.5237
I0124 14:42:28.059079 31536 solver.cpp:253]     Train net output #0: loss = 12.0209 (* 1 = 12.0209 loss)
I0124 14:42:28.059087 31536 sgd_solver.cpp:106] Iteration 2760, lr = 0.01
I0124 14:42:35.533085 31536 solver.cpp:237] Iteration 2780, loss = 17.4122
I0124 14:42:35.533121 31536 solver.cpp:253]     Train net output #0: loss = 8.00463 (* 1 = 8.00463 loss)
I0124 14:42:35.533133 31536 sgd_solver.cpp:106] Iteration 2780, lr = 0.01
I0124 14:42:43.041409 31536 solver.cpp:237] Iteration 2800, loss = 23.7751
I0124 14:42:43.041498 31536 solver.cpp:253]     Train net output #0: loss = 15.5011 (* 1 = 15.5011 loss)
I0124 14:42:43.041522 31536 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I0124 14:42:45.676044 31536 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 14:42:50.619029 31536 solver.cpp:237] Iteration 2820, loss = 26.189
I0124 14:42:50.619107 31536 solver.cpp:253]     Train net output #0: loss = 75.4778 (* 1 = 75.4778 loss)
I0124 14:42:50.619122 31536 sgd_solver.cpp:106] Iteration 2820, lr = 0.01
I0124 14:42:58.505657 31536 solver.cpp:237] Iteration 2840, loss = 17.7101
I0124 14:42:58.505805 31536 solver.cpp:253]     Train net output #0: loss = 8.13622 (* 1 = 8.13622 loss)
I0124 14:42:58.505817 31536 sgd_solver.cpp:106] Iteration 2840, lr = 0.01
