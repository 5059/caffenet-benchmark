I0124 13:57:00.505172  9563 caffe.cpp:184] Using GPUs 0
I0124 13:57:01.063313  9563 solver.cpp:47] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.02
display: 20
max_iter: 320000
lr_policy: "step"
gamma: 0.1
weight_decay: 0.0005
stepsize: 100000
snapshot: 10000
snapshot_prefix: "snapshots/caffenet128_lsuv_adagrad_lr002"
solver_mode: GPU
device_id: 0
net_param {
  name: "CaffeNet"
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TRAIN
    }
    transform_param {
      scale: 0.04
      mirror: true
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 124
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/local/temporary/imagenet_clc_orig/ilsvrc12_train_shuffle_lmdb"
      batch_size: 128
      backend: LMDB
    }
  }
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TEST
    }
    transform_param {
      scale: 0.04
      mirror: false
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 123
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/local/temporary/imagenet_clc_orig/ilsvrc12_val_lmdb"
      batch_size: 50
      backend: LMDB
    }
  }
  layer {
    name: "conv1"
    type: "Convolution"
    bottom: "data"
    top: "conv1"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 96
      kernel_size: 11
      stride: 4
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu1"
    type: "ReLU"
    bottom: "conv1"
    top: "relu1"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "relu1"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv2"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 2
      kernel_size: 5
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu2"
    type: "ReLU"
    bottom: "conv2"
    top: "relu2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "relu2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv3"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu3"
    type: "ReLU"
    bottom: "conv3"
    top: "relu3"
  }
  layer {
    name: "conv4"
    type: "Convolution"
    bottom: "relu3"
    top: "conv4"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu4"
    type: "ReLU"
    bottom: "conv4"
    top: "relu4"
  }
  layer {
    name: "conv5"
    type: "Convolution"
    bottom: "relu4"
    top: "conv5"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu5"
    type: "ReLU"
    bottom: "conv5"
    top: "relu5"
  }
  layer {
    name: "pool5"
    type: "Pooling"
    bottom: "relu5"
    top: "pool5"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "pool5"
    top: "fc6"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu6"
    type: "ReLU"
    bottom: "fc6"
    top: "relu6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "relu6"
    top: "drop6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "drop6"
    top: "fc7"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu7"
    type: "ReLU"
    bottom: "fc7"
    top: "relu7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "relu7"
    top: "drop7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc8"
    type: "InnerProduct"
    bottom: "drop7"
    top: "fc8"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 1000
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "accuracy"
    type: "Accuracy"
    bottom: "fc8"
    bottom: "label"
    top: "accuracy"
    include {
      phase: TEST
    }
  }
  layer {
    name: "loss"
    type: "SoftmaxWithLoss"
    bottom: "fc8"
    bottom: "label"
    top: "loss"
  }
}
test_initialization: false
iter_size: 1
type: "AdaGrad"
I0124 13:57:01.064260  9563 solver.cpp:85] Creating training net specified in net_param.
I0124 13:57:01.064401  9563 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0124 13:57:01.064431  9563 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0124 13:57:01.064708  9563 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.04
    mirror: true
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 124
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/local/temporary/imagenet_clc_orig/ilsvrc12_train_shuffle_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 13:57:01.064865  9563 layer_factory.hpp:76] Creating layer data
I0124 13:57:01.065485  9563 net.cpp:106] Creating Layer data
I0124 13:57:01.065497  9563 net.cpp:411] data -> data
I0124 13:57:01.065528  9563 net.cpp:411] data -> label
I0124 13:57:01.066504  9602 db_lmdb.cpp:38] Opened lmdb /local/temporary/imagenet_clc_orig/ilsvrc12_train_shuffle_lmdb
I0124 13:57:01.085809  9563 data_layer.cpp:41] output data size: 128,3,128,128
I0124 13:57:01.145581  9563 net.cpp:150] Setting up data
I0124 13:57:01.145630  9563 net.cpp:157] Top shape: 128 3 128 128 (6291456)
I0124 13:57:01.145637  9563 net.cpp:157] Top shape: 128 (128)
I0124 13:57:01.145642  9563 net.cpp:165] Memory required for data: 25166336
I0124 13:57:01.145655  9563 layer_factory.hpp:76] Creating layer conv1
I0124 13:57:01.145696  9563 net.cpp:106] Creating Layer conv1
I0124 13:57:01.145704  9563 net.cpp:454] conv1 <- data
I0124 13:57:01.145722  9563 net.cpp:411] conv1 -> conv1
I0124 13:57:01.279441  9563 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 4356
I0124 13:57:01.279593  9563 net.cpp:150] Setting up conv1
I0124 13:57:01.279608  9563 net.cpp:157] Top shape: 128 96 30 30 (11059200)
I0124 13:57:01.279611  9563 net.cpp:165] Memory required for data: 69403136
I0124 13:57:01.279631  9563 layer_factory.hpp:76] Creating layer relu1
I0124 13:57:01.279644  9563 net.cpp:106] Creating Layer relu1
I0124 13:57:01.279649  9563 net.cpp:454] relu1 <- conv1
I0124 13:57:01.279657  9563 net.cpp:411] relu1 -> relu1
I0124 13:57:01.279928  9563 net.cpp:150] Setting up relu1
I0124 13:57:01.279939  9563 net.cpp:157] Top shape: 128 96 30 30 (11059200)
I0124 13:57:01.279945  9563 net.cpp:165] Memory required for data: 113639936
I0124 13:57:01.279949  9563 layer_factory.hpp:76] Creating layer pool1
I0124 13:57:01.279961  9563 net.cpp:106] Creating Layer pool1
I0124 13:57:01.279965  9563 net.cpp:454] pool1 <- relu1
I0124 13:57:01.279970  9563 net.cpp:411] pool1 -> pool1
I0124 13:57:01.280350  9563 net.cpp:150] Setting up pool1
I0124 13:57:01.280364  9563 net.cpp:157] Top shape: 128 96 15 15 (2764800)
I0124 13:57:01.280366  9563 net.cpp:165] Memory required for data: 124699136
I0124 13:57:01.280375  9563 layer_factory.hpp:76] Creating layer conv2
I0124 13:57:01.280390  9563 net.cpp:106] Creating Layer conv2
I0124 13:57:01.280395  9563 net.cpp:454] conv2 <- pool1
I0124 13:57:01.280400  9563 net.cpp:411] conv2 -> conv2
I0124 13:57:01.292212  9563 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 28800
I0124 13:57:01.292243  9563 net.cpp:150] Setting up conv2
I0124 13:57:01.292249  9563 net.cpp:157] Top shape: 128 256 15 15 (7372800)
I0124 13:57:01.292253  9563 net.cpp:165] Memory required for data: 154190336
I0124 13:57:01.292269  9563 layer_factory.hpp:76] Creating layer relu2
I0124 13:57:01.292280  9563 net.cpp:106] Creating Layer relu2
I0124 13:57:01.292285  9563 net.cpp:454] relu2 <- conv2
I0124 13:57:01.292291  9563 net.cpp:411] relu2 -> relu2
I0124 13:57:01.292520  9563 net.cpp:150] Setting up relu2
I0124 13:57:01.292531  9563 net.cpp:157] Top shape: 128 256 15 15 (7372800)
I0124 13:57:01.292536  9563 net.cpp:165] Memory required for data: 183681536
I0124 13:57:01.292539  9563 layer_factory.hpp:76] Creating layer pool2
I0124 13:57:01.292556  9563 net.cpp:106] Creating Layer pool2
I0124 13:57:01.292558  9563 net.cpp:454] pool2 <- relu2
I0124 13:57:01.292563  9563 net.cpp:411] pool2 -> pool2
I0124 13:57:01.292923  9563 net.cpp:150] Setting up pool2
I0124 13:57:01.292937  9563 net.cpp:157] Top shape: 128 256 7 7 (1605632)
I0124 13:57:01.292939  9563 net.cpp:165] Memory required for data: 190104064
I0124 13:57:01.292943  9563 layer_factory.hpp:76] Creating layer conv3
I0124 13:57:01.292954  9563 net.cpp:106] Creating Layer conv3
I0124 13:57:01.292958  9563 net.cpp:454] conv3 <- pool2
I0124 13:57:01.292965  9563 net.cpp:411] conv3 -> conv3
I0124 13:57:01.322196  9563 net.cpp:150] Setting up conv3
I0124 13:57:01.322218  9563 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I0124 13:57:01.322222  9563 net.cpp:165] Memory required for data: 199737856
I0124 13:57:01.322234  9563 layer_factory.hpp:76] Creating layer relu3
I0124 13:57:01.322245  9563 net.cpp:106] Creating Layer relu3
I0124 13:57:01.322249  9563 net.cpp:454] relu3 <- conv3
I0124 13:57:01.322257  9563 net.cpp:411] relu3 -> relu3
I0124 13:57:01.322628  9563 net.cpp:150] Setting up relu3
I0124 13:57:01.322640  9563 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I0124 13:57:01.322643  9563 net.cpp:165] Memory required for data: 209371648
I0124 13:57:01.322650  9563 layer_factory.hpp:76] Creating layer conv4
I0124 13:57:01.322661  9563 net.cpp:106] Creating Layer conv4
I0124 13:57:01.322664  9563 net.cpp:454] conv4 <- relu3
I0124 13:57:01.322672  9563 net.cpp:411] conv4 -> conv4
I0124 13:57:01.345604  9563 net.cpp:150] Setting up conv4
I0124 13:57:01.345633  9563 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I0124 13:57:01.345638  9563 net.cpp:165] Memory required for data: 219005440
I0124 13:57:01.345645  9563 layer_factory.hpp:76] Creating layer relu4
I0124 13:57:01.345652  9563 net.cpp:106] Creating Layer relu4
I0124 13:57:01.345656  9563 net.cpp:454] relu4 <- conv4
I0124 13:57:01.345667  9563 net.cpp:411] relu4 -> relu4
I0124 13:57:01.345880  9563 net.cpp:150] Setting up relu4
I0124 13:57:01.345888  9563 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I0124 13:57:01.345893  9563 net.cpp:165] Memory required for data: 228639232
I0124 13:57:01.345897  9563 layer_factory.hpp:76] Creating layer conv5
I0124 13:57:01.345907  9563 net.cpp:106] Creating Layer conv5
I0124 13:57:01.345911  9563 net.cpp:454] conv5 <- relu4
I0124 13:57:01.345928  9563 net.cpp:411] conv5 -> conv5
I0124 13:57:01.361968  9563 net.cpp:150] Setting up conv5
I0124 13:57:01.361984  9563 net.cpp:157] Top shape: 128 256 7 7 (1605632)
I0124 13:57:01.361992  9563 net.cpp:165] Memory required for data: 235061760
I0124 13:57:01.362004  9563 layer_factory.hpp:76] Creating layer relu5
I0124 13:57:01.362012  9563 net.cpp:106] Creating Layer relu5
I0124 13:57:01.362015  9563 net.cpp:454] relu5 <- conv5
I0124 13:57:01.362022  9563 net.cpp:411] relu5 -> relu5
I0124 13:57:01.362237  9563 net.cpp:150] Setting up relu5
I0124 13:57:01.362247  9563 net.cpp:157] Top shape: 128 256 7 7 (1605632)
I0124 13:57:01.362252  9563 net.cpp:165] Memory required for data: 241484288
I0124 13:57:01.362256  9563 layer_factory.hpp:76] Creating layer pool5
I0124 13:57:01.362264  9563 net.cpp:106] Creating Layer pool5
I0124 13:57:01.362268  9563 net.cpp:454] pool5 <- relu5
I0124 13:57:01.362273  9563 net.cpp:411] pool5 -> pool5
I0124 13:57:01.362610  9563 net.cpp:150] Setting up pool5
I0124 13:57:01.362623  9563 net.cpp:157] Top shape: 128 256 3 3 (294912)
I0124 13:57:01.362627  9563 net.cpp:165] Memory required for data: 242663936
I0124 13:57:01.362632  9563 layer_factory.hpp:76] Creating layer fc6
I0124 13:57:01.362642  9563 net.cpp:106] Creating Layer fc6
I0124 13:57:01.362644  9563 net.cpp:454] fc6 <- pool5
I0124 13:57:01.362653  9563 net.cpp:411] fc6 -> fc6
I0124 13:57:01.513877  9563 net.cpp:150] Setting up fc6
I0124 13:57:01.513909  9563 net.cpp:157] Top shape: 128 2048 (262144)
I0124 13:57:01.513913  9563 net.cpp:165] Memory required for data: 243712512
I0124 13:57:01.513923  9563 layer_factory.hpp:76] Creating layer relu6
I0124 13:57:01.513936  9563 net.cpp:106] Creating Layer relu6
I0124 13:57:01.513943  9563 net.cpp:454] relu6 <- fc6
I0124 13:57:01.513950  9563 net.cpp:411] relu6 -> relu6
I0124 13:57:01.514221  9563 net.cpp:150] Setting up relu6
I0124 13:57:01.514231  9563 net.cpp:157] Top shape: 128 2048 (262144)
I0124 13:57:01.514237  9563 net.cpp:165] Memory required for data: 244761088
I0124 13:57:01.514240  9563 layer_factory.hpp:76] Creating layer drop6
I0124 13:57:01.514259  9563 net.cpp:106] Creating Layer drop6
I0124 13:57:01.514262  9563 net.cpp:454] drop6 <- relu6
I0124 13:57:01.514267  9563 net.cpp:411] drop6 -> drop6
I0124 13:57:01.514314  9563 net.cpp:150] Setting up drop6
I0124 13:57:01.514322  9563 net.cpp:157] Top shape: 128 2048 (262144)
I0124 13:57:01.514324  9563 net.cpp:165] Memory required for data: 245809664
I0124 13:57:01.514328  9563 layer_factory.hpp:76] Creating layer fc7
I0124 13:57:01.514338  9563 net.cpp:106] Creating Layer fc7
I0124 13:57:01.514343  9563 net.cpp:454] fc7 <- drop6
I0124 13:57:01.514348  9563 net.cpp:411] fc7 -> fc7
I0124 13:57:01.648265  9563 net.cpp:150] Setting up fc7
I0124 13:57:01.648324  9563 net.cpp:157] Top shape: 128 2048 (262144)
I0124 13:57:01.648327  9563 net.cpp:165] Memory required for data: 246858240
I0124 13:57:01.648337  9563 layer_factory.hpp:76] Creating layer relu7
I0124 13:57:01.648350  9563 net.cpp:106] Creating Layer relu7
I0124 13:57:01.648355  9563 net.cpp:454] relu7 <- fc7
I0124 13:57:01.648362  9563 net.cpp:411] relu7 -> relu7
I0124 13:57:01.648846  9563 net.cpp:150] Setting up relu7
I0124 13:57:01.648859  9563 net.cpp:157] Top shape: 128 2048 (262144)
I0124 13:57:01.648862  9563 net.cpp:165] Memory required for data: 247906816
I0124 13:57:01.648865  9563 layer_factory.hpp:76] Creating layer drop7
I0124 13:57:01.648874  9563 net.cpp:106] Creating Layer drop7
I0124 13:57:01.648879  9563 net.cpp:454] drop7 <- relu7
I0124 13:57:01.648883  9563 net.cpp:411] drop7 -> drop7
I0124 13:57:01.648929  9563 net.cpp:150] Setting up drop7
I0124 13:57:01.648936  9563 net.cpp:157] Top shape: 128 2048 (262144)
I0124 13:57:01.648939  9563 net.cpp:165] Memory required for data: 248955392
I0124 13:57:01.648943  9563 layer_factory.hpp:76] Creating layer fc8
I0124 13:57:01.648958  9563 net.cpp:106] Creating Layer fc8
I0124 13:57:01.648962  9563 net.cpp:454] fc8 <- drop7
I0124 13:57:01.648968  9563 net.cpp:411] fc8 -> fc8
I0124 13:57:01.714154  9563 net.cpp:150] Setting up fc8
I0124 13:57:01.714184  9563 net.cpp:157] Top shape: 128 1000 (128000)
I0124 13:57:01.714189  9563 net.cpp:165] Memory required for data: 249467392
I0124 13:57:01.714198  9563 layer_factory.hpp:76] Creating layer loss
I0124 13:57:01.714207  9563 net.cpp:106] Creating Layer loss
I0124 13:57:01.714212  9563 net.cpp:454] loss <- fc8
I0124 13:57:01.714217  9563 net.cpp:454] loss <- label
I0124 13:57:01.714226  9563 net.cpp:411] loss -> loss
I0124 13:57:01.714241  9563 layer_factory.hpp:76] Creating layer loss
I0124 13:57:01.715306  9563 net.cpp:150] Setting up loss
I0124 13:57:01.715319  9563 net.cpp:157] Top shape: (1)
I0124 13:57:01.715323  9563 net.cpp:160]     with loss weight 1
I0124 13:57:01.715353  9563 net.cpp:165] Memory required for data: 249467396
I0124 13:57:01.715358  9563 net.cpp:226] loss needs backward computation.
I0124 13:57:01.715361  9563 net.cpp:226] fc8 needs backward computation.
I0124 13:57:01.715369  9563 net.cpp:226] drop7 needs backward computation.
I0124 13:57:01.715373  9563 net.cpp:226] relu7 needs backward computation.
I0124 13:57:01.715376  9563 net.cpp:226] fc7 needs backward computation.
I0124 13:57:01.715379  9563 net.cpp:226] drop6 needs backward computation.
I0124 13:57:01.715384  9563 net.cpp:226] relu6 needs backward computation.
I0124 13:57:01.715386  9563 net.cpp:226] fc6 needs backward computation.
I0124 13:57:01.715390  9563 net.cpp:226] pool5 needs backward computation.
I0124 13:57:01.715394  9563 net.cpp:226] relu5 needs backward computation.
I0124 13:57:01.715396  9563 net.cpp:226] conv5 needs backward computation.
I0124 13:57:01.715400  9563 net.cpp:226] relu4 needs backward computation.
I0124 13:57:01.715404  9563 net.cpp:226] conv4 needs backward computation.
I0124 13:57:01.715409  9563 net.cpp:226] relu3 needs backward computation.
I0124 13:57:01.715411  9563 net.cpp:226] conv3 needs backward computation.
I0124 13:57:01.715421  9563 net.cpp:226] pool2 needs backward computation.
I0124 13:57:01.715425  9563 net.cpp:226] relu2 needs backward computation.
I0124 13:57:01.715428  9563 net.cpp:226] conv2 needs backward computation.
I0124 13:57:01.715432  9563 net.cpp:226] pool1 needs backward computation.
I0124 13:57:01.715436  9563 net.cpp:226] relu1 needs backward computation.
I0124 13:57:01.715440  9563 net.cpp:226] conv1 needs backward computation.
I0124 13:57:01.715445  9563 net.cpp:228] data does not need backward computation.
I0124 13:57:01.715447  9563 net.cpp:270] This network produces output loss
I0124 13:57:01.715468  9563 net.cpp:283] Network initialization done.
I0124 13:57:01.715608  9563 solver.cpp:180] Creating test net (#0) specified by net_param
I0124 13:57:01.715653  9563 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0124 13:57:01.715999  9563 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.04
    mirror: false
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 123
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/local/temporary/imagenet_clc_orig/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 13:57:01.716163  9563 layer_factory.hpp:76] Creating layer data
I0124 13:57:01.716266  9563 net.cpp:106] Creating Layer data
I0124 13:57:01.716280  9563 net.cpp:411] data -> data
I0124 13:57:01.716320  9563 net.cpp:411] data -> label
I0124 13:57:01.717420  9604 db_lmdb.cpp:38] Opened lmdb /local/temporary/imagenet_clc_orig/ilsvrc12_val_lmdb
I0124 13:57:01.720183  9563 data_layer.cpp:41] output data size: 50,3,128,128
I0124 13:57:01.738836  9563 net.cpp:150] Setting up data
I0124 13:57:01.738867  9563 net.cpp:157] Top shape: 50 3 128 128 (2457600)
I0124 13:57:01.738873  9563 net.cpp:157] Top shape: 50 (50)
I0124 13:57:01.738876  9563 net.cpp:165] Memory required for data: 9830600
I0124 13:57:01.738881  9563 layer_factory.hpp:76] Creating layer label_data_1_split
I0124 13:57:01.738896  9563 net.cpp:106] Creating Layer label_data_1_split
I0124 13:57:01.738900  9563 net.cpp:454] label_data_1_split <- label
I0124 13:57:01.738909  9563 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0124 13:57:01.738921  9563 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0124 13:57:01.739060  9563 net.cpp:150] Setting up label_data_1_split
I0124 13:57:01.739069  9563 net.cpp:157] Top shape: 50 (50)
I0124 13:57:01.739073  9563 net.cpp:157] Top shape: 50 (50)
I0124 13:57:01.739076  9563 net.cpp:165] Memory required for data: 9831000
I0124 13:57:01.739080  9563 layer_factory.hpp:76] Creating layer conv1
I0124 13:57:01.739094  9563 net.cpp:106] Creating Layer conv1
I0124 13:57:01.739099  9563 net.cpp:454] conv1 <- data
I0124 13:57:01.739106  9563 net.cpp:411] conv1 -> conv1
I0124 13:57:01.741415  9563 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 4356
I0124 13:57:01.741452  9563 net.cpp:150] Setting up conv1
I0124 13:57:01.741459  9563 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I0124 13:57:01.741466  9563 net.cpp:165] Memory required for data: 27111000
I0124 13:57:01.741483  9563 layer_factory.hpp:76] Creating layer relu1
I0124 13:57:01.741492  9563 net.cpp:106] Creating Layer relu1
I0124 13:57:01.741495  9563 net.cpp:454] relu1 <- conv1
I0124 13:57:01.741502  9563 net.cpp:411] relu1 -> relu1
I0124 13:57:01.741829  9563 net.cpp:150] Setting up relu1
I0124 13:57:01.741842  9563 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I0124 13:57:01.741845  9563 net.cpp:165] Memory required for data: 44391000
I0124 13:57:01.741848  9563 layer_factory.hpp:76] Creating layer pool1
I0124 13:57:01.741858  9563 net.cpp:106] Creating Layer pool1
I0124 13:57:01.741860  9563 net.cpp:454] pool1 <- relu1
I0124 13:57:01.741866  9563 net.cpp:411] pool1 -> pool1
I0124 13:57:01.742075  9563 net.cpp:150] Setting up pool1
I0124 13:57:01.742084  9563 net.cpp:157] Top shape: 50 96 15 15 (1080000)
I0124 13:57:01.742089  9563 net.cpp:165] Memory required for data: 48711000
I0124 13:57:01.742091  9563 layer_factory.hpp:76] Creating layer conv2
I0124 13:57:01.742100  9563 net.cpp:106] Creating Layer conv2
I0124 13:57:01.742105  9563 net.cpp:454] conv2 <- pool1
I0124 13:57:01.742111  9563 net.cpp:411] conv2 -> conv2
I0124 13:57:01.753795  9563 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 28800
I0124 13:57:01.753846  9563 net.cpp:150] Setting up conv2
I0124 13:57:01.753854  9563 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I0124 13:57:01.753856  9563 net.cpp:165] Memory required for data: 60231000
I0124 13:57:01.753866  9563 layer_factory.hpp:76] Creating layer relu2
I0124 13:57:01.753873  9563 net.cpp:106] Creating Layer relu2
I0124 13:57:01.753878  9563 net.cpp:454] relu2 <- conv2
I0124 13:57:01.753893  9563 net.cpp:411] relu2 -> relu2
I0124 13:57:01.754106  9563 net.cpp:150] Setting up relu2
I0124 13:57:01.754117  9563 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I0124 13:57:01.754122  9563 net.cpp:165] Memory required for data: 71751000
I0124 13:57:01.754124  9563 layer_factory.hpp:76] Creating layer pool2
I0124 13:57:01.754132  9563 net.cpp:106] Creating Layer pool2
I0124 13:57:01.754137  9563 net.cpp:454] pool2 <- relu2
I0124 13:57:01.754143  9563 net.cpp:411] pool2 -> pool2
I0124 13:57:01.754495  9563 net.cpp:150] Setting up pool2
I0124 13:57:01.754508  9563 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0124 13:57:01.754510  9563 net.cpp:165] Memory required for data: 74259800
I0124 13:57:01.754514  9563 layer_factory.hpp:76] Creating layer conv3
I0124 13:57:01.754525  9563 net.cpp:106] Creating Layer conv3
I0124 13:57:01.754528  9563 net.cpp:454] conv3 <- pool2
I0124 13:57:01.754536  9563 net.cpp:411] conv3 -> conv3
I0124 13:57:01.783838  9563 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 27648
I0124 13:57:01.783874  9563 net.cpp:150] Setting up conv3
I0124 13:57:01.783882  9563 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0124 13:57:01.783886  9563 net.cpp:165] Memory required for data: 78023000
I0124 13:57:01.783898  9563 layer_factory.hpp:76] Creating layer relu3
I0124 13:57:01.783907  9563 net.cpp:106] Creating Layer relu3
I0124 13:57:01.783916  9563 net.cpp:454] relu3 <- conv3
I0124 13:57:01.783929  9563 net.cpp:411] relu3 -> relu3
I0124 13:57:01.784137  9563 net.cpp:150] Setting up relu3
I0124 13:57:01.784152  9563 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0124 13:57:01.784158  9563 net.cpp:165] Memory required for data: 81786200
I0124 13:57:01.784162  9563 layer_factory.hpp:76] Creating layer conv4
I0124 13:57:01.784173  9563 net.cpp:106] Creating Layer conv4
I0124 13:57:01.784176  9563 net.cpp:454] conv4 <- relu3
I0124 13:57:01.784184  9563 net.cpp:411] conv4 -> conv4
I0124 13:57:01.807349  9563 net.cpp:150] Setting up conv4
I0124 13:57:01.807368  9563 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0124 13:57:01.807379  9563 net.cpp:165] Memory required for data: 85549400
I0124 13:57:01.807386  9563 layer_factory.hpp:76] Creating layer relu4
I0124 13:57:01.807402  9563 net.cpp:106] Creating Layer relu4
I0124 13:57:01.807406  9563 net.cpp:454] relu4 <- conv4
I0124 13:57:01.807415  9563 net.cpp:411] relu4 -> relu4
I0124 13:57:01.807627  9563 net.cpp:150] Setting up relu4
I0124 13:57:01.807636  9563 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0124 13:57:01.807641  9563 net.cpp:165] Memory required for data: 89312600
I0124 13:57:01.807646  9563 layer_factory.hpp:76] Creating layer conv5
I0124 13:57:01.807658  9563 net.cpp:106] Creating Layer conv5
I0124 13:57:01.807662  9563 net.cpp:454] conv5 <- relu4
I0124 13:57:01.807668  9563 net.cpp:411] conv5 -> conv5
I0124 13:57:01.823724  9563 net.cpp:150] Setting up conv5
I0124 13:57:01.823740  9563 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0124 13:57:01.823748  9563 net.cpp:165] Memory required for data: 91821400
I0124 13:57:01.823760  9563 layer_factory.hpp:76] Creating layer relu5
I0124 13:57:01.823768  9563 net.cpp:106] Creating Layer relu5
I0124 13:57:01.823771  9563 net.cpp:454] relu5 <- conv5
I0124 13:57:01.823777  9563 net.cpp:411] relu5 -> relu5
I0124 13:57:01.824111  9563 net.cpp:150] Setting up relu5
I0124 13:57:01.824123  9563 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0124 13:57:01.824126  9563 net.cpp:165] Memory required for data: 94330200
I0124 13:57:01.824131  9563 layer_factory.hpp:76] Creating layer pool5
I0124 13:57:01.824139  9563 net.cpp:106] Creating Layer pool5
I0124 13:57:01.824142  9563 net.cpp:454] pool5 <- relu5
I0124 13:57:01.824185  9563 net.cpp:411] pool5 -> pool5
I0124 13:57:01.824415  9563 net.cpp:150] Setting up pool5
I0124 13:57:01.824426  9563 net.cpp:157] Top shape: 50 256 3 3 (115200)
I0124 13:57:01.824430  9563 net.cpp:165] Memory required for data: 94791000
I0124 13:57:01.824434  9563 layer_factory.hpp:76] Creating layer fc6
I0124 13:57:01.824445  9563 net.cpp:106] Creating Layer fc6
I0124 13:57:01.824450  9563 net.cpp:454] fc6 <- pool5
I0124 13:57:01.824455  9563 net.cpp:411] fc6 -> fc6
I0124 13:57:01.961138  9563 net.cpp:150] Setting up fc6
I0124 13:57:01.961174  9563 net.cpp:157] Top shape: 50 2048 (102400)
I0124 13:57:01.961177  9563 net.cpp:165] Memory required for data: 95200600
I0124 13:57:01.961187  9563 layer_factory.hpp:76] Creating layer relu6
I0124 13:57:01.961200  9563 net.cpp:106] Creating Layer relu6
I0124 13:57:01.961205  9563 net.cpp:454] relu6 <- fc6
I0124 13:57:01.961211  9563 net.cpp:411] relu6 -> relu6
I0124 13:57:01.961686  9563 net.cpp:150] Setting up relu6
I0124 13:57:01.961696  9563 net.cpp:157] Top shape: 50 2048 (102400)
I0124 13:57:01.961710  9563 net.cpp:165] Memory required for data: 95610200
I0124 13:57:01.961714  9563 layer_factory.hpp:76] Creating layer drop6
I0124 13:57:01.961722  9563 net.cpp:106] Creating Layer drop6
I0124 13:57:01.961726  9563 net.cpp:454] drop6 <- relu6
I0124 13:57:01.961730  9563 net.cpp:411] drop6 -> drop6
I0124 13:57:01.961798  9563 net.cpp:150] Setting up drop6
I0124 13:57:01.961804  9563 net.cpp:157] Top shape: 50 2048 (102400)
I0124 13:57:01.961807  9563 net.cpp:165] Memory required for data: 96019800
I0124 13:57:01.961812  9563 layer_factory.hpp:76] Creating layer fc7
I0124 13:57:01.961833  9563 net.cpp:106] Creating Layer fc7
I0124 13:57:01.961835  9563 net.cpp:454] fc7 <- drop6
I0124 13:57:01.961843  9563 net.cpp:411] fc7 -> fc7
I0124 13:57:02.071079  9563 net.cpp:150] Setting up fc7
I0124 13:57:02.071116  9563 net.cpp:157] Top shape: 50 2048 (102400)
I0124 13:57:02.071120  9563 net.cpp:165] Memory required for data: 96429400
I0124 13:57:02.071130  9563 layer_factory.hpp:76] Creating layer relu7
I0124 13:57:02.071141  9563 net.cpp:106] Creating Layer relu7
I0124 13:57:02.071144  9563 net.cpp:454] relu7 <- fc7
I0124 13:57:02.071152  9563 net.cpp:411] relu7 -> relu7
I0124 13:57:02.071449  9563 net.cpp:150] Setting up relu7
I0124 13:57:02.071458  9563 net.cpp:157] Top shape: 50 2048 (102400)
I0124 13:57:02.071462  9563 net.cpp:165] Memory required for data: 96839000
I0124 13:57:02.071465  9563 layer_factory.hpp:76] Creating layer drop7
I0124 13:57:02.071485  9563 net.cpp:106] Creating Layer drop7
I0124 13:57:02.071488  9563 net.cpp:454] drop7 <- relu7
I0124 13:57:02.071494  9563 net.cpp:411] drop7 -> drop7
I0124 13:57:02.071532  9563 net.cpp:150] Setting up drop7
I0124 13:57:02.071538  9563 net.cpp:157] Top shape: 50 2048 (102400)
I0124 13:57:02.071542  9563 net.cpp:165] Memory required for data: 97248600
I0124 13:57:02.071545  9563 layer_factory.hpp:76] Creating layer fc8
I0124 13:57:02.071554  9563 net.cpp:106] Creating Layer fc8
I0124 13:57:02.071558  9563 net.cpp:454] fc8 <- drop7
I0124 13:57:02.071563  9563 net.cpp:411] fc8 -> fc8
I0124 13:57:02.125004  9563 net.cpp:150] Setting up fc8
I0124 13:57:02.125038  9563 net.cpp:157] Top shape: 50 1000 (50000)
I0124 13:57:02.125041  9563 net.cpp:165] Memory required for data: 97448600
I0124 13:57:02.125051  9563 layer_factory.hpp:76] Creating layer fc8_fc8_0_split
I0124 13:57:02.125072  9563 net.cpp:106] Creating Layer fc8_fc8_0_split
I0124 13:57:02.125077  9563 net.cpp:454] fc8_fc8_0_split <- fc8
I0124 13:57:02.125088  9563 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0124 13:57:02.125111  9563 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0124 13:57:02.125166  9563 net.cpp:150] Setting up fc8_fc8_0_split
I0124 13:57:02.125172  9563 net.cpp:157] Top shape: 50 1000 (50000)
I0124 13:57:02.125176  9563 net.cpp:157] Top shape: 50 1000 (50000)
I0124 13:57:02.125180  9563 net.cpp:165] Memory required for data: 97848600
I0124 13:57:02.125183  9563 layer_factory.hpp:76] Creating layer accuracy
I0124 13:57:02.125218  9563 net.cpp:106] Creating Layer accuracy
I0124 13:57:02.125223  9563 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0124 13:57:02.125227  9563 net.cpp:454] accuracy <- label_data_1_split_0
I0124 13:57:02.125233  9563 net.cpp:411] accuracy -> accuracy
I0124 13:57:02.125242  9563 net.cpp:150] Setting up accuracy
I0124 13:57:02.125247  9563 net.cpp:157] Top shape: (1)
I0124 13:57:02.125250  9563 net.cpp:165] Memory required for data: 97848604
I0124 13:57:02.125254  9563 layer_factory.hpp:76] Creating layer loss
I0124 13:57:02.125259  9563 net.cpp:106] Creating Layer loss
I0124 13:57:02.125264  9563 net.cpp:454] loss <- fc8_fc8_0_split_1
I0124 13:57:02.125267  9563 net.cpp:454] loss <- label_data_1_split_1
I0124 13:57:02.125274  9563 net.cpp:411] loss -> loss
I0124 13:57:02.125283  9563 layer_factory.hpp:76] Creating layer loss
I0124 13:57:02.125847  9563 net.cpp:150] Setting up loss
I0124 13:57:02.125857  9563 net.cpp:157] Top shape: (1)
I0124 13:57:02.125860  9563 net.cpp:160]     with loss weight 1
I0124 13:57:02.125882  9563 net.cpp:165] Memory required for data: 97848608
I0124 13:57:02.125885  9563 net.cpp:226] loss needs backward computation.
I0124 13:57:02.125891  9563 net.cpp:228] accuracy does not need backward computation.
I0124 13:57:02.125895  9563 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0124 13:57:02.125897  9563 net.cpp:226] fc8 needs backward computation.
I0124 13:57:02.125901  9563 net.cpp:226] drop7 needs backward computation.
I0124 13:57:02.125905  9563 net.cpp:226] relu7 needs backward computation.
I0124 13:57:02.125907  9563 net.cpp:226] fc7 needs backward computation.
I0124 13:57:02.125911  9563 net.cpp:226] drop6 needs backward computation.
I0124 13:57:02.125915  9563 net.cpp:226] relu6 needs backward computation.
I0124 13:57:02.125917  9563 net.cpp:226] fc6 needs backward computation.
I0124 13:57:02.125921  9563 net.cpp:226] pool5 needs backward computation.
I0124 13:57:02.125926  9563 net.cpp:226] relu5 needs backward computation.
I0124 13:57:02.125928  9563 net.cpp:226] conv5 needs backward computation.
I0124 13:57:02.125942  9563 net.cpp:226] relu4 needs backward computation.
I0124 13:57:02.125946  9563 net.cpp:226] conv4 needs backward computation.
I0124 13:57:02.125948  9563 net.cpp:226] relu3 needs backward computation.
I0124 13:57:02.125952  9563 net.cpp:226] conv3 needs backward computation.
I0124 13:57:02.125953  9563 net.cpp:226] pool2 needs backward computation.
I0124 13:57:02.125957  9563 net.cpp:226] relu2 needs backward computation.
I0124 13:57:02.125959  9563 net.cpp:226] conv2 needs backward computation.
I0124 13:57:02.125962  9563 net.cpp:226] pool1 needs backward computation.
I0124 13:57:02.125965  9563 net.cpp:226] relu1 needs backward computation.
I0124 13:57:02.125969  9563 net.cpp:226] conv1 needs backward computation.
I0124 13:57:02.125974  9563 net.cpp:228] label_data_1_split does not need backward computation.
I0124 13:57:02.125978  9563 net.cpp:228] data does not need backward computation.
I0124 13:57:02.125982  9563 net.cpp:270] This network produces output accuracy
I0124 13:57:02.125985  9563 net.cpp:270] This network produces output loss
I0124 13:57:02.126004  9563 net.cpp:283] Network initialization done.
I0124 13:57:02.126124  9563 solver.cpp:59] Solver scaffolding done.
I0124 13:57:02.126703  9563 caffe.cpp:128] Finetuning from caffenet128_lsuv_adagrad.prototxt.caffemodel
I0124 13:57:02.260929  9563 caffe.cpp:212] Starting Optimization
I0124 13:57:02.260970  9563 solver.cpp:287] Solving CaffeNet
I0124 13:57:02.260974  9563 solver.cpp:288] Learning Rate Policy: step
I0124 13:57:02.325570  9563 solver.cpp:236] Iteration 0, loss = 7.29626
I0124 13:57:02.325626  9563 solver.cpp:252]     Train net output #0: loss = 7.29626 (* 1 = 7.29626 loss)
I0124 13:57:02.325646  9563 sgd_solver.cpp:106] Iteration 0, lr = 0.02
I0124 13:57:02.704478  9563 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 13:57:06.653702  9563 solver.cpp:236] Iteration 20, loss = 7.05814
I0124 13:57:06.653762  9563 solver.cpp:252]     Train net output #0: loss = 7.05813 (* 1 = 7.05813 loss)
I0124 13:57:06.653846  9563 sgd_solver.cpp:106] Iteration 20, lr = 0.02
I0124 13:57:11.231498  9563 solver.cpp:236] Iteration 40, loss = 6.92352
I0124 13:57:11.231564  9563 solver.cpp:252]     Train net output #0: loss = 6.92351 (* 1 = 6.92351 loss)
I0124 13:57:11.231575  9563 sgd_solver.cpp:106] Iteration 40, lr = 0.02
I0124 13:57:15.925983  9563 solver.cpp:236] Iteration 60, loss = 6.91236
I0124 13:57:15.926048  9563 solver.cpp:252]     Train net output #0: loss = 6.91235 (* 1 = 6.91235 loss)
I0124 13:57:15.926059  9563 sgd_solver.cpp:106] Iteration 60, lr = 0.02
I0124 13:57:20.633420  9563 solver.cpp:236] Iteration 80, loss = 6.8971
I0124 13:57:20.633496  9563 solver.cpp:252]     Train net output #0: loss = 6.89709 (* 1 = 6.89709 loss)
I0124 13:57:20.633508  9563 sgd_solver.cpp:106] Iteration 80, lr = 0.02
I0124 13:57:25.343981  9563 solver.cpp:236] Iteration 100, loss = 6.9039
I0124 13:57:25.344046  9563 solver.cpp:252]     Train net output #0: loss = 6.90389 (* 1 = 6.90389 loss)
I0124 13:57:25.344058  9563 sgd_solver.cpp:106] Iteration 100, lr = 0.02
I0124 13:57:30.520717  9563 solver.cpp:236] Iteration 120, loss = 6.9131
I0124 13:57:30.521010  9563 solver.cpp:252]     Train net output #0: loss = 6.91309 (* 1 = 6.91309 loss)
I0124 13:57:30.521036  9563 sgd_solver.cpp:106] Iteration 120, lr = 0.02
I0124 13:57:35.671217  9563 solver.cpp:236] Iteration 140, loss = 6.92469
I0124 13:57:35.671280  9563 solver.cpp:252]     Train net output #0: loss = 6.92468 (* 1 = 6.92468 loss)
I0124 13:57:35.671295  9563 sgd_solver.cpp:106] Iteration 140, lr = 0.02
I0124 13:57:40.364565  9563 solver.cpp:236] Iteration 160, loss = 6.91573
I0124 13:57:40.364631  9563 solver.cpp:252]     Train net output #0: loss = 6.91572 (* 1 = 6.91572 loss)
I0124 13:57:40.364645  9563 sgd_solver.cpp:106] Iteration 160, lr = 0.02
I0124 13:57:45.041909  9563 solver.cpp:236] Iteration 180, loss = 6.92115
I0124 13:57:45.041981  9563 solver.cpp:252]     Train net output #0: loss = 6.92114 (* 1 = 6.92114 loss)
I0124 13:57:45.041996  9563 sgd_solver.cpp:106] Iteration 180, lr = 0.02
I0124 13:57:49.718147  9563 solver.cpp:236] Iteration 200, loss = 6.90866
I0124 13:57:49.718217  9563 solver.cpp:252]     Train net output #0: loss = 6.90865 (* 1 = 6.90865 loss)
I0124 13:57:49.718241  9563 sgd_solver.cpp:106] Iteration 200, lr = 0.02
I0124 13:57:54.385171  9563 solver.cpp:236] Iteration 220, loss = 6.91029
I0124 13:57:54.385228  9563 solver.cpp:252]     Train net output #0: loss = 6.91028 (* 1 = 6.91028 loss)
I0124 13:57:54.385237  9563 sgd_solver.cpp:106] Iteration 220, lr = 0.02
I0124 13:57:58.966414  9563 solver.cpp:236] Iteration 240, loss = 6.91303
I0124 13:57:58.966488  9563 solver.cpp:252]     Train net output #0: loss = 6.91302 (* 1 = 6.91302 loss)
I0124 13:57:58.966501  9563 sgd_solver.cpp:106] Iteration 240, lr = 0.02
I0124 13:58:03.707805  9563 solver.cpp:236] Iteration 260, loss = 6.91276
I0124 13:58:03.707937  9563 solver.cpp:252]     Train net output #0: loss = 6.91275 (* 1 = 6.91275 loss)
I0124 13:58:03.707952  9563 sgd_solver.cpp:106] Iteration 260, lr = 0.02
I0124 13:58:08.435190  9563 solver.cpp:236] Iteration 280, loss = 6.92372
I0124 13:58:08.435250  9563 solver.cpp:252]     Train net output #0: loss = 6.92372 (* 1 = 6.92372 loss)
I0124 13:58:08.435263  9563 sgd_solver.cpp:106] Iteration 280, lr = 0.02
I0124 13:58:13.565382  9563 solver.cpp:236] Iteration 300, loss = 6.89373
I0124 13:58:13.565538  9563 solver.cpp:252]     Train net output #0: loss = 6.89373 (* 1 = 6.89373 loss)
I0124 13:58:13.565569  9563 sgd_solver.cpp:106] Iteration 300, lr = 0.02
I0124 13:58:18.845227  9563 solver.cpp:236] Iteration 320, loss = 6.91288
I0124 13:58:18.845295  9563 solver.cpp:252]     Train net output #0: loss = 6.91287 (* 1 = 6.91287 loss)
I0124 13:58:18.845311  9563 sgd_solver.cpp:106] Iteration 320, lr = 0.02
I0124 13:58:23.631430  9563 solver.cpp:236] Iteration 340, loss = 6.89353
I0124 13:58:23.631484  9563 solver.cpp:252]     Train net output #0: loss = 6.89352 (* 1 = 6.89352 loss)
I0124 13:58:23.631505  9563 sgd_solver.cpp:106] Iteration 340, lr = 0.02
I0124 13:58:28.324885  9563 solver.cpp:236] Iteration 360, loss = 6.91666
I0124 13:58:28.324960  9563 solver.cpp:252]     Train net output #0: loss = 6.91665 (* 1 = 6.91665 loss)
I0124 13:58:28.324976  9563 sgd_solver.cpp:106] Iteration 360, lr = 0.02
I0124 13:58:33.014534  9563 solver.cpp:236] Iteration 380, loss = 6.9214
I0124 13:58:33.014601  9563 solver.cpp:252]     Train net output #0: loss = 6.92139 (* 1 = 6.92139 loss)
I0124 13:58:33.014617  9563 sgd_solver.cpp:106] Iteration 380, lr = 0.02
I0124 13:58:37.659768  9563 solver.cpp:236] Iteration 400, loss = 6.90827
I0124 13:58:37.659957  9563 solver.cpp:252]     Train net output #0: loss = 6.90826 (* 1 = 6.90826 loss)
I0124 13:58:37.659970  9563 sgd_solver.cpp:106] Iteration 400, lr = 0.02
I0124 13:58:42.294368  9563 solver.cpp:236] Iteration 420, loss = 6.92297
I0124 13:58:42.294438  9563 solver.cpp:252]     Train net output #0: loss = 6.92296 (* 1 = 6.92296 loss)
I0124 13:58:42.294456  9563 sgd_solver.cpp:106] Iteration 420, lr = 0.02
I0124 13:58:47.057958  9563 solver.cpp:236] Iteration 440, loss = 6.91978
I0124 13:58:47.058037  9563 solver.cpp:252]     Train net output #0: loss = 6.91977 (* 1 = 6.91977 loss)
I0124 13:58:47.058051  9563 sgd_solver.cpp:106] Iteration 440, lr = 0.02
I0124 13:58:51.830121  9563 solver.cpp:236] Iteration 460, loss = 6.90555
I0124 13:58:51.830184  9563 solver.cpp:252]     Train net output #0: loss = 6.90554 (* 1 = 6.90554 loss)
I0124 13:58:51.830196  9563 sgd_solver.cpp:106] Iteration 460, lr = 0.02
I0124 13:58:56.805191  9563 solver.cpp:236] Iteration 480, loss = 6.90795
I0124 13:58:56.805259  9563 solver.cpp:252]     Train net output #0: loss = 6.90794 (* 1 = 6.90794 loss)
I0124 13:58:56.805276  9563 sgd_solver.cpp:106] Iteration 480, lr = 0.02
I0124 13:59:02.004547  9563 solver.cpp:236] Iteration 500, loss = 6.92787
I0124 13:59:02.004622  9563 solver.cpp:252]     Train net output #0: loss = 6.92786 (* 1 = 6.92786 loss)
I0124 13:59:02.004636  9563 sgd_solver.cpp:106] Iteration 500, lr = 0.02
I0124 13:59:06.846992  9563 solver.cpp:236] Iteration 520, loss = 6.91077
I0124 13:59:06.847060  9563 solver.cpp:252]     Train net output #0: loss = 6.91076 (* 1 = 6.91076 loss)
I0124 13:59:06.847074  9563 sgd_solver.cpp:106] Iteration 520, lr = 0.02
I0124 13:59:11.844967  9563 solver.cpp:236] Iteration 540, loss = 6.90819
I0124 13:59:11.845203  9563 solver.cpp:252]     Train net output #0: loss = 6.90819 (* 1 = 6.90819 loss)
I0124 13:59:11.845226  9563 sgd_solver.cpp:106] Iteration 540, lr = 0.02
I0124 13:59:16.839565  9563 solver.cpp:236] Iteration 560, loss = 6.90904
I0124 13:59:16.839617  9563 solver.cpp:252]     Train net output #0: loss = 6.90903 (* 1 = 6.90903 loss)
I0124 13:59:16.839627  9563 sgd_solver.cpp:106] Iteration 560, lr = 0.02
I0124 13:59:21.801125  9563 solver.cpp:236] Iteration 580, loss = 6.90523
I0124 13:59:21.801178  9563 solver.cpp:252]     Train net output #0: loss = 6.90522 (* 1 = 6.90522 loss)
I0124 13:59:21.801192  9563 sgd_solver.cpp:106] Iteration 580, lr = 0.02
I0124 13:59:26.755580  9563 solver.cpp:236] Iteration 600, loss = 6.90597
I0124 13:59:26.755620  9563 solver.cpp:252]     Train net output #0: loss = 6.90597 (* 1 = 6.90597 loss)
I0124 13:59:26.755628  9563 sgd_solver.cpp:106] Iteration 600, lr = 0.02
I0124 13:59:31.694735  9563 solver.cpp:236] Iteration 620, loss = 6.91287
I0124 13:59:31.694789  9563 solver.cpp:252]     Train net output #0: loss = 6.91286 (* 1 = 6.91286 loss)
I0124 13:59:31.694807  9563 sgd_solver.cpp:106] Iteration 620, lr = 0.02
I0124 13:59:36.727198  9563 solver.cpp:236] Iteration 640, loss = 6.90382
I0124 13:59:36.727259  9563 solver.cpp:252]     Train net output #0: loss = 6.90381 (* 1 = 6.90381 loss)
I0124 13:59:36.727272  9563 sgd_solver.cpp:106] Iteration 640, lr = 0.02
I0124 13:59:42.295018  9563 solver.cpp:236] Iteration 660, loss = 6.9116
I0124 13:59:42.295239  9563 solver.cpp:252]     Train net output #0: loss = 6.91159 (* 1 = 6.91159 loss)
I0124 13:59:42.295253  9563 sgd_solver.cpp:106] Iteration 660, lr = 0.02
I0124 13:59:47.753522  9563 solver.cpp:236] Iteration 680, loss = 6.91817
I0124 13:59:47.753579  9563 solver.cpp:252]     Train net output #0: loss = 6.91816 (* 1 = 6.91816 loss)
I0124 13:59:47.753597  9563 sgd_solver.cpp:106] Iteration 680, lr = 0.02
I0124 13:59:52.632341  9563 solver.cpp:236] Iteration 700, loss = 6.92138
I0124 13:59:52.632400  9563 solver.cpp:252]     Train net output #0: loss = 6.92137 (* 1 = 6.92137 loss)
I0124 13:59:52.632413  9563 sgd_solver.cpp:106] Iteration 700, lr = 0.02
I0124 13:59:57.312116  9563 solver.cpp:236] Iteration 720, loss = 6.90699
I0124 13:59:57.312180  9563 solver.cpp:252]     Train net output #0: loss = 6.90698 (* 1 = 6.90698 loss)
I0124 13:59:57.312191  9563 sgd_solver.cpp:106] Iteration 720, lr = 0.02
I0124 14:00:02.016619  9563 solver.cpp:236] Iteration 740, loss = 6.90802
I0124 14:00:02.016680  9563 solver.cpp:252]     Train net output #0: loss = 6.90802 (* 1 = 6.90802 loss)
I0124 14:00:02.016693  9563 sgd_solver.cpp:106] Iteration 740, lr = 0.02
I0124 14:00:06.617441  9563 solver.cpp:236] Iteration 760, loss = 6.91497
I0124 14:00:06.617511  9563 solver.cpp:252]     Train net output #0: loss = 6.91496 (* 1 = 6.91496 loss)
I0124 14:00:06.617524  9563 sgd_solver.cpp:106] Iteration 760, lr = 0.02
I0124 14:00:11.168710  9563 solver.cpp:236] Iteration 780, loss = 6.90852
I0124 14:00:11.168773  9563 solver.cpp:252]     Train net output #0: loss = 6.90851 (* 1 = 6.90851 loss)
I0124 14:00:11.168787  9563 sgd_solver.cpp:106] Iteration 780, lr = 0.02
I0124 14:00:15.768327  9563 solver.cpp:236] Iteration 800, loss = 6.91051
I0124 14:00:15.768458  9563 solver.cpp:252]     Train net output #0: loss = 6.9105 (* 1 = 6.9105 loss)
I0124 14:00:15.768471  9563 sgd_solver.cpp:106] Iteration 800, lr = 0.02
I0124 14:00:20.539355  9563 solver.cpp:236] Iteration 820, loss = 6.91352
I0124 14:00:20.539404  9563 solver.cpp:252]     Train net output #0: loss = 6.91351 (* 1 = 6.91351 loss)
I0124 14:00:20.539417  9563 sgd_solver.cpp:106] Iteration 820, lr = 0.02
I0124 14:00:25.256254  9563 solver.cpp:236] Iteration 840, loss = 6.91152
I0124 14:00:25.256305  9563 solver.cpp:252]     Train net output #0: loss = 6.91152 (* 1 = 6.91152 loss)
I0124 14:00:25.256315  9563 sgd_solver.cpp:106] Iteration 840, lr = 0.02
I0124 14:00:30.199064  9563 solver.cpp:236] Iteration 860, loss = 6.90799
I0124 14:00:30.199121  9563 solver.cpp:252]     Train net output #0: loss = 6.90798 (* 1 = 6.90798 loss)
I0124 14:00:30.199131  9563 sgd_solver.cpp:106] Iteration 860, lr = 0.02
I0124 14:00:35.392395  9563 solver.cpp:236] Iteration 880, loss = 6.90688
I0124 14:00:35.392482  9563 solver.cpp:252]     Train net output #0: loss = 6.90687 (* 1 = 6.90687 loss)
I0124 14:00:35.392495  9563 sgd_solver.cpp:106] Iteration 880, lr = 0.02
I0124 14:00:40.141741  9563 solver.cpp:236] Iteration 900, loss = 6.91112
I0124 14:00:40.141809  9563 solver.cpp:252]     Train net output #0: loss = 6.91111 (* 1 = 6.91111 loss)
I0124 14:00:40.141825  9563 sgd_solver.cpp:106] Iteration 900, lr = 0.02
I0124 14:00:44.760243  9563 solver.cpp:236] Iteration 920, loss = 6.91191
I0124 14:00:44.760308  9563 solver.cpp:252]     Train net output #0: loss = 6.9119 (* 1 = 6.9119 loss)
I0124 14:00:44.760321  9563 sgd_solver.cpp:106] Iteration 920, lr = 0.02
I0124 14:00:49.319106  9563 solver.cpp:236] Iteration 940, loss = 6.91623
I0124 14:00:49.319257  9563 solver.cpp:252]     Train net output #0: loss = 6.91622 (* 1 = 6.91622 loss)
I0124 14:00:49.319272  9563 sgd_solver.cpp:106] Iteration 940, lr = 0.02
I0124 14:00:53.865938  9563 solver.cpp:236] Iteration 960, loss = 6.90624
I0124 14:00:53.866000  9563 solver.cpp:252]     Train net output #0: loss = 6.90623 (* 1 = 6.90623 loss)
I0124 14:00:53.866013  9563 sgd_solver.cpp:106] Iteration 960, lr = 0.02
