I0124 13:56:29.307452 30591 caffe.cpp:184] Using GPUs 0
I0124 13:56:29.467180 30591 solver.cpp:48] Initializing solver from parameters: 
test_iter: 200
test_interval: 2000
base_lr: 0.001
display: 20
max_iter: 320000
lr_policy: "step"
weight_decay: 0.0005
stepsize: 100000
snapshot: 10000
snapshot_prefix: "snapshots1/caffenet128_rmsprop"
solver_mode: GPU
device_id: 0
net_param {
  name: "CaffeNet"
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TRAIN
    }
    transform_param {
      scale: 0.04
      mirror: true
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 124
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/home/old-ufo/datasets/imagenet/ilsvrc12_train_shuffle_lmdb"
      batch_size: 256
      backend: LMDB
    }
  }
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TEST
    }
    transform_param {
      scale: 0.04
      mirror: false
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 123
      force_color: true
      resize_param {
        prob: 1
        resize_mode: FIT_SMALL_SIZE
        height: 144
        width: 144
        pad_mode: MIRRORED
        pad_value: 104
        pad_value: 117
        pad_value: 124
        interp_mode: LINEAR
        center_object: false
        max_angle: 0
      }
    }
    data_param {
      source: "/home/old-ufo/datasets/imagenet/ilsvrc12_val_lmdb"
      batch_size: 250
      backend: LMDB
    }
  }
  layer {
    name: "conv1"
    type: "Convolution"
    bottom: "data"
    top: "conv1"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 96
      kernel_size: 11
      stride: 4
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu1"
    type: "ReLU"
    bottom: "conv1"
    top: "relu1"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "relu1"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv2"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 2
      kernel_size: 5
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu2"
    type: "ReLU"
    bottom: "conv2"
    top: "conv2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "conv2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv3"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu3"
    type: "ReLU"
    bottom: "conv3"
    top: "conv3"
  }
  layer {
    name: "conv4"
    type: "Convolution"
    bottom: "conv3"
    top: "conv4"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu4"
    type: "ReLU"
    bottom: "conv4"
    top: "conv4"
  }
  layer {
    name: "conv5"
    type: "Convolution"
    bottom: "conv4"
    top: "conv5"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu5"
    type: "ReLU"
    bottom: "conv5"
    top: "conv5"
  }
  layer {
    name: "pool5"
    type: "Pooling"
    bottom: "conv5"
    top: "pool5"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "pool5"
    top: "fc6"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu6"
    type: "ReLU"
    bottom: "fc6"
    top: "fc6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "fc6"
    top: "fc6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "fc6"
    top: "fc7"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu7"
    type: "ReLU"
    bottom: "fc7"
    top: "fc7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "fc7"
    top: "fc7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc8"
    type: "InnerProduct"
    bottom: "fc7"
    top: "fc8"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 1000
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "accuracy"
    type: "Accuracy"
    bottom: "fc8"
    bottom: "label"
    top: "accuracy"
    include {
      phase: TEST
    }
  }
  layer {
    name: "loss"
    type: "SoftmaxWithLoss"
    bottom: "fc8"
    bottom: "label"
    top: "loss"
  }
}
test_initialization: false
average_loss: 20
iter_size: 1
rms_decay: 0.98
type: "RMSProp"
I0124 13:56:29.880388 30591 solver.cpp:86] Creating training net specified in net_param.
I0124 13:56:29.880539 30591 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0124 13:56:29.880568 30591 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0124 13:56:29.880769 30591 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.04
    mirror: true
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 124
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/old-ufo/datasets/imagenet/ilsvrc12_train_shuffle_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 13:56:29.880909 30591 layer_factory.hpp:76] Creating layer data
I0124 13:56:29.881759 30591 net.cpp:106] Creating Layer data
I0124 13:56:29.881780 30591 net.cpp:411] data -> data
I0124 13:56:29.881829 30591 net.cpp:411] data -> label
I0124 13:56:29.882640 30595 db_lmdb.cpp:38] Opened lmdb /home/old-ufo/datasets/imagenet/ilsvrc12_train_shuffle_lmdb
I0124 13:56:29.897620 30591 data_layer.cpp:41] output data size: 256,3,128,128
I0124 13:56:29.977198 30591 net.cpp:150] Setting up data
I0124 13:56:29.977227 30591 net.cpp:157] Top shape: 256 3 128 128 (12582912)
I0124 13:56:29.977237 30591 net.cpp:157] Top shape: 256 (256)
I0124 13:56:29.977242 30591 net.cpp:165] Memory required for data: 50332672
I0124 13:56:29.977257 30591 layer_factory.hpp:76] Creating layer conv1
I0124 13:56:29.977277 30591 net.cpp:106] Creating Layer conv1
I0124 13:56:29.977284 30591 net.cpp:454] conv1 <- data
I0124 13:56:29.977303 30591 net.cpp:411] conv1 -> conv1
I0124 13:56:30.157291 30591 net.cpp:150] Setting up conv1
I0124 13:56:30.157321 30591 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I0124 13:56:30.157325 30591 net.cpp:165] Memory required for data: 138806272
I0124 13:56:30.157346 30591 layer_factory.hpp:76] Creating layer relu1
I0124 13:56:30.157359 30591 net.cpp:106] Creating Layer relu1
I0124 13:56:30.157366 30591 net.cpp:454] relu1 <- conv1
I0124 13:56:30.157373 30591 net.cpp:411] relu1 -> relu1
I0124 13:56:30.158094 30591 net.cpp:150] Setting up relu1
I0124 13:56:30.158108 30591 net.cpp:157] Top shape: 256 96 30 30 (22118400)
I0124 13:56:30.158113 30591 net.cpp:165] Memory required for data: 227279872
I0124 13:56:30.158118 30591 layer_factory.hpp:76] Creating layer pool1
I0124 13:56:30.158126 30591 net.cpp:106] Creating Layer pool1
I0124 13:56:30.158131 30591 net.cpp:454] pool1 <- relu1
I0124 13:56:30.158138 30591 net.cpp:411] pool1 -> pool1
I0124 13:56:30.158885 30591 net.cpp:150] Setting up pool1
I0124 13:56:30.158897 30591 net.cpp:157] Top shape: 256 96 15 15 (5529600)
I0124 13:56:30.158902 30591 net.cpp:165] Memory required for data: 249398272
I0124 13:56:30.158906 30591 layer_factory.hpp:76] Creating layer conv2
I0124 13:56:30.158918 30591 net.cpp:106] Creating Layer conv2
I0124 13:56:30.158923 30591 net.cpp:454] conv2 <- pool1
I0124 13:56:30.158932 30591 net.cpp:411] conv2 -> conv2
I0124 13:56:30.174641 30591 net.cpp:150] Setting up conv2
I0124 13:56:30.174667 30591 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I0124 13:56:30.174672 30591 net.cpp:165] Memory required for data: 308380672
I0124 13:56:30.174686 30591 layer_factory.hpp:76] Creating layer relu2
I0124 13:56:30.174696 30591 net.cpp:106] Creating Layer relu2
I0124 13:56:30.174703 30591 net.cpp:454] relu2 <- conv2
I0124 13:56:30.174713 30591 net.cpp:397] relu2 -> conv2 (in-place)
I0124 13:56:30.175429 30591 net.cpp:150] Setting up relu2
I0124 13:56:30.175441 30591 net.cpp:157] Top shape: 256 256 15 15 (14745600)
I0124 13:56:30.175446 30591 net.cpp:165] Memory required for data: 367363072
I0124 13:56:30.175449 30591 layer_factory.hpp:76] Creating layer pool2
I0124 13:56:30.175458 30591 net.cpp:106] Creating Layer pool2
I0124 13:56:30.175463 30591 net.cpp:454] pool2 <- conv2
I0124 13:56:30.175470 30591 net.cpp:411] pool2 -> pool2
I0124 13:56:30.176368 30591 net.cpp:150] Setting up pool2
I0124 13:56:30.176383 30591 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I0124 13:56:30.176388 30591 net.cpp:165] Memory required for data: 380208128
I0124 13:56:30.176391 30591 layer_factory.hpp:76] Creating layer conv3
I0124 13:56:30.176404 30591 net.cpp:106] Creating Layer conv3
I0124 13:56:30.176409 30591 net.cpp:454] conv3 <- pool2
I0124 13:56:30.176419 30591 net.cpp:411] conv3 -> conv3
I0124 13:56:30.211017 30591 net.cpp:150] Setting up conv3
I0124 13:56:30.211045 30591 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 13:56:30.211051 30591 net.cpp:165] Memory required for data: 399475712
I0124 13:56:30.211068 30591 layer_factory.hpp:76] Creating layer relu3
I0124 13:56:30.211081 30591 net.cpp:106] Creating Layer relu3
I0124 13:56:30.211088 30591 net.cpp:454] relu3 <- conv3
I0124 13:56:30.211097 30591 net.cpp:397] relu3 -> conv3 (in-place)
I0124 13:56:30.211860 30591 net.cpp:150] Setting up relu3
I0124 13:56:30.211874 30591 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 13:56:30.211879 30591 net.cpp:165] Memory required for data: 418743296
I0124 13:56:30.211904 30591 layer_factory.hpp:76] Creating layer conv4
I0124 13:56:30.211916 30591 net.cpp:106] Creating Layer conv4
I0124 13:56:30.211921 30591 net.cpp:454] conv4 <- conv3
I0124 13:56:30.211930 30591 net.cpp:411] conv4 -> conv4
I0124 13:56:30.240803 30591 net.cpp:150] Setting up conv4
I0124 13:56:30.240828 30591 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 13:56:30.240834 30591 net.cpp:165] Memory required for data: 438010880
I0124 13:56:30.240845 30591 layer_factory.hpp:76] Creating layer relu4
I0124 13:56:30.240856 30591 net.cpp:106] Creating Layer relu4
I0124 13:56:30.240861 30591 net.cpp:454] relu4 <- conv4
I0124 13:56:30.240870 30591 net.cpp:397] relu4 -> conv4 (in-place)
I0124 13:56:30.241590 30591 net.cpp:150] Setting up relu4
I0124 13:56:30.241601 30591 net.cpp:157] Top shape: 256 384 7 7 (4816896)
I0124 13:56:30.241605 30591 net.cpp:165] Memory required for data: 457278464
I0124 13:56:30.241610 30591 layer_factory.hpp:76] Creating layer conv5
I0124 13:56:30.241621 30591 net.cpp:106] Creating Layer conv5
I0124 13:56:30.241624 30591 net.cpp:454] conv5 <- conv4
I0124 13:56:30.241634 30591 net.cpp:411] conv5 -> conv5
I0124 13:56:30.262614 30591 net.cpp:150] Setting up conv5
I0124 13:56:30.262640 30591 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I0124 13:56:30.262645 30591 net.cpp:165] Memory required for data: 470123520
I0124 13:56:30.262660 30591 layer_factory.hpp:76] Creating layer relu5
I0124 13:56:30.262670 30591 net.cpp:106] Creating Layer relu5
I0124 13:56:30.262676 30591 net.cpp:454] relu5 <- conv5
I0124 13:56:30.262684 30591 net.cpp:397] relu5 -> conv5 (in-place)
I0124 13:56:30.263511 30591 net.cpp:150] Setting up relu5
I0124 13:56:30.263523 30591 net.cpp:157] Top shape: 256 256 7 7 (3211264)
I0124 13:56:30.263527 30591 net.cpp:165] Memory required for data: 482968576
I0124 13:56:30.263531 30591 layer_factory.hpp:76] Creating layer pool5
I0124 13:56:30.263540 30591 net.cpp:106] Creating Layer pool5
I0124 13:56:30.263546 30591 net.cpp:454] pool5 <- conv5
I0124 13:56:30.263552 30591 net.cpp:411] pool5 -> pool5
I0124 13:56:30.264437 30591 net.cpp:150] Setting up pool5
I0124 13:56:30.264452 30591 net.cpp:157] Top shape: 256 256 3 3 (589824)
I0124 13:56:30.264456 30591 net.cpp:165] Memory required for data: 485327872
I0124 13:56:30.264461 30591 layer_factory.hpp:76] Creating layer fc6
I0124 13:56:30.264470 30591 net.cpp:106] Creating Layer fc6
I0124 13:56:30.264475 30591 net.cpp:454] fc6 <- pool5
I0124 13:56:30.264484 30591 net.cpp:411] fc6 -> fc6
I0124 13:56:30.433197 30591 net.cpp:150] Setting up fc6
I0124 13:56:30.433225 30591 net.cpp:157] Top shape: 256 2048 (524288)
I0124 13:56:30.433230 30591 net.cpp:165] Memory required for data: 487425024
I0124 13:56:30.433240 30591 layer_factory.hpp:76] Creating layer relu6
I0124 13:56:30.433251 30591 net.cpp:106] Creating Layer relu6
I0124 13:56:30.433256 30591 net.cpp:454] relu6 <- fc6
I0124 13:56:30.433264 30591 net.cpp:397] relu6 -> fc6 (in-place)
I0124 13:56:30.434213 30591 net.cpp:150] Setting up relu6
I0124 13:56:30.434231 30591 net.cpp:157] Top shape: 256 2048 (524288)
I0124 13:56:30.434234 30591 net.cpp:165] Memory required for data: 489522176
I0124 13:56:30.434238 30591 layer_factory.hpp:76] Creating layer drop6
I0124 13:56:30.434258 30591 net.cpp:106] Creating Layer drop6
I0124 13:56:30.434265 30591 net.cpp:454] drop6 <- fc6
I0124 13:56:30.434273 30591 net.cpp:397] drop6 -> fc6 (in-place)
I0124 13:56:30.434309 30591 net.cpp:150] Setting up drop6
I0124 13:56:30.434315 30591 net.cpp:157] Top shape: 256 2048 (524288)
I0124 13:56:30.434319 30591 net.cpp:165] Memory required for data: 491619328
I0124 13:56:30.434322 30591 layer_factory.hpp:76] Creating layer fc7
I0124 13:56:30.434331 30591 net.cpp:106] Creating Layer fc7
I0124 13:56:30.434336 30591 net.cpp:454] fc7 <- fc6
I0124 13:56:30.434345 30591 net.cpp:411] fc7 -> fc7
I0124 13:56:30.583978 30591 net.cpp:150] Setting up fc7
I0124 13:56:30.584004 30591 net.cpp:157] Top shape: 256 2048 (524288)
I0124 13:56:30.584009 30591 net.cpp:165] Memory required for data: 493716480
I0124 13:56:30.584048 30591 layer_factory.hpp:76] Creating layer relu7
I0124 13:56:30.584059 30591 net.cpp:106] Creating Layer relu7
I0124 13:56:30.584065 30591 net.cpp:454] relu7 <- fc7
I0124 13:56:30.584074 30591 net.cpp:397] relu7 -> fc7 (in-place)
I0124 13:56:30.584969 30591 net.cpp:150] Setting up relu7
I0124 13:56:30.584981 30591 net.cpp:157] Top shape: 256 2048 (524288)
I0124 13:56:30.584985 30591 net.cpp:165] Memory required for data: 495813632
I0124 13:56:30.584990 30591 layer_factory.hpp:76] Creating layer drop7
I0124 13:56:30.584998 30591 net.cpp:106] Creating Layer drop7
I0124 13:56:30.585003 30591 net.cpp:454] drop7 <- fc7
I0124 13:56:30.585010 30591 net.cpp:397] drop7 -> fc7 (in-place)
I0124 13:56:30.585042 30591 net.cpp:150] Setting up drop7
I0124 13:56:30.585049 30591 net.cpp:157] Top shape: 256 2048 (524288)
I0124 13:56:30.585052 30591 net.cpp:165] Memory required for data: 497910784
I0124 13:56:30.585058 30591 layer_factory.hpp:76] Creating layer fc8
I0124 13:56:30.585077 30591 net.cpp:106] Creating Layer fc8
I0124 13:56:30.585083 30591 net.cpp:454] fc8 <- fc7
I0124 13:56:30.585094 30591 net.cpp:411] fc8 -> fc8
I0124 13:56:30.657969 30591 net.cpp:150] Setting up fc8
I0124 13:56:30.657995 30591 net.cpp:157] Top shape: 256 1000 (256000)
I0124 13:56:30.658000 30591 net.cpp:165] Memory required for data: 498934784
I0124 13:56:30.658013 30591 layer_factory.hpp:76] Creating layer loss
I0124 13:56:30.658025 30591 net.cpp:106] Creating Layer loss
I0124 13:56:30.658030 30591 net.cpp:454] loss <- fc8
I0124 13:56:30.658038 30591 net.cpp:454] loss <- label
I0124 13:56:30.658048 30591 net.cpp:411] loss -> loss
I0124 13:56:30.658064 30591 layer_factory.hpp:76] Creating layer loss
I0124 13:56:30.659987 30591 net.cpp:150] Setting up loss
I0124 13:56:30.660001 30591 net.cpp:157] Top shape: (1)
I0124 13:56:30.660006 30591 net.cpp:160]     with loss weight 1
I0124 13:56:30.660023 30591 net.cpp:165] Memory required for data: 498934788
I0124 13:56:30.660027 30591 net.cpp:226] loss needs backward computation.
I0124 13:56:30.660034 30591 net.cpp:226] fc8 needs backward computation.
I0124 13:56:30.660038 30591 net.cpp:226] drop7 needs backward computation.
I0124 13:56:30.660042 30591 net.cpp:226] relu7 needs backward computation.
I0124 13:56:30.660046 30591 net.cpp:226] fc7 needs backward computation.
I0124 13:56:30.660050 30591 net.cpp:226] drop6 needs backward computation.
I0124 13:56:30.660055 30591 net.cpp:226] relu6 needs backward computation.
I0124 13:56:30.660059 30591 net.cpp:226] fc6 needs backward computation.
I0124 13:56:30.660064 30591 net.cpp:226] pool5 needs backward computation.
I0124 13:56:30.660069 30591 net.cpp:226] relu5 needs backward computation.
I0124 13:56:30.660073 30591 net.cpp:226] conv5 needs backward computation.
I0124 13:56:30.660079 30591 net.cpp:226] relu4 needs backward computation.
I0124 13:56:30.660082 30591 net.cpp:226] conv4 needs backward computation.
I0124 13:56:30.660086 30591 net.cpp:226] relu3 needs backward computation.
I0124 13:56:30.660090 30591 net.cpp:226] conv3 needs backward computation.
I0124 13:56:30.660096 30591 net.cpp:226] pool2 needs backward computation.
I0124 13:56:30.660101 30591 net.cpp:226] relu2 needs backward computation.
I0124 13:56:30.660105 30591 net.cpp:226] conv2 needs backward computation.
I0124 13:56:30.660110 30591 net.cpp:226] pool1 needs backward computation.
I0124 13:56:30.660115 30591 net.cpp:226] relu1 needs backward computation.
I0124 13:56:30.660120 30591 net.cpp:226] conv1 needs backward computation.
I0124 13:56:30.660125 30591 net.cpp:228] data does not need backward computation.
I0124 13:56:30.660130 30591 net.cpp:270] This network produces output loss
I0124 13:56:30.660145 30591 net.cpp:283] Network initialization done.
I0124 13:56:30.660260 30591 solver.cpp:181] Creating test net (#0) specified by net_param
I0124 13:56:30.660305 30591 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0124 13:56:30.660501 30591 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.04
    mirror: false
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 123
    force_color: true
    resize_param {
      prob: 1
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: LINEAR
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/old-ufo/datasets/imagenet/ilsvrc12_val_lmdb"
    batch_size: 250
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0124 13:56:30.660643 30591 layer_factory.hpp:76] Creating layer data
I0124 13:56:30.660873 30591 net.cpp:106] Creating Layer data
I0124 13:56:30.660886 30591 net.cpp:411] data -> data
I0124 13:56:30.660897 30591 net.cpp:411] data -> label
I0124 13:56:30.661566 30604 db_lmdb.cpp:38] Opened lmdb /home/old-ufo/datasets/imagenet/ilsvrc12_val_lmdb
I0124 13:56:30.664448 30591 data_layer.cpp:41] output data size: 250,3,128,128
I0124 13:56:30.735831 30591 net.cpp:150] Setting up data
I0124 13:56:30.735863 30591 net.cpp:157] Top shape: 250 3 128 128 (12288000)
I0124 13:56:30.735870 30591 net.cpp:157] Top shape: 250 (250)
I0124 13:56:30.735874 30591 net.cpp:165] Memory required for data: 49153000
I0124 13:56:30.735882 30591 layer_factory.hpp:76] Creating layer label_data_1_split
I0124 13:56:30.735893 30591 net.cpp:106] Creating Layer label_data_1_split
I0124 13:56:30.735898 30591 net.cpp:454] label_data_1_split <- label
I0124 13:56:30.735904 30591 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0124 13:56:30.735913 30591 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0124 13:56:30.735977 30591 net.cpp:150] Setting up label_data_1_split
I0124 13:56:30.735983 30591 net.cpp:157] Top shape: 250 (250)
I0124 13:56:30.735988 30591 net.cpp:157] Top shape: 250 (250)
I0124 13:56:30.735991 30591 net.cpp:165] Memory required for data: 49155000
I0124 13:56:30.735994 30591 layer_factory.hpp:76] Creating layer conv1
I0124 13:56:30.736007 30591 net.cpp:106] Creating Layer conv1
I0124 13:56:30.736011 30591 net.cpp:454] conv1 <- data
I0124 13:56:30.736017 30591 net.cpp:411] conv1 -> conv1
I0124 13:56:30.739797 30591 net.cpp:150] Setting up conv1
I0124 13:56:30.739830 30591 net.cpp:157] Top shape: 250 96 30 30 (21600000)
I0124 13:56:30.739833 30591 net.cpp:165] Memory required for data: 135555000
I0124 13:56:30.739848 30591 layer_factory.hpp:76] Creating layer relu1
I0124 13:56:30.739861 30591 net.cpp:106] Creating Layer relu1
I0124 13:56:30.739866 30591 net.cpp:454] relu1 <- conv1
I0124 13:56:30.739874 30591 net.cpp:411] relu1 -> relu1
I0124 13:56:30.740890 30591 net.cpp:150] Setting up relu1
I0124 13:56:30.740916 30591 net.cpp:157] Top shape: 250 96 30 30 (21600000)
I0124 13:56:30.740921 30591 net.cpp:165] Memory required for data: 221955000
I0124 13:56:30.740927 30591 layer_factory.hpp:76] Creating layer pool1
I0124 13:56:30.740941 30591 net.cpp:106] Creating Layer pool1
I0124 13:56:30.740945 30591 net.cpp:454] pool1 <- relu1
I0124 13:56:30.740953 30591 net.cpp:411] pool1 -> pool1
I0124 13:56:30.741859 30591 net.cpp:150] Setting up pool1
I0124 13:56:30.741885 30591 net.cpp:157] Top shape: 250 96 15 15 (5400000)
I0124 13:56:30.741889 30591 net.cpp:165] Memory required for data: 243555000
I0124 13:56:30.741894 30591 layer_factory.hpp:76] Creating layer conv2
I0124 13:56:30.741910 30591 net.cpp:106] Creating Layer conv2
I0124 13:56:30.741916 30591 net.cpp:454] conv2 <- pool1
I0124 13:56:30.741926 30591 net.cpp:411] conv2 -> conv2
I0124 13:56:30.759232 30591 net.cpp:150] Setting up conv2
I0124 13:56:30.759260 30591 net.cpp:157] Top shape: 250 256 15 15 (14400000)
I0124 13:56:30.759268 30591 net.cpp:165] Memory required for data: 301155000
I0124 13:56:30.759281 30591 layer_factory.hpp:76] Creating layer relu2
I0124 13:56:30.759292 30591 net.cpp:106] Creating Layer relu2
I0124 13:56:30.759316 30591 net.cpp:454] relu2 <- conv2
I0124 13:56:30.759327 30591 net.cpp:397] relu2 -> conv2 (in-place)
I0124 13:56:30.760094 30591 net.cpp:150] Setting up relu2
I0124 13:56:30.760108 30591 net.cpp:157] Top shape: 250 256 15 15 (14400000)
I0124 13:56:30.760113 30591 net.cpp:165] Memory required for data: 358755000
I0124 13:56:30.760118 30591 layer_factory.hpp:76] Creating layer pool2
I0124 13:56:30.760125 30591 net.cpp:106] Creating Layer pool2
I0124 13:56:30.760129 30591 net.cpp:454] pool2 <- conv2
I0124 13:56:30.760138 30591 net.cpp:411] pool2 -> pool2
I0124 13:56:30.761010 30591 net.cpp:150] Setting up pool2
I0124 13:56:30.761026 30591 net.cpp:157] Top shape: 250 256 7 7 (3136000)
I0124 13:56:30.761030 30591 net.cpp:165] Memory required for data: 371299000
I0124 13:56:30.761035 30591 layer_factory.hpp:76] Creating layer conv3
I0124 13:56:30.761050 30591 net.cpp:106] Creating Layer conv3
I0124 13:56:30.761057 30591 net.cpp:454] conv3 <- pool2
I0124 13:56:30.761065 30591 net.cpp:411] conv3 -> conv3
I0124 13:56:30.797188 30591 net.cpp:150] Setting up conv3
I0124 13:56:30.797217 30591 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 13:56:30.797224 30591 net.cpp:165] Memory required for data: 390115000
I0124 13:56:30.797240 30591 layer_factory.hpp:76] Creating layer relu3
I0124 13:56:30.797253 30591 net.cpp:106] Creating Layer relu3
I0124 13:56:30.797260 30591 net.cpp:454] relu3 <- conv3
I0124 13:56:30.797268 30591 net.cpp:397] relu3 -> conv3 (in-place)
I0124 13:56:30.798132 30591 net.cpp:150] Setting up relu3
I0124 13:56:30.798147 30591 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 13:56:30.798152 30591 net.cpp:165] Memory required for data: 408931000
I0124 13:56:30.798157 30591 layer_factory.hpp:76] Creating layer conv4
I0124 13:56:30.798171 30591 net.cpp:106] Creating Layer conv4
I0124 13:56:30.798178 30591 net.cpp:454] conv4 <- conv3
I0124 13:56:30.798188 30591 net.cpp:411] conv4 -> conv4
I0124 13:56:30.828768 30591 net.cpp:150] Setting up conv4
I0124 13:56:30.828796 30591 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 13:56:30.828800 30591 net.cpp:165] Memory required for data: 427747000
I0124 13:56:30.828812 30591 layer_factory.hpp:76] Creating layer relu4
I0124 13:56:30.828824 30591 net.cpp:106] Creating Layer relu4
I0124 13:56:30.828830 30591 net.cpp:454] relu4 <- conv4
I0124 13:56:30.828842 30591 net.cpp:397] relu4 -> conv4 (in-place)
I0124 13:56:30.829690 30591 net.cpp:150] Setting up relu4
I0124 13:56:30.829716 30591 net.cpp:157] Top shape: 250 384 7 7 (4704000)
I0124 13:56:30.829720 30591 net.cpp:165] Memory required for data: 446563000
I0124 13:56:30.829725 30591 layer_factory.hpp:76] Creating layer conv5
I0124 13:56:30.829740 30591 net.cpp:106] Creating Layer conv5
I0124 13:56:30.829746 30591 net.cpp:454] conv5 <- conv4
I0124 13:56:30.829757 30591 net.cpp:411] conv5 -> conv5
I0124 13:56:30.851709 30591 net.cpp:150] Setting up conv5
I0124 13:56:30.851737 30591 net.cpp:157] Top shape: 250 256 7 7 (3136000)
I0124 13:56:30.851740 30591 net.cpp:165] Memory required for data: 459107000
I0124 13:56:30.851757 30591 layer_factory.hpp:76] Creating layer relu5
I0124 13:56:30.851766 30591 net.cpp:106] Creating Layer relu5
I0124 13:56:30.851773 30591 net.cpp:454] relu5 <- conv5
I0124 13:56:30.851780 30591 net.cpp:397] relu5 -> conv5 (in-place)
I0124 13:56:30.852605 30591 net.cpp:150] Setting up relu5
I0124 13:56:30.852623 30591 net.cpp:157] Top shape: 250 256 7 7 (3136000)
I0124 13:56:30.852628 30591 net.cpp:165] Memory required for data: 471651000
I0124 13:56:30.852632 30591 layer_factory.hpp:76] Creating layer pool5
I0124 13:56:30.852643 30591 net.cpp:106] Creating Layer pool5
I0124 13:56:30.852648 30591 net.cpp:454] pool5 <- conv5
I0124 13:56:30.852656 30591 net.cpp:411] pool5 -> pool5
I0124 13:56:30.853519 30591 net.cpp:150] Setting up pool5
I0124 13:56:30.853535 30591 net.cpp:157] Top shape: 250 256 3 3 (576000)
I0124 13:56:30.853539 30591 net.cpp:165] Memory required for data: 473955000
I0124 13:56:30.853544 30591 layer_factory.hpp:76] Creating layer fc6
I0124 13:56:30.853572 30591 net.cpp:106] Creating Layer fc6
I0124 13:56:30.853579 30591 net.cpp:454] fc6 <- pool5
I0124 13:56:30.853590 30591 net.cpp:411] fc6 -> fc6
I0124 13:56:31.026444 30591 net.cpp:150] Setting up fc6
I0124 13:56:31.026473 30591 net.cpp:157] Top shape: 250 2048 (512000)
I0124 13:56:31.026478 30591 net.cpp:165] Memory required for data: 476003000
I0124 13:56:31.026489 30591 layer_factory.hpp:76] Creating layer relu6
I0124 13:56:31.026501 30591 net.cpp:106] Creating Layer relu6
I0124 13:56:31.026507 30591 net.cpp:454] relu6 <- fc6
I0124 13:56:31.026515 30591 net.cpp:397] relu6 -> fc6 (in-place)
I0124 13:56:31.027734 30591 net.cpp:150] Setting up relu6
I0124 13:56:31.027750 30591 net.cpp:157] Top shape: 250 2048 (512000)
I0124 13:56:31.027755 30591 net.cpp:165] Memory required for data: 478051000
I0124 13:56:31.027760 30591 layer_factory.hpp:76] Creating layer drop6
I0124 13:56:31.027770 30591 net.cpp:106] Creating Layer drop6
I0124 13:56:31.027776 30591 net.cpp:454] drop6 <- fc6
I0124 13:56:31.027784 30591 net.cpp:397] drop6 -> fc6 (in-place)
I0124 13:56:31.027829 30591 net.cpp:150] Setting up drop6
I0124 13:56:31.027837 30591 net.cpp:157] Top shape: 250 2048 (512000)
I0124 13:56:31.027842 30591 net.cpp:165] Memory required for data: 480099000
I0124 13:56:31.027845 30591 layer_factory.hpp:76] Creating layer fc7
I0124 13:56:31.027855 30591 net.cpp:106] Creating Layer fc7
I0124 13:56:31.027860 30591 net.cpp:454] fc7 <- fc6
I0124 13:56:31.027868 30591 net.cpp:411] fc7 -> fc7
I0124 13:56:31.180554 30591 net.cpp:150] Setting up fc7
I0124 13:56:31.180588 30591 net.cpp:157] Top shape: 250 2048 (512000)
I0124 13:56:31.180593 30591 net.cpp:165] Memory required for data: 482147000
I0124 13:56:31.180606 30591 layer_factory.hpp:76] Creating layer relu7
I0124 13:56:31.180618 30591 net.cpp:106] Creating Layer relu7
I0124 13:56:31.180624 30591 net.cpp:454] relu7 <- fc7
I0124 13:56:31.180631 30591 net.cpp:397] relu7 -> fc7 (in-place)
I0124 13:56:31.181694 30591 net.cpp:150] Setting up relu7
I0124 13:56:31.181717 30591 net.cpp:157] Top shape: 250 2048 (512000)
I0124 13:56:31.181722 30591 net.cpp:165] Memory required for data: 484195000
I0124 13:56:31.181726 30591 layer_factory.hpp:76] Creating layer drop7
I0124 13:56:31.181735 30591 net.cpp:106] Creating Layer drop7
I0124 13:56:31.181740 30591 net.cpp:454] drop7 <- fc7
I0124 13:56:31.181746 30591 net.cpp:397] drop7 -> fc7 (in-place)
I0124 13:56:31.181793 30591 net.cpp:150] Setting up drop7
I0124 13:56:31.181802 30591 net.cpp:157] Top shape: 250 2048 (512000)
I0124 13:56:31.181807 30591 net.cpp:165] Memory required for data: 486243000
I0124 13:56:31.181810 30591 layer_factory.hpp:76] Creating layer fc8
I0124 13:56:31.181820 30591 net.cpp:106] Creating Layer fc8
I0124 13:56:31.181825 30591 net.cpp:454] fc8 <- fc7
I0124 13:56:31.181833 30591 net.cpp:411] fc8 -> fc8
I0124 13:56:31.255678 30591 net.cpp:150] Setting up fc8
I0124 13:56:31.255707 30591 net.cpp:157] Top shape: 250 1000 (250000)
I0124 13:56:31.255710 30591 net.cpp:165] Memory required for data: 487243000
I0124 13:56:31.255722 30591 layer_factory.hpp:76] Creating layer fc8_fc8_0_split
I0124 13:56:31.255734 30591 net.cpp:106] Creating Layer fc8_fc8_0_split
I0124 13:56:31.255740 30591 net.cpp:454] fc8_fc8_0_split <- fc8
I0124 13:56:31.255749 30591 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0124 13:56:31.255764 30591 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0124 13:56:31.255872 30591 net.cpp:150] Setting up fc8_fc8_0_split
I0124 13:56:31.255884 30591 net.cpp:157] Top shape: 250 1000 (250000)
I0124 13:56:31.255892 30591 net.cpp:157] Top shape: 250 1000 (250000)
I0124 13:56:31.255895 30591 net.cpp:165] Memory required for data: 489243000
I0124 13:56:31.255899 30591 layer_factory.hpp:76] Creating layer accuracy
I0124 13:56:31.255913 30591 net.cpp:106] Creating Layer accuracy
I0124 13:56:31.255919 30591 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0124 13:56:31.255925 30591 net.cpp:454] accuracy <- label_data_1_split_0
I0124 13:56:31.255931 30591 net.cpp:411] accuracy -> accuracy
I0124 13:56:31.255960 30591 net.cpp:150] Setting up accuracy
I0124 13:56:31.255967 30591 net.cpp:157] Top shape: (1)
I0124 13:56:31.255971 30591 net.cpp:165] Memory required for data: 489243004
I0124 13:56:31.255975 30591 layer_factory.hpp:76] Creating layer loss
I0124 13:56:31.255985 30591 net.cpp:106] Creating Layer loss
I0124 13:56:31.255990 30591 net.cpp:454] loss <- fc8_fc8_0_split_1
I0124 13:56:31.255995 30591 net.cpp:454] loss <- label_data_1_split_1
I0124 13:56:31.256001 30591 net.cpp:411] loss -> loss
I0124 13:56:31.256011 30591 layer_factory.hpp:76] Creating layer loss
I0124 13:56:31.257928 30591 net.cpp:150] Setting up loss
I0124 13:56:31.257944 30591 net.cpp:157] Top shape: (1)
I0124 13:56:31.257948 30591 net.cpp:160]     with loss weight 1
I0124 13:56:31.257961 30591 net.cpp:165] Memory required for data: 489243008
I0124 13:56:31.257966 30591 net.cpp:226] loss needs backward computation.
I0124 13:56:31.257971 30591 net.cpp:228] accuracy does not need backward computation.
I0124 13:56:31.257975 30591 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0124 13:56:31.257979 30591 net.cpp:226] fc8 needs backward computation.
I0124 13:56:31.257983 30591 net.cpp:226] drop7 needs backward computation.
I0124 13:56:31.257987 30591 net.cpp:226] relu7 needs backward computation.
I0124 13:56:31.257990 30591 net.cpp:226] fc7 needs backward computation.
I0124 13:56:31.257994 30591 net.cpp:226] drop6 needs backward computation.
I0124 13:56:31.257998 30591 net.cpp:226] relu6 needs backward computation.
I0124 13:56:31.258002 30591 net.cpp:226] fc6 needs backward computation.
I0124 13:56:31.258007 30591 net.cpp:226] pool5 needs backward computation.
I0124 13:56:31.258010 30591 net.cpp:226] relu5 needs backward computation.
I0124 13:56:31.258016 30591 net.cpp:226] conv5 needs backward computation.
I0124 13:56:31.258020 30591 net.cpp:226] relu4 needs backward computation.
I0124 13:56:31.258024 30591 net.cpp:226] conv4 needs backward computation.
I0124 13:56:31.258028 30591 net.cpp:226] relu3 needs backward computation.
I0124 13:56:31.258031 30591 net.cpp:226] conv3 needs backward computation.
I0124 13:56:31.258036 30591 net.cpp:226] pool2 needs backward computation.
I0124 13:56:31.258040 30591 net.cpp:226] relu2 needs backward computation.
I0124 13:56:31.258044 30591 net.cpp:226] conv2 needs backward computation.
I0124 13:56:31.258049 30591 net.cpp:226] pool1 needs backward computation.
I0124 13:56:31.258052 30591 net.cpp:226] relu1 needs backward computation.
I0124 13:56:31.258056 30591 net.cpp:226] conv1 needs backward computation.
I0124 13:56:31.258061 30591 net.cpp:228] label_data_1_split does not need backward computation.
I0124 13:56:31.258065 30591 net.cpp:228] data does not need backward computation.
I0124 13:56:31.258069 30591 net.cpp:270] This network produces output accuracy
I0124 13:56:31.258074 30591 net.cpp:270] This network produces output loss
I0124 13:56:31.258095 30591 net.cpp:283] Network initialization done.
I0124 13:56:31.258203 30591 solver.cpp:60] Solver scaffolding done.
I0124 13:56:31.259063 30591 caffe.cpp:128] Finetuning from ./caffenet_lsuv_rmsprop_098_lr0001.prototxt.caffemodel
I0124 13:56:31.533581 30591 caffe.cpp:212] Starting Optimization
I0124 13:56:31.533607 30591 solver.cpp:288] Solving CaffeNet
I0124 13:56:31.533610 30591 solver.cpp:289] Learning Rate Policy: step
I0124 13:56:31.589750 30591 solver.cpp:237] Iteration 0, loss = 7.40145
I0124 13:56:31.589786 30591 solver.cpp:253]     Train net output #0: loss = 7.40145 (* 1 = 7.40145 loss)
I0124 13:56:31.589802 30591 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0124 13:56:31.695256 30591 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 13:56:38.692893 30591 solver.cpp:237] Iteration 20, loss = 11.3233
I0124 13:56:38.692932 30591 solver.cpp:253]     Train net output #0: loss = 6.90283 (* 1 = 6.90283 loss)
I0124 13:56:38.692942 30591 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I0124 13:56:46.320631 30591 solver.cpp:237] Iteration 40, loss = 6.91219
I0124 13:56:46.320667 30591 solver.cpp:253]     Train net output #0: loss = 6.9106 (* 1 = 6.9106 loss)
I0124 13:56:46.320703 30591 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I0124 13:56:53.878815 30591 solver.cpp:237] Iteration 60, loss = 6.90922
I0124 13:56:53.878854 30591 solver.cpp:253]     Train net output #0: loss = 6.90612 (* 1 = 6.90612 loss)
I0124 13:56:53.878865 30591 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I0124 13:57:01.532845 30591 solver.cpp:237] Iteration 80, loss = 6.90935
I0124 13:57:01.532990 30591 solver.cpp:253]     Train net output #0: loss = 6.9103 (* 1 = 6.9103 loss)
I0124 13:57:01.533006 30591 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I0124 13:57:09.173105 30591 solver.cpp:237] Iteration 100, loss = 6.90783
I0124 13:57:09.173138 30591 solver.cpp:253]     Train net output #0: loss = 6.90798 (* 1 = 6.90798 loss)
I0124 13:57:09.173149 30591 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I0124 13:57:16.532824 30591 solver.cpp:237] Iteration 120, loss = 6.90784
I0124 13:57:16.532919 30591 solver.cpp:253]     Train net output #0: loss = 6.90796 (* 1 = 6.90796 loss)
I0124 13:57:16.532955 30591 sgd_solver.cpp:106] Iteration 120, lr = 0.001
I0124 13:57:23.933893 30591 solver.cpp:237] Iteration 140, loss = 6.91279
I0124 13:57:23.933929 30591 solver.cpp:253]     Train net output #0: loss = 7.00803 (* 1 = 7.00803 loss)
I0124 13:57:23.933939 30591 sgd_solver.cpp:106] Iteration 140, lr = 0.001
I0124 13:57:31.371129 30591 solver.cpp:237] Iteration 160, loss = 6.90802
I0124 13:57:31.371163 30591 solver.cpp:253]     Train net output #0: loss = 6.90732 (* 1 = 6.90732 loss)
I0124 13:57:31.371175 30591 sgd_solver.cpp:106] Iteration 160, lr = 0.001
I0124 13:57:38.852936 30591 solver.cpp:237] Iteration 180, loss = 6.90844
I0124 13:57:38.853054 30591 solver.cpp:253]     Train net output #0: loss = 6.90662 (* 1 = 6.90662 loss)
I0124 13:57:38.853066 30591 sgd_solver.cpp:106] Iteration 180, lr = 0.001
I0124 13:57:46.270102 30591 solver.cpp:237] Iteration 200, loss = 6.9138
I0124 13:57:46.270135 30591 solver.cpp:253]     Train net output #0: loss = 6.90696 (* 1 = 6.90696 loss)
I0124 13:57:46.270146 30591 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0124 13:57:53.976984 30591 solver.cpp:237] Iteration 220, loss = 6.91541
I0124 13:57:53.977207 30591 solver.cpp:253]     Train net output #0: loss = 6.90637 (* 1 = 6.90637 loss)
I0124 13:57:53.977289 30591 sgd_solver.cpp:106] Iteration 220, lr = 0.001
I0124 13:58:02.160662 30591 solver.cpp:237] Iteration 240, loss = 6.90762
I0124 13:58:02.160697 30591 solver.cpp:253]     Train net output #0: loss = 6.90739 (* 1 = 6.90739 loss)
I0124 13:58:02.160707 30591 sgd_solver.cpp:106] Iteration 240, lr = 0.001
I0124 13:58:11.976790 30591 solver.cpp:237] Iteration 260, loss = 6.90675
I0124 13:58:11.977063 30591 solver.cpp:253]     Train net output #0: loss = 6.89739 (* 1 = 6.89739 loss)
I0124 13:58:11.977074 30591 sgd_solver.cpp:106] Iteration 260, lr = 0.001
I0124 13:58:19.788131 30591 solver.cpp:237] Iteration 280, loss = 6.92637
I0124 13:58:19.788179 30591 solver.cpp:253]     Train net output #0: loss = 6.90328 (* 1 = 6.90328 loss)
I0124 13:58:19.788197 30591 sgd_solver.cpp:106] Iteration 280, lr = 0.001
I0124 13:58:27.299614 30591 solver.cpp:237] Iteration 300, loss = 6.90792
I0124 13:58:27.299656 30591 solver.cpp:253]     Train net output #0: loss = 6.9004 (* 1 = 6.9004 loss)
I0124 13:58:27.299666 30591 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I0124 13:58:34.857049 30591 solver.cpp:237] Iteration 320, loss = 6.90679
I0124 13:58:34.857082 30591 solver.cpp:253]     Train net output #0: loss = 6.89713 (* 1 = 6.89713 loss)
I0124 13:58:34.857091 30591 sgd_solver.cpp:106] Iteration 320, lr = 0.001
I0124 13:58:42.444628 30591 solver.cpp:237] Iteration 340, loss = 6.95692
I0124 13:58:42.444705 30591 solver.cpp:253]     Train net output #0: loss = 6.91244 (* 1 = 6.91244 loss)
I0124 13:58:42.444715 30591 sgd_solver.cpp:106] Iteration 340, lr = 0.001
I0124 13:58:50.036098 30591 solver.cpp:237] Iteration 360, loss = 6.90639
I0124 13:58:50.036299 30591 solver.cpp:253]     Train net output #0: loss = 6.91219 (* 1 = 6.91219 loss)
I0124 13:58:50.036409 30591 sgd_solver.cpp:106] Iteration 360, lr = 0.001
I0124 13:58:57.614236 30591 solver.cpp:237] Iteration 380, loss = 6.9304
I0124 13:58:57.614269 30591 solver.cpp:253]     Train net output #0: loss = 6.91743 (* 1 = 6.91743 loss)
I0124 13:58:57.614279 30591 sgd_solver.cpp:106] Iteration 380, lr = 0.001
I0124 13:59:05.212172 30591 solver.cpp:237] Iteration 400, loss = 6.90528
I0124 13:59:05.212210 30591 solver.cpp:253]     Train net output #0: loss = 6.90624 (* 1 = 6.90624 loss)
I0124 13:59:05.212220 30591 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0124 13:59:12.965833 30591 solver.cpp:237] Iteration 420, loss = 6.90374
I0124 13:59:12.965950 30591 solver.cpp:253]     Train net output #0: loss = 6.90088 (* 1 = 6.90088 loss)
I0124 13:59:12.965960 30591 sgd_solver.cpp:106] Iteration 420, lr = 0.001
I0124 13:59:20.716377 30591 solver.cpp:237] Iteration 440, loss = 6.90768
I0124 13:59:20.716415 30591 solver.cpp:253]     Train net output #0: loss = 6.90666 (* 1 = 6.90666 loss)
I0124 13:59:20.716425 30591 sgd_solver.cpp:106] Iteration 440, lr = 0.001
I0124 13:59:28.336105 30591 solver.cpp:237] Iteration 460, loss = 6.91799
I0124 13:59:28.336141 30591 solver.cpp:253]     Train net output #0: loss = 6.90836 (* 1 = 6.90836 loss)
I0124 13:59:28.336153 30591 sgd_solver.cpp:106] Iteration 460, lr = 0.001
I0124 13:59:35.846447 30591 solver.cpp:237] Iteration 480, loss = 6.91347
I0124 13:59:35.846479 30591 solver.cpp:253]     Train net output #0: loss = 6.89536 (* 1 = 6.89536 loss)
I0124 13:59:35.846488 30591 sgd_solver.cpp:106] Iteration 480, lr = 0.001
I0124 13:59:43.413439 30591 solver.cpp:237] Iteration 500, loss = 6.90959
I0124 13:59:43.413516 30591 solver.cpp:253]     Train net output #0: loss = 6.94369 (* 1 = 6.94369 loss)
I0124 13:59:43.413527 30591 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I0124 13:59:50.992534 30591 solver.cpp:237] Iteration 520, loss = 6.91413
I0124 13:59:50.992631 30591 solver.cpp:253]     Train net output #0: loss = 6.9047 (* 1 = 6.9047 loss)
I0124 13:59:50.992662 30591 sgd_solver.cpp:106] Iteration 520, lr = 0.001
I0124 13:59:58.675112 30591 solver.cpp:237] Iteration 540, loss = 6.90555
I0124 13:59:58.675140 30591 solver.cpp:253]     Train net output #0: loss = 6.89925 (* 1 = 6.89925 loss)
I0124 13:59:58.675148 30591 sgd_solver.cpp:106] Iteration 540, lr = 0.001
I0124 14:00:06.290693 30591 solver.cpp:237] Iteration 560, loss = 6.90978
I0124 14:00:06.290746 30591 solver.cpp:253]     Train net output #0: loss = 6.90412 (* 1 = 6.90412 loss)
I0124 14:00:06.290756 30591 sgd_solver.cpp:106] Iteration 560, lr = 0.001
I0124 14:00:14.161731 30591 solver.cpp:237] Iteration 580, loss = 6.90379
I0124 14:00:14.161799 30591 solver.cpp:253]     Train net output #0: loss = 6.90352 (* 1 = 6.90352 loss)
I0124 14:00:14.161806 30591 sgd_solver.cpp:106] Iteration 580, lr = 0.001
I0124 14:00:22.376757 30591 solver.cpp:237] Iteration 600, loss = 6.91403
I0124 14:00:22.376799 30591 solver.cpp:253]     Train net output #0: loss = 6.90268 (* 1 = 6.90268 loss)
I0124 14:00:22.376812 30591 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0124 14:00:30.180342 30591 solver.cpp:237] Iteration 620, loss = 6.90822
I0124 14:00:30.180390 30591 solver.cpp:253]     Train net output #0: loss = 6.91647 (* 1 = 6.91647 loss)
I0124 14:00:30.180403 30591 sgd_solver.cpp:106] Iteration 620, lr = 0.001
I0124 14:00:37.936506 30591 solver.cpp:237] Iteration 640, loss = 6.90518
I0124 14:00:37.936538 30591 solver.cpp:253]     Train net output #0: loss = 6.91028 (* 1 = 6.91028 loss)
I0124 14:00:37.936548 30591 sgd_solver.cpp:106] Iteration 640, lr = 0.001
I0124 14:00:45.745641 30591 solver.cpp:237] Iteration 660, loss = 6.92532
I0124 14:00:45.745762 30591 solver.cpp:253]     Train net output #0: loss = 6.97779 (* 1 = 6.97779 loss)
I0124 14:00:45.745785 30591 sgd_solver.cpp:106] Iteration 660, lr = 0.001
I0124 14:00:53.574693 30591 solver.cpp:237] Iteration 680, loss = 6.95256
I0124 14:00:53.574729 30591 solver.cpp:253]     Train net output #0: loss = 7.25311 (* 1 = 7.25311 loss)
I0124 14:00:53.574738 30591 sgd_solver.cpp:106] Iteration 680, lr = 0.001
I0124 14:01:01.170680 30591 solver.cpp:237] Iteration 700, loss = 6.90623
I0124 14:01:01.170713 30591 solver.cpp:253]     Train net output #0: loss = 6.90019 (* 1 = 6.90019 loss)
I0124 14:01:01.170720 30591 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I0124 14:01:08.761729 30591 solver.cpp:237] Iteration 720, loss = 6.95404
I0124 14:01:08.761762 30591 solver.cpp:253]     Train net output #0: loss = 6.9053 (* 1 = 6.9053 loss)
I0124 14:01:08.761773 30591 sgd_solver.cpp:106] Iteration 720, lr = 0.001
I0124 14:01:16.284227 30591 solver.cpp:237] Iteration 740, loss = 6.91814
I0124 14:01:16.284380 30591 solver.cpp:253]     Train net output #0: loss = 6.90774 (* 1 = 6.90774 loss)
I0124 14:01:16.284394 30591 sgd_solver.cpp:106] Iteration 740, lr = 0.001
I0124 14:01:23.808152 30591 solver.cpp:237] Iteration 760, loss = 6.90482
I0124 14:01:23.808188 30591 solver.cpp:253]     Train net output #0: loss = 6.89852 (* 1 = 6.89852 loss)
I0124 14:01:23.808199 30591 sgd_solver.cpp:106] Iteration 760, lr = 0.001
I0124 14:01:31.337702 30591 solver.cpp:237] Iteration 780, loss = 6.91913
I0124 14:01:31.337735 30591 solver.cpp:253]     Train net output #0: loss = 6.91323 (* 1 = 6.91323 loss)
I0124 14:01:31.337745 30591 sgd_solver.cpp:106] Iteration 780, lr = 0.001
I0124 14:01:38.907260 30591 solver.cpp:237] Iteration 800, loss = 6.92439
I0124 14:01:38.907296 30591 solver.cpp:253]     Train net output #0: loss = 7.11618 (* 1 = 7.11618 loss)
I0124 14:01:38.907307 30591 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0124 14:01:46.450876 30591 solver.cpp:237] Iteration 820, loss = 6.91003
I0124 14:01:46.450952 30591 solver.cpp:253]     Train net output #0: loss = 6.91221 (* 1 = 6.91221 loss)
I0124 14:01:46.450959 30591 sgd_solver.cpp:106] Iteration 820, lr = 0.001
I0124 14:01:53.901940 30591 solver.cpp:237] Iteration 840, loss = 6.91469
I0124 14:01:53.901973 30591 solver.cpp:253]     Train net output #0: loss = 6.90521 (* 1 = 6.90521 loss)
I0124 14:01:53.901981 30591 sgd_solver.cpp:106] Iteration 840, lr = 0.001
I0124 14:02:01.390787 30591 solver.cpp:237] Iteration 860, loss = 6.93231
I0124 14:02:01.390820 30591 solver.cpp:253]     Train net output #0: loss = 6.90771 (* 1 = 6.90771 loss)
I0124 14:02:01.390826 30591 sgd_solver.cpp:106] Iteration 860, lr = 0.001
I0124 14:02:09.013401 30591 solver.cpp:237] Iteration 880, loss = 6.93306
I0124 14:02:09.013445 30591 solver.cpp:253]     Train net output #0: loss = 6.92329 (* 1 = 6.92329 loss)
I0124 14:02:09.013458 30591 sgd_solver.cpp:106] Iteration 880, lr = 0.001
I0124 14:02:16.460053 30591 solver.cpp:237] Iteration 900, loss = 6.91028
I0124 14:02:16.460136 30591 solver.cpp:253]     Train net output #0: loss = 6.90548 (* 1 = 6.90548 loss)
I0124 14:02:16.460145 30591 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I0124 14:02:23.897624 30591 solver.cpp:237] Iteration 920, loss = 6.91246
I0124 14:02:23.897721 30591 solver.cpp:253]     Train net output #0: loss = 6.90147 (* 1 = 6.90147 loss)
I0124 14:02:23.897747 30591 sgd_solver.cpp:106] Iteration 920, lr = 0.001
I0124 14:02:31.383105 30591 solver.cpp:237] Iteration 940, loss = 6.92091
I0124 14:02:31.383141 30591 solver.cpp:253]     Train net output #0: loss = 7.1683 (* 1 = 7.1683 loss)
I0124 14:02:31.383148 30591 sgd_solver.cpp:106] Iteration 940, lr = 0.001
I0124 14:02:38.808907 30591 solver.cpp:237] Iteration 960, loss = 10.6454
I0124 14:02:38.809018 30591 solver.cpp:253]     Train net output #0: loss = 6.90835 (* 1 = 6.90835 loss)
I0124 14:02:38.809048 30591 sgd_solver.cpp:106] Iteration 960, lr = 0.001
I0124 14:02:46.317739 30591 solver.cpp:237] Iteration 980, loss = 6.9073
I0124 14:02:46.317777 30591 solver.cpp:253]     Train net output #0: loss = 6.90251 (* 1 = 6.90251 loss)
I0124 14:02:46.317788 30591 sgd_solver.cpp:106] Iteration 980, lr = 0.001
I0124 14:02:53.829324 30591 solver.cpp:237] Iteration 1000, loss = 6.91195
I0124 14:02:53.829398 30591 solver.cpp:253]     Train net output #0: loss = 6.90641 (* 1 = 6.90641 loss)
I0124 14:02:53.829408 30591 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I0124 14:02:54.199831 30591 blocking_queue.cpp:50] Data layer prefetch queue empty
I0124 14:03:01.393399 30591 solver.cpp:237] Iteration 1020, loss = 6.90869
I0124 14:03:01.393676 30591 solver.cpp:253]     Train net output #0: loss = 6.90757 (* 1 = 6.90757 loss)
I0124 14:03:01.393756 30591 sgd_solver.cpp:106] Iteration 1020, lr = 0.001
I0124 14:03:09.000001 30591 solver.cpp:237] Iteration 1040, loss = 6.90445
I0124 14:03:09.000036 30591 solver.cpp:253]     Train net output #0: loss = 6.89809 (* 1 = 6.89809 loss)
I0124 14:03:09.000046 30591 sgd_solver.cpp:106] Iteration 1040, lr = 0.001
I0124 14:03:16.713073 30591 solver.cpp:237] Iteration 1060, loss = 6.91025
I0124 14:03:16.713107 30591 solver.cpp:253]     Train net output #0: loss = 6.90301 (* 1 = 6.90301 loss)
I0124 14:03:16.713119 30591 sgd_solver.cpp:106] Iteration 1060, lr = 0.001
I0124 14:03:24.332764 30591 solver.cpp:237] Iteration 1080, loss = 6.91642
I0124 14:03:24.332895 30591 solver.cpp:253]     Train net output #0: loss = 6.90128 (* 1 = 6.90128 loss)
I0124 14:03:24.332906 30591 sgd_solver.cpp:106] Iteration 1080, lr = 0.001
