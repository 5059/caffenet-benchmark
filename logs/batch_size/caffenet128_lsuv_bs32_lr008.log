I0402 07:38:10.338021 28499 upgrade_proto.cpp:990] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': ./caffenet128_lsuv_bs32_lr008.prototxt
I0402 07:38:10.338207 28499 upgrade_proto.cpp:997] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W0402 07:38:10.338213 28499 upgrade_proto.cpp:999] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I0402 07:38:10.338307 28499 caffe.cpp:184] Using GPUs 0
I0402 07:38:10.506273 28499 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.08
display: 20
max_iter: 2560000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 800000
snapshot: 80000
snapshot_prefix: "snapshots/caffenet128_no_lrn_lsuv_tanh_bs32_lr008"
solver_mode: GPU
device_id: 0
net_param {
  name: "CaffeNet"
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TRAIN
    }
    transform_param {
      scale: 0.04
      mirror: true
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 124
      force_color: true
    }
    data_param {
      source: "/home/old-ufo/storage/datasets/imagenet/imgs128/lmdb/ilsvrc12_128_train_lmdb"
      batch_size: 32
      backend: LMDB
    }
  }
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TEST
    }
    transform_param {
      scale: 0.04
      mirror: false
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 123
      force_color: true
    }
    data_param {
      source: "/home/old-ufo/storage/datasets/imagenet/imgs128/lmdb/ilsvrc12_128_val_lmdb"
      batch_size: 50
      backend: LMDB
    }
  }
  layer {
    name: "conv1"
    type: "Convolution"
    bottom: "data"
    top: "conv1"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 96
      kernel_size: 11
      stride: 4
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu1"
    type: "TanH"
    bottom: "conv1"
    top: "relu1"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "relu1"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv2"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 2
      kernel_size: 5
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu2"
    type: "TanH"
    bottom: "conv2"
    top: "relu2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "relu2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv3"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu3"
    type: "TanH"
    bottom: "conv3"
    top: "relu3"
  }
  layer {
    name: "conv4"
    type: "Convolution"
    bottom: "relu3"
    top: "conv4"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu4"
    type: "TanH"
    bottom: "conv4"
    top: "relu4"
  }
  layer {
    name: "conv5"
    type: "Convolution"
    bottom: "relu4"
    top: "conv5"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu5"
    type: "TanH"
    bottom: "conv5"
    top: "relu5"
  }
  layer {
    name: "pool5"
    type: "Pooling"
    bottom: "relu5"
    top: "pool5"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "pool5"
    top: "fc6"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu6"
    type: "TanH"
    bottom: "fc6"
    top: "relu6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "relu6"
    top: "drop6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "drop6"
    top: "fc7"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu7"
    type: "TanH"
    bottom: "fc7"
    top: "relu7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "relu7"
    top: "drop7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc8"
    type: "InnerProduct"
    bottom: "drop7"
    top: "fc8"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 1000
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "accuracy"
    type: "Accuracy"
    bottom: "fc8"
    bottom: "label"
    top: "accuracy"
    include {
      phase: TEST
    }
  }
  layer {
    name: "loss"
    type: "SoftmaxWithLoss"
    bottom: "fc8"
    bottom: "label"
    top: "loss"
  }
}
test_initialization: true
iter_size: 1
type: "SGD"
I0402 07:38:10.506460 28499 solver.cpp:86] Creating training net specified in net_param.
I0402 07:38:10.506578 28499 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0402 07:38:10.506629 28499 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0402 07:38:10.506826 28499 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.04
    mirror: true
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 124
    force_color: true
  }
  data_param {
    source: "/home/old-ufo/storage/datasets/imagenet/imgs128/lmdb/ilsvrc12_128_train_lmdb"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "TanH"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "TanH"
  bottom: "conv2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "TanH"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "TanH"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "TanH"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "TanH"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "TanH"
  bottom: "fc7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0402 07:38:10.506964 28499 layer_factory.hpp:76] Creating layer data
I0402 07:38:10.507441 28499 net.cpp:106] Creating Layer data
I0402 07:38:10.507472 28499 net.cpp:411] data -> data
I0402 07:38:10.507520 28499 net.cpp:411] data -> label
I0402 07:38:10.508098 28504 db_lmdb.cpp:38] Opened lmdb /home/old-ufo/storage/datasets/imagenet/imgs128/lmdb/ilsvrc12_128_train_lmdb
I0402 07:38:10.518282 28499 data_layer.cpp:41] output data size: 32,3,128,128
I0402 07:38:10.527515 28499 net.cpp:150] Setting up data
I0402 07:38:10.527629 28499 net.cpp:157] Top shape: 32 3 128 128 (1572864)
I0402 07:38:10.527665 28499 net.cpp:157] Top shape: 32 (32)
I0402 07:38:10.527688 28499 net.cpp:165] Memory required for data: 6291584
I0402 07:38:10.527729 28499 layer_factory.hpp:76] Creating layer conv1
I0402 07:38:10.527767 28499 net.cpp:106] Creating Layer conv1
I0402 07:38:10.527791 28499 net.cpp:454] conv1 <- data
I0402 07:38:10.527828 28499 net.cpp:411] conv1 -> conv1
I0402 07:38:10.694905 28499 net.cpp:150] Setting up conv1
I0402 07:38:10.694934 28499 net.cpp:157] Top shape: 32 96 30 30 (2764800)
I0402 07:38:10.694939 28499 net.cpp:165] Memory required for data: 17350784
I0402 07:38:10.694962 28499 layer_factory.hpp:76] Creating layer relu1
I0402 07:38:10.694977 28499 net.cpp:106] Creating Layer relu1
I0402 07:38:10.694982 28499 net.cpp:454] relu1 <- conv1
I0402 07:38:10.694988 28499 net.cpp:411] relu1 -> relu1
I0402 07:38:10.695716 28499 net.cpp:150] Setting up relu1
I0402 07:38:10.695730 28499 net.cpp:157] Top shape: 32 96 30 30 (2764800)
I0402 07:38:10.695735 28499 net.cpp:165] Memory required for data: 28409984
I0402 07:38:10.695740 28499 layer_factory.hpp:76] Creating layer pool1
I0402 07:38:10.695749 28499 net.cpp:106] Creating Layer pool1
I0402 07:38:10.695755 28499 net.cpp:454] pool1 <- relu1
I0402 07:38:10.695761 28499 net.cpp:411] pool1 -> pool1
I0402 07:38:10.696413 28499 net.cpp:150] Setting up pool1
I0402 07:38:10.696425 28499 net.cpp:157] Top shape: 32 96 15 15 (691200)
I0402 07:38:10.696431 28499 net.cpp:165] Memory required for data: 31174784
I0402 07:38:10.696435 28499 layer_factory.hpp:76] Creating layer conv2
I0402 07:38:10.696449 28499 net.cpp:106] Creating Layer conv2
I0402 07:38:10.696455 28499 net.cpp:454] conv2 <- pool1
I0402 07:38:10.696463 28499 net.cpp:411] conv2 -> conv2
I0402 07:38:10.707869 28499 net.cpp:150] Setting up conv2
I0402 07:38:10.707901 28499 net.cpp:157] Top shape: 32 256 15 15 (1843200)
I0402 07:38:10.707906 28499 net.cpp:165] Memory required for data: 38547584
I0402 07:38:10.707924 28499 layer_factory.hpp:76] Creating layer relu2
I0402 07:38:10.707938 28499 net.cpp:106] Creating Layer relu2
I0402 07:38:10.707944 28499 net.cpp:454] relu2 <- conv2
I0402 07:38:10.707952 28499 net.cpp:411] relu2 -> relu2
I0402 07:38:10.708655 28499 net.cpp:150] Setting up relu2
I0402 07:38:10.708667 28499 net.cpp:157] Top shape: 32 256 15 15 (1843200)
I0402 07:38:10.708673 28499 net.cpp:165] Memory required for data: 45920384
I0402 07:38:10.708678 28499 layer_factory.hpp:76] Creating layer pool2
I0402 07:38:10.708686 28499 net.cpp:106] Creating Layer pool2
I0402 07:38:10.708691 28499 net.cpp:454] pool2 <- relu2
I0402 07:38:10.708698 28499 net.cpp:411] pool2 -> pool2
I0402 07:38:10.709461 28499 net.cpp:150] Setting up pool2
I0402 07:38:10.709473 28499 net.cpp:157] Top shape: 32 256 7 7 (401408)
I0402 07:38:10.709478 28499 net.cpp:165] Memory required for data: 47526016
I0402 07:38:10.709482 28499 layer_factory.hpp:76] Creating layer conv3
I0402 07:38:10.709494 28499 net.cpp:106] Creating Layer conv3
I0402 07:38:10.709501 28499 net.cpp:454] conv3 <- pool2
I0402 07:38:10.709508 28499 net.cpp:411] conv3 -> conv3
I0402 07:38:10.734652 28499 net.cpp:150] Setting up conv3
I0402 07:38:10.734680 28499 net.cpp:157] Top shape: 32 384 7 7 (602112)
I0402 07:38:10.734685 28499 net.cpp:165] Memory required for data: 49934464
I0402 07:38:10.734700 28499 layer_factory.hpp:76] Creating layer relu3
I0402 07:38:10.734714 28499 net.cpp:106] Creating Layer relu3
I0402 07:38:10.734719 28499 net.cpp:454] relu3 <- conv3
I0402 07:38:10.734726 28499 net.cpp:411] relu3 -> relu3
I0402 07:38:10.735437 28499 net.cpp:150] Setting up relu3
I0402 07:38:10.735450 28499 net.cpp:157] Top shape: 32 384 7 7 (602112)
I0402 07:38:10.735455 28499 net.cpp:165] Memory required for data: 52342912
I0402 07:38:10.735458 28499 layer_factory.hpp:76] Creating layer conv4
I0402 07:38:10.735474 28499 net.cpp:106] Creating Layer conv4
I0402 07:38:10.735481 28499 net.cpp:454] conv4 <- relu3
I0402 07:38:10.735491 28499 net.cpp:411] conv4 -> conv4
I0402 07:38:10.756439 28499 net.cpp:150] Setting up conv4
I0402 07:38:10.756465 28499 net.cpp:157] Top shape: 32 384 7 7 (602112)
I0402 07:38:10.756470 28499 net.cpp:165] Memory required for data: 54751360
I0402 07:38:10.756506 28499 layer_factory.hpp:76] Creating layer relu4
I0402 07:38:10.756518 28499 net.cpp:106] Creating Layer relu4
I0402 07:38:10.756525 28499 net.cpp:454] relu4 <- conv4
I0402 07:38:10.756538 28499 net.cpp:411] relu4 -> relu4
I0402 07:38:10.757297 28499 net.cpp:150] Setting up relu4
I0402 07:38:10.757309 28499 net.cpp:157] Top shape: 32 384 7 7 (602112)
I0402 07:38:10.757314 28499 net.cpp:165] Memory required for data: 57159808
I0402 07:38:10.757318 28499 layer_factory.hpp:76] Creating layer conv5
I0402 07:38:10.757333 28499 net.cpp:106] Creating Layer conv5
I0402 07:38:10.757339 28499 net.cpp:454] conv5 <- relu4
I0402 07:38:10.757349 28499 net.cpp:411] conv5 -> conv5
I0402 07:38:10.774760 28499 net.cpp:150] Setting up conv5
I0402 07:38:10.774786 28499 net.cpp:157] Top shape: 32 256 7 7 (401408)
I0402 07:38:10.774791 28499 net.cpp:165] Memory required for data: 58765440
I0402 07:38:10.774807 28499 layer_factory.hpp:76] Creating layer relu5
I0402 07:38:10.774821 28499 net.cpp:106] Creating Layer relu5
I0402 07:38:10.774827 28499 net.cpp:454] relu5 <- conv5
I0402 07:38:10.774834 28499 net.cpp:411] relu5 -> relu5
I0402 07:38:10.775589 28499 net.cpp:150] Setting up relu5
I0402 07:38:10.775602 28499 net.cpp:157] Top shape: 32 256 7 7 (401408)
I0402 07:38:10.775607 28499 net.cpp:165] Memory required for data: 60371072
I0402 07:38:10.775611 28499 layer_factory.hpp:76] Creating layer pool5
I0402 07:38:10.775620 28499 net.cpp:106] Creating Layer pool5
I0402 07:38:10.775625 28499 net.cpp:454] pool5 <- relu5
I0402 07:38:10.775635 28499 net.cpp:411] pool5 -> pool5
I0402 07:38:10.776409 28499 net.cpp:150] Setting up pool5
I0402 07:38:10.776422 28499 net.cpp:157] Top shape: 32 256 3 3 (73728)
I0402 07:38:10.776427 28499 net.cpp:165] Memory required for data: 60665984
I0402 07:38:10.776430 28499 layer_factory.hpp:76] Creating layer fc6
I0402 07:38:10.776449 28499 net.cpp:106] Creating Layer fc6
I0402 07:38:10.776456 28499 net.cpp:454] fc6 <- pool5
I0402 07:38:10.776464 28499 net.cpp:411] fc6 -> fc6
I0402 07:38:10.897855 28499 net.cpp:150] Setting up fc6
I0402 07:38:10.897882 28499 net.cpp:157] Top shape: 32 2048 (65536)
I0402 07:38:10.897887 28499 net.cpp:165] Memory required for data: 60928128
I0402 07:38:10.897899 28499 layer_factory.hpp:76] Creating layer relu6
I0402 07:38:10.897912 28499 net.cpp:106] Creating Layer relu6
I0402 07:38:10.897917 28499 net.cpp:454] relu6 <- fc6
I0402 07:38:10.897924 28499 net.cpp:411] relu6 -> relu6
I0402 07:38:10.898769 28499 net.cpp:150] Setting up relu6
I0402 07:38:10.898783 28499 net.cpp:157] Top shape: 32 2048 (65536)
I0402 07:38:10.898788 28499 net.cpp:165] Memory required for data: 61190272
I0402 07:38:10.898792 28499 layer_factory.hpp:76] Creating layer drop6
I0402 07:38:10.898802 28499 net.cpp:106] Creating Layer drop6
I0402 07:38:10.898808 28499 net.cpp:454] drop6 <- relu6
I0402 07:38:10.898816 28499 net.cpp:411] drop6 -> drop6
I0402 07:38:10.898875 28499 net.cpp:150] Setting up drop6
I0402 07:38:10.898885 28499 net.cpp:157] Top shape: 32 2048 (65536)
I0402 07:38:10.898890 28499 net.cpp:165] Memory required for data: 61452416
I0402 07:38:10.898895 28499 layer_factory.hpp:76] Creating layer fc7
I0402 07:38:10.898906 28499 net.cpp:106] Creating Layer fc7
I0402 07:38:10.898911 28499 net.cpp:454] fc7 <- drop6
I0402 07:38:10.898921 28499 net.cpp:411] fc7 -> fc7
I0402 07:38:11.006685 28499 net.cpp:150] Setting up fc7
I0402 07:38:11.006713 28499 net.cpp:157] Top shape: 32 2048 (65536)
I0402 07:38:11.006717 28499 net.cpp:165] Memory required for data: 61714560
I0402 07:38:11.006728 28499 layer_factory.hpp:76] Creating layer relu7
I0402 07:38:11.006739 28499 net.cpp:106] Creating Layer relu7
I0402 07:38:11.006764 28499 net.cpp:454] relu7 <- fc7
I0402 07:38:11.006788 28499 net.cpp:411] relu7 -> relu7
I0402 07:38:11.007534 28499 net.cpp:150] Setting up relu7
I0402 07:38:11.007545 28499 net.cpp:157] Top shape: 32 2048 (65536)
I0402 07:38:11.007550 28499 net.cpp:165] Memory required for data: 61976704
I0402 07:38:11.007555 28499 layer_factory.hpp:76] Creating layer drop7
I0402 07:38:11.007596 28499 net.cpp:106] Creating Layer drop7
I0402 07:38:11.007618 28499 net.cpp:454] drop7 <- relu7
I0402 07:38:11.007639 28499 net.cpp:411] drop7 -> drop7
I0402 07:38:11.007709 28499 net.cpp:150] Setting up drop7
I0402 07:38:11.007719 28499 net.cpp:157] Top shape: 32 2048 (65536)
I0402 07:38:11.007724 28499 net.cpp:165] Memory required for data: 62238848
I0402 07:38:11.007728 28499 layer_factory.hpp:76] Creating layer fc8
I0402 07:38:11.007753 28499 net.cpp:106] Creating Layer fc8
I0402 07:38:11.007774 28499 net.cpp:454] fc8 <- drop7
I0402 07:38:11.007797 28499 net.cpp:411] fc8 -> fc8
I0402 07:38:11.060577 28499 net.cpp:150] Setting up fc8
I0402 07:38:11.060606 28499 net.cpp:157] Top shape: 32 1000 (32000)
I0402 07:38:11.060611 28499 net.cpp:165] Memory required for data: 62366848
I0402 07:38:11.060621 28499 layer_factory.hpp:76] Creating layer loss
I0402 07:38:11.060633 28499 net.cpp:106] Creating Layer loss
I0402 07:38:11.060660 28499 net.cpp:454] loss <- fc8
I0402 07:38:11.060679 28499 net.cpp:454] loss <- label
I0402 07:38:11.060699 28499 net.cpp:411] loss -> loss
I0402 07:38:11.060736 28499 layer_factory.hpp:76] Creating layer loss
I0402 07:38:11.061671 28499 net.cpp:150] Setting up loss
I0402 07:38:11.061683 28499 net.cpp:157] Top shape: (1)
I0402 07:38:11.061688 28499 net.cpp:160]     with loss weight 1
I0402 07:38:11.061724 28499 net.cpp:165] Memory required for data: 62366852
I0402 07:38:11.061750 28499 net.cpp:226] loss needs backward computation.
I0402 07:38:11.061770 28499 net.cpp:226] fc8 needs backward computation.
I0402 07:38:11.061789 28499 net.cpp:226] drop7 needs backward computation.
I0402 07:38:11.061806 28499 net.cpp:226] relu7 needs backward computation.
I0402 07:38:11.061825 28499 net.cpp:226] fc7 needs backward computation.
I0402 07:38:11.061842 28499 net.cpp:226] drop6 needs backward computation.
I0402 07:38:11.061859 28499 net.cpp:226] relu6 needs backward computation.
I0402 07:38:11.061878 28499 net.cpp:226] fc6 needs backward computation.
I0402 07:38:11.061895 28499 net.cpp:226] pool5 needs backward computation.
I0402 07:38:11.061913 28499 net.cpp:226] relu5 needs backward computation.
I0402 07:38:11.061931 28499 net.cpp:226] conv5 needs backward computation.
I0402 07:38:11.061949 28499 net.cpp:226] relu4 needs backward computation.
I0402 07:38:11.061967 28499 net.cpp:226] conv4 needs backward computation.
I0402 07:38:11.061985 28499 net.cpp:226] relu3 needs backward computation.
I0402 07:38:11.062001 28499 net.cpp:226] conv3 needs backward computation.
I0402 07:38:11.062019 28499 net.cpp:226] pool2 needs backward computation.
I0402 07:38:11.062037 28499 net.cpp:226] relu2 needs backward computation.
I0402 07:38:11.062054 28499 net.cpp:226] conv2 needs backward computation.
I0402 07:38:11.062072 28499 net.cpp:226] pool1 needs backward computation.
I0402 07:38:11.062090 28499 net.cpp:226] relu1 needs backward computation.
I0402 07:38:11.062108 28499 net.cpp:226] conv1 needs backward computation.
I0402 07:38:11.062126 28499 net.cpp:228] data does not need backward computation.
I0402 07:38:11.062144 28499 net.cpp:270] This network produces output loss
I0402 07:38:11.062180 28499 net.cpp:283] Network initialization done.
I0402 07:38:11.062317 28499 solver.cpp:181] Creating test net (#0) specified by net_param
I0402 07:38:11.062372 28499 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0402 07:38:11.062541 28499 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.04
    mirror: false
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 123
    force_color: true
  }
  data_param {
    source: "/home/old-ufo/storage/datasets/imagenet/imgs128/lmdb/ilsvrc12_128_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "TanH"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "TanH"
  bottom: "conv2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "TanH"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "TanH"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "TanH"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "TanH"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "TanH"
  bottom: "fc7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0402 07:38:11.062702 28499 layer_factory.hpp:76] Creating layer data
I0402 07:38:11.062826 28499 net.cpp:106] Creating Layer data
I0402 07:38:11.062855 28499 net.cpp:411] data -> data
I0402 07:38:11.062882 28499 net.cpp:411] data -> label
I0402 07:38:11.063524 28506 db_lmdb.cpp:38] Opened lmdb /home/old-ufo/storage/datasets/imagenet/imgs128/lmdb/ilsvrc12_128_val_lmdb
I0402 07:38:11.064347 28499 data_layer.cpp:41] output data size: 50,3,128,128
I0402 07:38:11.079030 28499 net.cpp:150] Setting up data
I0402 07:38:11.079103 28499 net.cpp:157] Top shape: 50 3 128 128 (2457600)
I0402 07:38:11.079123 28499 net.cpp:157] Top shape: 50 (50)
I0402 07:38:11.079140 28499 net.cpp:165] Memory required for data: 9830600
I0402 07:38:11.079161 28499 layer_factory.hpp:76] Creating layer label_data_1_split
I0402 07:38:11.079185 28499 net.cpp:106] Creating Layer label_data_1_split
I0402 07:38:11.079201 28499 net.cpp:454] label_data_1_split <- label
I0402 07:38:11.079221 28499 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0402 07:38:11.079248 28499 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0402 07:38:11.079401 28499 net.cpp:150] Setting up label_data_1_split
I0402 07:38:11.079429 28499 net.cpp:157] Top shape: 50 (50)
I0402 07:38:11.079475 28499 net.cpp:157] Top shape: 50 (50)
I0402 07:38:11.079493 28499 net.cpp:165] Memory required for data: 9831000
I0402 07:38:11.079545 28499 layer_factory.hpp:76] Creating layer conv1
I0402 07:38:11.079602 28499 net.cpp:106] Creating Layer conv1
I0402 07:38:11.079622 28499 net.cpp:454] conv1 <- data
I0402 07:38:11.079668 28499 net.cpp:411] conv1 -> conv1
I0402 07:38:11.084496 28499 net.cpp:150] Setting up conv1
I0402 07:38:11.084553 28499 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I0402 07:38:11.084631 28499 net.cpp:165] Memory required for data: 27111000
I0402 07:38:11.084691 28499 layer_factory.hpp:76] Creating layer relu1
I0402 07:38:11.084743 28499 net.cpp:106] Creating Layer relu1
I0402 07:38:11.084796 28499 net.cpp:454] relu1 <- conv1
I0402 07:38:11.084867 28499 net.cpp:411] relu1 -> relu1
I0402 07:38:11.085727 28499 net.cpp:150] Setting up relu1
I0402 07:38:11.085768 28499 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I0402 07:38:11.085816 28499 net.cpp:165] Memory required for data: 44391000
I0402 07:38:11.085858 28499 layer_factory.hpp:76] Creating layer pool1
I0402 07:38:11.085927 28499 net.cpp:106] Creating Layer pool1
I0402 07:38:11.085949 28499 net.cpp:454] pool1 <- relu1
I0402 07:38:11.086011 28499 net.cpp:411] pool1 -> pool1
I0402 07:38:11.086887 28499 net.cpp:150] Setting up pool1
I0402 07:38:11.086917 28499 net.cpp:157] Top shape: 50 96 15 15 (1080000)
I0402 07:38:11.086964 28499 net.cpp:165] Memory required for data: 48711000
I0402 07:38:11.087007 28499 layer_factory.hpp:76] Creating layer conv2
I0402 07:38:11.087079 28499 net.cpp:106] Creating Layer conv2
I0402 07:38:11.087100 28499 net.cpp:454] conv2 <- pool1
I0402 07:38:11.087164 28499 net.cpp:411] conv2 -> conv2
I0402 07:38:11.103824 28499 net.cpp:150] Setting up conv2
I0402 07:38:11.103890 28499 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I0402 07:38:11.103907 28499 net.cpp:165] Memory required for data: 60231000
I0402 07:38:11.104034 28499 layer_factory.hpp:76] Creating layer relu2
I0402 07:38:11.104086 28499 net.cpp:106] Creating Layer relu2
I0402 07:38:11.104110 28499 net.cpp:454] relu2 <- conv2
I0402 07:38:11.104174 28499 net.cpp:411] relu2 -> relu2
I0402 07:38:11.105046 28499 net.cpp:150] Setting up relu2
I0402 07:38:11.105078 28499 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I0402 07:38:11.105126 28499 net.cpp:165] Memory required for data: 71751000
I0402 07:38:11.105167 28499 layer_factory.hpp:76] Creating layer pool2
I0402 07:38:11.105231 28499 net.cpp:106] Creating Layer pool2
I0402 07:38:11.105252 28499 net.cpp:454] pool2 <- relu2
I0402 07:38:11.105315 28499 net.cpp:411] pool2 -> pool2
I0402 07:38:11.106215 28499 net.cpp:150] Setting up pool2
I0402 07:38:11.106248 28499 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0402 07:38:11.106297 28499 net.cpp:165] Memory required for data: 74259800
I0402 07:38:11.106348 28499 layer_factory.hpp:76] Creating layer conv3
I0402 07:38:11.106421 28499 net.cpp:106] Creating Layer conv3
I0402 07:38:11.106443 28499 net.cpp:454] conv3 <- pool2
I0402 07:38:11.106508 28499 net.cpp:411] conv3 -> conv3
I0402 07:38:11.141679 28499 net.cpp:150] Setting up conv3
I0402 07:38:11.141752 28499 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0402 07:38:11.141770 28499 net.cpp:165] Memory required for data: 78023000
I0402 07:38:11.141898 28499 layer_factory.hpp:76] Creating layer relu3
I0402 07:38:11.141947 28499 net.cpp:106] Creating Layer relu3
I0402 07:38:11.141970 28499 net.cpp:454] relu3 <- conv3
I0402 07:38:11.142036 28499 net.cpp:411] relu3 -> relu3
I0402 07:38:11.142917 28499 net.cpp:150] Setting up relu3
I0402 07:38:11.142951 28499 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0402 07:38:11.142999 28499 net.cpp:165] Memory required for data: 81786200
I0402 07:38:11.143043 28499 layer_factory.hpp:76] Creating layer conv4
I0402 07:38:11.143110 28499 net.cpp:106] Creating Layer conv4
I0402 07:38:11.143132 28499 net.cpp:454] conv4 <- relu3
I0402 07:38:11.143196 28499 net.cpp:411] conv4 -> conv4
I0402 07:38:11.172752 28499 net.cpp:150] Setting up conv4
I0402 07:38:11.172824 28499 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0402 07:38:11.172842 28499 net.cpp:165] Memory required for data: 85549400
I0402 07:38:11.172965 28499 layer_factory.hpp:76] Creating layer relu4
I0402 07:38:11.173012 28499 net.cpp:106] Creating Layer relu4
I0402 07:38:11.173035 28499 net.cpp:454] relu4 <- conv4
I0402 07:38:11.173099 28499 net.cpp:411] relu4 -> relu4
I0402 07:38:11.173998 28499 net.cpp:150] Setting up relu4
I0402 07:38:11.174031 28499 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0402 07:38:11.174079 28499 net.cpp:165] Memory required for data: 89312600
I0402 07:38:11.174121 28499 layer_factory.hpp:76] Creating layer conv5
I0402 07:38:11.174190 28499 net.cpp:106] Creating Layer conv5
I0402 07:38:11.174214 28499 net.cpp:454] conv5 <- relu4
I0402 07:38:11.174286 28499 net.cpp:411] conv5 -> conv5
I0402 07:38:11.195878 28499 net.cpp:150] Setting up conv5
I0402 07:38:11.195945 28499 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0402 07:38:11.195961 28499 net.cpp:165] Memory required for data: 91821400
I0402 07:38:11.196090 28499 layer_factory.hpp:76] Creating layer relu5
I0402 07:38:11.196142 28499 net.cpp:106] Creating Layer relu5
I0402 07:38:11.196166 28499 net.cpp:454] relu5 <- conv5
I0402 07:38:11.196230 28499 net.cpp:411] relu5 -> relu5
I0402 07:38:11.197119 28499 net.cpp:150] Setting up relu5
I0402 07:38:11.197152 28499 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0402 07:38:11.197201 28499 net.cpp:165] Memory required for data: 94330200
I0402 07:38:11.197242 28499 layer_factory.hpp:76] Creating layer pool5
I0402 07:38:11.197309 28499 net.cpp:106] Creating Layer pool5
I0402 07:38:11.197330 28499 net.cpp:454] pool5 <- relu5
I0402 07:38:11.197394 28499 net.cpp:411] pool5 -> pool5
I0402 07:38:11.198305 28499 net.cpp:150] Setting up pool5
I0402 07:38:11.198339 28499 net.cpp:157] Top shape: 50 256 3 3 (115200)
I0402 07:38:11.198388 28499 net.cpp:165] Memory required for data: 94791000
I0402 07:38:11.198431 28499 layer_factory.hpp:76] Creating layer fc6
I0402 07:38:11.198508 28499 net.cpp:106] Creating Layer fc6
I0402 07:38:11.198529 28499 net.cpp:454] fc6 <- pool5
I0402 07:38:11.198592 28499 net.cpp:411] fc6 -> fc6
I0402 07:38:11.319160 28499 net.cpp:150] Setting up fc6
I0402 07:38:11.319190 28499 net.cpp:157] Top shape: 50 2048 (102400)
I0402 07:38:11.319195 28499 net.cpp:165] Memory required for data: 95200600
I0402 07:38:11.319206 28499 layer_factory.hpp:76] Creating layer relu6
I0402 07:38:11.319219 28499 net.cpp:106] Creating Layer relu6
I0402 07:38:11.319224 28499 net.cpp:454] relu6 <- fc6
I0402 07:38:11.319236 28499 net.cpp:411] relu6 -> relu6
I0402 07:38:11.320245 28499 net.cpp:150] Setting up relu6
I0402 07:38:11.320258 28499 net.cpp:157] Top shape: 50 2048 (102400)
I0402 07:38:11.320262 28499 net.cpp:165] Memory required for data: 95610200
I0402 07:38:11.320287 28499 layer_factory.hpp:76] Creating layer drop6
I0402 07:38:11.320298 28499 net.cpp:106] Creating Layer drop6
I0402 07:38:11.320303 28499 net.cpp:454] drop6 <- relu6
I0402 07:38:11.320312 28499 net.cpp:411] drop6 -> drop6
I0402 07:38:11.320380 28499 net.cpp:150] Setting up drop6
I0402 07:38:11.320391 28499 net.cpp:157] Top shape: 50 2048 (102400)
I0402 07:38:11.320396 28499 net.cpp:165] Memory required for data: 96019800
I0402 07:38:11.320400 28499 layer_factory.hpp:76] Creating layer fc7
I0402 07:38:11.320410 28499 net.cpp:106] Creating Layer fc7
I0402 07:38:11.320416 28499 net.cpp:454] fc7 <- drop6
I0402 07:38:11.320426 28499 net.cpp:411] fc7 -> fc7
I0402 07:38:11.461004 28499 net.cpp:150] Setting up fc7
I0402 07:38:11.461036 28499 net.cpp:157] Top shape: 50 2048 (102400)
I0402 07:38:11.461043 28499 net.cpp:165] Memory required for data: 96429400
I0402 07:38:11.461055 28499 layer_factory.hpp:76] Creating layer relu7
I0402 07:38:11.461071 28499 net.cpp:106] Creating Layer relu7
I0402 07:38:11.461081 28499 net.cpp:454] relu7 <- fc7
I0402 07:38:11.461112 28499 net.cpp:411] relu7 -> relu7
I0402 07:38:11.461987 28499 net.cpp:150] Setting up relu7
I0402 07:38:11.462002 28499 net.cpp:157] Top shape: 50 2048 (102400)
I0402 07:38:11.462007 28499 net.cpp:165] Memory required for data: 96839000
I0402 07:38:11.462012 28499 layer_factory.hpp:76] Creating layer drop7
I0402 07:38:11.462023 28499 net.cpp:106] Creating Layer drop7
I0402 07:38:11.462028 28499 net.cpp:454] drop7 <- relu7
I0402 07:38:11.462039 28499 net.cpp:411] drop7 -> drop7
I0402 07:38:11.462091 28499 net.cpp:150] Setting up drop7
I0402 07:38:11.462098 28499 net.cpp:157] Top shape: 50 2048 (102400)
I0402 07:38:11.462100 28499 net.cpp:165] Memory required for data: 97248600
I0402 07:38:11.462103 28499 layer_factory.hpp:76] Creating layer fc8
I0402 07:38:11.462111 28499 net.cpp:106] Creating Layer fc8
I0402 07:38:11.462116 28499 net.cpp:454] fc8 <- drop7
I0402 07:38:11.462126 28499 net.cpp:411] fc8 -> fc8
I0402 07:38:11.514233 28499 net.cpp:150] Setting up fc8
I0402 07:38:11.514266 28499 net.cpp:157] Top shape: 50 1000 (50000)
I0402 07:38:11.514271 28499 net.cpp:165] Memory required for data: 97448600
I0402 07:38:11.514286 28499 layer_factory.hpp:76] Creating layer fc8_fc8_0_split
I0402 07:38:11.514297 28499 net.cpp:106] Creating Layer fc8_fc8_0_split
I0402 07:38:11.514324 28499 net.cpp:454] fc8_fc8_0_split <- fc8
I0402 07:38:11.514338 28499 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0402 07:38:11.514364 28499 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0402 07:38:11.514459 28499 net.cpp:150] Setting up fc8_fc8_0_split
I0402 07:38:11.514468 28499 net.cpp:157] Top shape: 50 1000 (50000)
I0402 07:38:11.514474 28499 net.cpp:157] Top shape: 50 1000 (50000)
I0402 07:38:11.514490 28499 net.cpp:165] Memory required for data: 97848600
I0402 07:38:11.514497 28499 layer_factory.hpp:76] Creating layer accuracy
I0402 07:38:11.514508 28499 net.cpp:106] Creating Layer accuracy
I0402 07:38:11.514514 28499 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0402 07:38:11.514520 28499 net.cpp:454] accuracy <- label_data_1_split_0
I0402 07:38:11.514536 28499 net.cpp:411] accuracy -> accuracy
I0402 07:38:11.514555 28499 net.cpp:150] Setting up accuracy
I0402 07:38:11.514562 28499 net.cpp:157] Top shape: (1)
I0402 07:38:11.514566 28499 net.cpp:165] Memory required for data: 97848604
I0402 07:38:11.514582 28499 layer_factory.hpp:76] Creating layer loss
I0402 07:38:11.514590 28499 net.cpp:106] Creating Layer loss
I0402 07:38:11.514598 28499 net.cpp:454] loss <- fc8_fc8_0_split_1
I0402 07:38:11.514603 28499 net.cpp:454] loss <- label_data_1_split_1
I0402 07:38:11.514607 28499 net.cpp:411] loss -> loss
I0402 07:38:11.514614 28499 layer_factory.hpp:76] Creating layer loss
I0402 07:38:11.515594 28499 net.cpp:150] Setting up loss
I0402 07:38:11.515605 28499 net.cpp:157] Top shape: (1)
I0402 07:38:11.515610 28499 net.cpp:160]     with loss weight 1
I0402 07:38:11.515621 28499 net.cpp:165] Memory required for data: 97848608
I0402 07:38:11.515626 28499 net.cpp:226] loss needs backward computation.
I0402 07:38:11.515643 28499 net.cpp:228] accuracy does not need backward computation.
I0402 07:38:11.515662 28499 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0402 07:38:11.515667 28499 net.cpp:226] fc8 needs backward computation.
I0402 07:38:11.515672 28499 net.cpp:226] drop7 needs backward computation.
I0402 07:38:11.515676 28499 net.cpp:226] relu7 needs backward computation.
I0402 07:38:11.515691 28499 net.cpp:226] fc7 needs backward computation.
I0402 07:38:11.515696 28499 net.cpp:226] drop6 needs backward computation.
I0402 07:38:11.515700 28499 net.cpp:226] relu6 needs backward computation.
I0402 07:38:11.515714 28499 net.cpp:226] fc6 needs backward computation.
I0402 07:38:11.515718 28499 net.cpp:226] pool5 needs backward computation.
I0402 07:38:11.515723 28499 net.cpp:226] relu5 needs backward computation.
I0402 07:38:11.515738 28499 net.cpp:226] conv5 needs backward computation.
I0402 07:38:11.515743 28499 net.cpp:226] relu4 needs backward computation.
I0402 07:38:11.515746 28499 net.cpp:226] conv4 needs backward computation.
I0402 07:38:11.515760 28499 net.cpp:226] relu3 needs backward computation.
I0402 07:38:11.515765 28499 net.cpp:226] conv3 needs backward computation.
I0402 07:38:11.515769 28499 net.cpp:226] pool2 needs backward computation.
I0402 07:38:11.515784 28499 net.cpp:226] relu2 needs backward computation.
I0402 07:38:11.515789 28499 net.cpp:226] conv2 needs backward computation.
I0402 07:38:11.515792 28499 net.cpp:226] pool1 needs backward computation.
I0402 07:38:11.515806 28499 net.cpp:226] relu1 needs backward computation.
I0402 07:38:11.515811 28499 net.cpp:226] conv1 needs backward computation.
I0402 07:38:11.515816 28499 net.cpp:228] label_data_1_split does not need backward computation.
I0402 07:38:11.515830 28499 net.cpp:228] data does not need backward computation.
I0402 07:38:11.515835 28499 net.cpp:270] This network produces output accuracy
I0402 07:38:11.515839 28499 net.cpp:270] This network produces output loss
I0402 07:38:11.515861 28499 net.cpp:283] Network initialization done.
I0402 07:38:11.515964 28499 solver.cpp:60] Solver scaffolding done.
I0402 07:38:11.516551 28499 caffe.cpp:128] Finetuning from ./caffenet128_lsuv_bs32_lr008.prototxt.caffemodel
I0402 07:38:11.793753 28499 caffe.cpp:212] Starting Optimization
I0402 07:38:11.793779 28499 solver.cpp:288] Solving CaffeNet
I0402 07:38:11.793786 28499 solver.cpp:289] Learning Rate Policy: step
I0402 07:38:11.795140 28499 solver.cpp:341] Iteration 0, Testing net (#0)
I0402 07:38:11.967068 28499 blocking_queue.cpp:50] Data layer prefetch queue empty
I0402 07:38:46.220540 28499 solver.cpp:409]     Test net output #0: accuracy = 0.00092
I0402 07:38:46.220654 28499 solver.cpp:409]     Test net output #1: loss = 7.08238 (* 1 = 7.08238 loss)
I0402 07:38:46.244179 28499 solver.cpp:237] Iteration 0, loss = 7.90261
I0402 07:38:46.244271 28499 solver.cpp:253]     Train net output #0: loss = 7.90261 (* 1 = 7.90261 loss)
I0402 07:38:46.244380 28499 sgd_solver.cpp:106] Iteration 0, lr = 0.08
I0402 07:38:46.818570 28499 solver.cpp:237] Iteration 20, loss = 53.2593
I0402 07:38:46.818611 28499 solver.cpp:253]     Train net output #0: loss = 53.2593 (* 1 = 53.2593 loss)
I0402 07:38:46.818621 28499 sgd_solver.cpp:106] Iteration 20, lr = 0.08
I0402 07:38:47.401953 28499 solver.cpp:237] Iteration 40, loss = 34.9588
I0402 07:38:47.401996 28499 solver.cpp:253]     Train net output #0: loss = 34.9588 (* 1 = 34.9588 loss)
I0402 07:38:47.402009 28499 sgd_solver.cpp:106] Iteration 40, lr = 0.08
I0402 07:38:47.978691 28499 solver.cpp:237] Iteration 60, loss = 34.7318
I0402 07:38:47.978785 28499 solver.cpp:253]     Train net output #0: loss = 34.7318 (* 1 = 34.7318 loss)
I0402 07:38:47.978816 28499 sgd_solver.cpp:106] Iteration 60, lr = 0.08
I0402 07:38:48.551754 28499 solver.cpp:237] Iteration 80, loss = 53.6387
I0402 07:38:48.551853 28499 solver.cpp:253]     Train net output #0: loss = 53.6386 (* 1 = 53.6386 loss)
I0402 07:38:48.551897 28499 sgd_solver.cpp:106] Iteration 80, lr = 0.08
I0402 07:38:49.126199 28499 solver.cpp:237] Iteration 100, loss = 37.6121
I0402 07:38:49.126341 28499 solver.cpp:253]     Train net output #0: loss = 37.6121 (* 1 = 37.6121 loss)
I0402 07:38:49.126356 28499 sgd_solver.cpp:106] Iteration 100, lr = 0.08
I0402 07:38:49.700985 28499 solver.cpp:237] Iteration 120, loss = 42.2226
I0402 07:38:49.701076 28499 solver.cpp:253]     Train net output #0: loss = 42.2226 (* 1 = 42.2226 loss)
I0402 07:38:49.701098 28499 sgd_solver.cpp:106] Iteration 120, lr = 0.08
I0402 07:38:50.272013 28499 solver.cpp:237] Iteration 140, loss = 45.0714
I0402 07:38:50.272104 28499 solver.cpp:253]     Train net output #0: loss = 45.0714 (* 1 = 45.0714 loss)
I0402 07:38:50.272133 28499 sgd_solver.cpp:106] Iteration 140, lr = 0.08
I0402 07:38:50.846688 28499 solver.cpp:237] Iteration 160, loss = 44.9869
I0402 07:38:50.846771 28499 solver.cpp:253]     Train net output #0: loss = 44.9869 (* 1 = 44.9869 loss)
I0402 07:38:50.846796 28499 sgd_solver.cpp:106] Iteration 160, lr = 0.08
I0402 07:38:51.428318 28499 solver.cpp:237] Iteration 180, loss = 43.3081
I0402 07:38:51.428401 28499 solver.cpp:253]     Train net output #0: loss = 43.3081 (* 1 = 43.3081 loss)
I0402 07:38:51.428423 28499 sgd_solver.cpp:106] Iteration 180, lr = 0.08
I0402 07:38:52.004370 28499 solver.cpp:237] Iteration 200, loss = 42.3091
I0402 07:38:52.004458 28499 solver.cpp:253]     Train net output #0: loss = 42.3091 (* 1 = 42.3091 loss)
I0402 07:38:52.004479 28499 sgd_solver.cpp:106] Iteration 200, lr = 0.08
I0402 07:38:52.583382 28499 solver.cpp:237] Iteration 220, loss = 46.576
I0402 07:38:52.583482 28499 solver.cpp:253]     Train net output #0: loss = 46.576 (* 1 = 46.576 loss)
I0402 07:38:52.583516 28499 sgd_solver.cpp:106] Iteration 220, lr = 0.08
I0402 07:38:53.157829 28499 solver.cpp:237] Iteration 240, loss = 53.8893
I0402 07:38:53.157917 28499 solver.cpp:253]     Train net output #0: loss = 53.8893 (* 1 = 53.8893 loss)
I0402 07:38:53.157940 28499 sgd_solver.cpp:106] Iteration 240, lr = 0.08
I0402 07:38:53.727260 28499 solver.cpp:237] Iteration 260, loss = 49.8512
I0402 07:38:53.727349 28499 solver.cpp:253]     Train net output #0: loss = 49.8512 (* 1 = 49.8512 loss)
I0402 07:38:53.727380 28499 sgd_solver.cpp:106] Iteration 260, lr = 0.08
I0402 07:38:54.312141 28499 solver.cpp:237] Iteration 280, loss = 33.2421
I0402 07:38:54.312278 28499 solver.cpp:253]     Train net output #0: loss = 33.2421 (* 1 = 33.2421 loss)
I0402 07:38:54.312293 28499 sgd_solver.cpp:106] Iteration 280, lr = 0.08
I0402 07:38:54.888305 28499 solver.cpp:237] Iteration 300, loss = 48.4238
I0402 07:38:54.888393 28499 solver.cpp:253]     Train net output #0: loss = 48.4238 (* 1 = 48.4238 loss)
I0402 07:38:54.888416 28499 sgd_solver.cpp:106] Iteration 300, lr = 0.08
I0402 07:38:55.459461 28499 solver.cpp:237] Iteration 320, loss = 50.448
I0402 07:38:55.459550 28499 solver.cpp:253]     Train net output #0: loss = 50.448 (* 1 = 50.448 loss)
I0402 07:38:55.459573 28499 sgd_solver.cpp:106] Iteration 320, lr = 0.08
I0402 07:38:56.039660 28499 solver.cpp:237] Iteration 340, loss = 47.3485
I0402 07:38:56.039753 28499 solver.cpp:253]     Train net output #0: loss = 47.3484 (* 1 = 47.3484 loss)
I0402 07:38:56.039777 28499 sgd_solver.cpp:106] Iteration 340, lr = 0.08
I0402 07:38:56.612534 28499 solver.cpp:237] Iteration 360, loss = 42.8388
I0402 07:38:56.612623 28499 solver.cpp:253]     Train net output #0: loss = 42.8388 (* 1 = 42.8388 loss)
I0402 07:38:56.612645 28499 sgd_solver.cpp:106] Iteration 360, lr = 0.08
I0402 07:38:57.193244 28499 solver.cpp:237] Iteration 380, loss = 56.6022
I0402 07:38:57.193342 28499 solver.cpp:253]     Train net output #0: loss = 56.6022 (* 1 = 56.6022 loss)
I0402 07:38:57.193366 28499 sgd_solver.cpp:106] Iteration 380, lr = 0.08
I0402 07:38:57.766855 28499 solver.cpp:237] Iteration 400, loss = 53.408
I0402 07:38:57.766940 28499 solver.cpp:253]     Train net output #0: loss = 53.408 (* 1 = 53.408 loss)
I0402 07:38:57.766974 28499 sgd_solver.cpp:106] Iteration 400, lr = 0.08
I0402 07:38:58.341624 28499 solver.cpp:237] Iteration 420, loss = 59.6468
I0402 07:38:58.341712 28499 solver.cpp:253]     Train net output #0: loss = 59.6468 (* 1 = 59.6468 loss)
I0402 07:38:58.341735 28499 sgd_solver.cpp:106] Iteration 420, lr = 0.08
I0402 07:38:58.921799 28499 solver.cpp:237] Iteration 440, loss = 52.3977
I0402 07:38:58.921886 28499 solver.cpp:253]     Train net output #0: loss = 52.3977 (* 1 = 52.3977 loss)
I0402 07:38:58.921908 28499 sgd_solver.cpp:106] Iteration 440, lr = 0.08
I0402 07:38:59.494666 28499 solver.cpp:237] Iteration 460, loss = 55.3059
I0402 07:38:59.494760 28499 solver.cpp:253]     Train net output #0: loss = 55.3059 (* 1 = 55.3059 loss)
I0402 07:38:59.494782 28499 sgd_solver.cpp:106] Iteration 460, lr = 0.08
I0402 07:39:00.066581 28499 solver.cpp:237] Iteration 480, loss = 47.9981
I0402 07:39:00.066675 28499 solver.cpp:253]     Train net output #0: loss = 47.9981 (* 1 = 47.9981 loss)
I0402 07:39:00.066699 28499 sgd_solver.cpp:106] Iteration 480, lr = 0.08
I0402 07:39:00.640360 28499 solver.cpp:237] Iteration 500, loss = 51.8936
I0402 07:39:00.640450 28499 solver.cpp:253]     Train net output #0: loss = 51.8936 (* 1 = 51.8936 loss)
I0402 07:39:00.640480 28499 sgd_solver.cpp:106] Iteration 500, lr = 0.08
I0402 07:39:01.224361 28499 solver.cpp:237] Iteration 520, loss = 48.5993
I0402 07:39:01.224452 28499 solver.cpp:253]     Train net output #0: loss = 48.5993 (* 1 = 48.5993 loss)
I0402 07:39:01.224475 28499 sgd_solver.cpp:106] Iteration 520, lr = 0.08
I0402 07:39:01.801111 28499 solver.cpp:237] Iteration 540, loss = 55.2013
I0402 07:39:01.801203 28499 solver.cpp:253]     Train net output #0: loss = 55.2013 (* 1 = 55.2013 loss)
I0402 07:39:01.801223 28499 sgd_solver.cpp:106] Iteration 540, lr = 0.08
I0402 07:39:02.378973 28499 solver.cpp:237] Iteration 560, loss = 53.8841
I0402 07:39:02.379066 28499 solver.cpp:253]     Train net output #0: loss = 53.8841 (* 1 = 53.8841 loss)
I0402 07:39:02.379096 28499 sgd_solver.cpp:106] Iteration 560, lr = 0.08
I0402 07:39:02.953490 28499 solver.cpp:237] Iteration 580, loss = 48.8101
I0402 07:39:02.953583 28499 solver.cpp:253]     Train net output #0: loss = 48.8101 (* 1 = 48.8101 loss)
I0402 07:39:02.953604 28499 sgd_solver.cpp:106] Iteration 580, lr = 0.08
I0402 07:39:03.528851 28499 solver.cpp:237] Iteration 600, loss = 50.5517
I0402 07:39:03.528931 28499 solver.cpp:253]     Train net output #0: loss = 50.5517 (* 1 = 50.5517 loss)
I0402 07:39:03.528954 28499 sgd_solver.cpp:106] Iteration 600, lr = 0.08
I0402 07:39:04.103461 28499 solver.cpp:237] Iteration 620, loss = 53.7496
I0402 07:39:04.103551 28499 solver.cpp:253]     Train net output #0: loss = 53.7496 (* 1 = 53.7496 loss)
I0402 07:39:04.103581 28499 sgd_solver.cpp:106] Iteration 620, lr = 0.08
I0402 07:39:04.679566 28499 solver.cpp:237] Iteration 640, loss = 45.9056
I0402 07:39:04.679671 28499 solver.cpp:253]     Train net output #0: loss = 45.9056 (* 1 = 45.9056 loss)
I0402 07:39:04.679682 28499 sgd_solver.cpp:106] Iteration 640, lr = 0.08
I0402 07:39:05.253592 28499 solver.cpp:237] Iteration 660, loss = 56.2474
I0402 07:39:05.253679 28499 solver.cpp:253]     Train net output #0: loss = 56.2474 (* 1 = 56.2474 loss)
I0402 07:39:05.253701 28499 sgd_solver.cpp:106] Iteration 660, lr = 0.08
I0402 07:39:05.829704 28499 solver.cpp:237] Iteration 680, loss = 53.6729
I0402 07:39:05.829802 28499 solver.cpp:253]     Train net output #0: loss = 53.6729 (* 1 = 53.6729 loss)
I0402 07:39:05.829828 28499 sgd_solver.cpp:106] Iteration 680, lr = 0.08
I0402 07:39:06.407187 28499 solver.cpp:237] Iteration 700, loss = 55.7914
I0402 07:39:06.407275 28499 solver.cpp:253]     Train net output #0: loss = 55.7914 (* 1 = 55.7914 loss)
I0402 07:39:06.407297 28499 sgd_solver.cpp:106] Iteration 700, lr = 0.08
I0402 07:39:06.980139 28499 solver.cpp:237] Iteration 720, loss = 47.3025
I0402 07:39:06.980228 28499 solver.cpp:253]     Train net output #0: loss = 47.3025 (* 1 = 47.3025 loss)
I0402 07:39:06.980252 28499 sgd_solver.cpp:106] Iteration 720, lr = 0.08
I0402 07:39:07.557981 28499 solver.cpp:237] Iteration 740, loss = 52.9717
I0402 07:39:07.558075 28499 solver.cpp:253]     Train net output #0: loss = 52.9717 (* 1 = 52.9717 loss)
I0402 07:39:07.558097 28499 sgd_solver.cpp:106] Iteration 740, lr = 0.08
I0402 07:39:08.139444 28499 solver.cpp:237] Iteration 760, loss = 52.467
I0402 07:39:08.139535 28499 solver.cpp:253]     Train net output #0: loss = 52.467 (* 1 = 52.467 loss)
I0402 07:39:08.139559 28499 sgd_solver.cpp:106] Iteration 760, lr = 0.08
I0402 07:39:08.714745 28499 solver.cpp:237] Iteration 780, loss = 57.1916
I0402 07:39:08.714848 28499 solver.cpp:253]     Train net output #0: loss = 57.1916 (* 1 = 57.1916 loss)
I0402 07:39:08.714879 28499 sgd_solver.cpp:106] Iteration 780, lr = 0.08
I0402 07:39:09.296705 28499 solver.cpp:237] Iteration 800, loss = 44.2027
I0402 07:39:09.296803 28499 solver.cpp:253]     Train net output #0: loss = 44.2027 (* 1 = 44.2027 loss)
I0402 07:39:09.296838 28499 sgd_solver.cpp:106] Iteration 800, lr = 0.08
I0402 07:39:09.871788 28499 solver.cpp:237] Iteration 820, loss = 60.0001
I0402 07:39:09.871911 28499 solver.cpp:253]     Train net output #0: loss = 60.0001 (* 1 = 60.0001 loss)
I0402 07:39:09.871943 28499 sgd_solver.cpp:106] Iteration 820, lr = 0.08
I0402 07:39:10.444850 28499 solver.cpp:237] Iteration 840, loss = 50.4356
I0402 07:39:10.444947 28499 solver.cpp:253]     Train net output #0: loss = 50.4356 (* 1 = 50.4356 loss)
I0402 07:39:10.444978 28499 sgd_solver.cpp:106] Iteration 840, lr = 0.08
I0402 07:39:11.024488 28499 solver.cpp:237] Iteration 860, loss = 56.1089
I0402 07:39:11.024593 28499 solver.cpp:253]     Train net output #0: loss = 56.1089 (* 1 = 56.1089 loss)
I0402 07:39:11.024634 28499 sgd_solver.cpp:106] Iteration 860, lr = 0.08
I0402 07:39:11.595839 28499 solver.cpp:237] Iteration 880, loss = 52.3518
I0402 07:39:11.595927 28499 solver.cpp:253]     Train net output #0: loss = 52.3518 (* 1 = 52.3518 loss)
I0402 07:39:11.595950 28499 sgd_solver.cpp:106] Iteration 880, lr = 0.08
I0402 07:39:12.173421 28499 solver.cpp:237] Iteration 900, loss = 59.0406
I0402 07:39:12.173512 28499 solver.cpp:253]     Train net output #0: loss = 59.0406 (* 1 = 59.0406 loss)
I0402 07:39:12.173540 28499 sgd_solver.cpp:106] Iteration 900, lr = 0.08
I0402 07:39:12.746439 28499 solver.cpp:237] Iteration 920, loss = 46.2256
I0402 07:39:12.746523 28499 solver.cpp:253]     Train net output #0: loss = 46.2256 (* 1 = 46.2256 loss)
I0402 07:39:12.746546 28499 sgd_solver.cpp:106] Iteration 920, lr = 0.08
I0402 07:39:13.327394 28499 solver.cpp:237] Iteration 940, loss = 47.2763
I0402 07:39:13.327482 28499 solver.cpp:253]     Train net output #0: loss = 47.2763 (* 1 = 47.2763 loss)
I0402 07:39:13.327505 28499 sgd_solver.cpp:106] Iteration 940, lr = 0.08
I0402 07:39:13.499815 28499 blocking_queue.cpp:50] Data layer prefetch queue empty
I0402 07:39:14.047377 28499 solver.cpp:237] Iteration 960, loss = 54.1762
I0402 07:39:14.047600 28499 solver.cpp:253]     Train net output #0: loss = 54.1762 (* 1 = 54.1762 loss)
I0402 07:39:14.047696 28499 sgd_solver.cpp:106] Iteration 960, lr = 0.08
I0402 07:39:14.961690 28499 solver.cpp:237] Iteration 980, loss = 53.576
I0402 07:39:14.966085 28499 solver.cpp:253]     Train net output #0: loss = 53.576 (* 1 = 53.576 loss)
I0402 07:39:14.966115 28499 sgd_solver.cpp:106] Iteration 980, lr = 0.08
I0402 07:39:15.714833 28499 solver.cpp:341] Iteration 1000, Testing net (#0)
*** Aborted at 1459571987 (unix time) try "date -d @1459571987" if you are using GNU date ***
PC: @     0x7f9de9247414 __pthread_cond_wait
*** SIGTERM (@0x3e800006dc4) received by PID 28499 (TID 0x7f9df58c5ac0) from PID 28100; stack trace: ***
    @     0x7f9df3ab1d40 (unknown)
    @     0x7f9de9247414 __pthread_cond_wait
    @     0x7f9df4db6833 boost::condition_variable::wait()
    @     0x7f9df4db7a68 caffe::BlockingQueue<>::pop()
    @     0x7f9df4f796dd caffe::BasePrefetchingDataLayer<>::Forward_gpu()
    @     0x7f9df4f35d51 caffe::Net<>::ForwardFromTo()
    @     0x7f9df4f360c7 caffe::Net<>::ForwardPrefilled()
    @     0x7f9df4f4dd31 caffe::Solver<>::Test()
    @     0x7f9df4f4e84e caffe::Solver<>::TestAll()
    @     0x7f9df4f4e98a caffe::Solver<>::Step()
    @     0x7f9df4f4f665 caffe::Solver<>::Solve()
    @           0x4081f7 train()
    @           0x405b11 main
    @     0x7f9df3a9cec5 (unknown)
    @           0x406221 (unknown)
    @                0x0 (unknown)
