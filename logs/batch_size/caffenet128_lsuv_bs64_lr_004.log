I0402 07:43:41.192811 28891 upgrade_proto.cpp:990] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': ./caffenet128_lsuv_bs64_lr_004.prototxt
I0402 07:43:41.193049 28891 upgrade_proto.cpp:997] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W0402 07:43:41.193056 28891 upgrade_proto.cpp:999] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I0402 07:43:41.193158 28891 caffe.cpp:184] Using GPUs 0
I0402 07:43:41.482285 28891 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.04
display: 20
max_iter: 1280000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 400000
snapshot: 80000
snapshot_prefix: "snapshots/caffenet128_no_lrn_lsuv_tanh_bs64_004"
solver_mode: GPU
device_id: 0
net_param {
  name: "CaffeNet"
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TRAIN
    }
    transform_param {
      scale: 0.04
      mirror: true
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 124
      force_color: true
    }
    data_param {
      source: "/home/old-ufo/storage/datasets/imagenet/imgs128/lmdb/ilsvrc12_128_train_lmdb"
      batch_size: 64
      backend: LMDB
    }
  }
  layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
      phase: TEST
    }
    transform_param {
      scale: 0.04
      mirror: false
      crop_size: 128
      mean_value: 104
      mean_value: 117
      mean_value: 123
      force_color: true
    }
    data_param {
      source: "/home/old-ufo/storage/datasets/imagenet/imgs128/lmdb/ilsvrc12_128_val_lmdb"
      batch_size: 50
      backend: LMDB
    }
  }
  layer {
    name: "conv1"
    type: "Convolution"
    bottom: "data"
    top: "conv1"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 96
      kernel_size: 11
      stride: 4
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu1"
    type: "TanH"
    bottom: "conv1"
    top: "relu1"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "relu1"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv2"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 2
      kernel_size: 5
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu2"
    type: "TanH"
    bottom: "conv2"
    top: "relu2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "relu2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "conv3"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu3"
    type: "TanH"
    bottom: "conv3"
    top: "relu3"
  }
  layer {
    name: "conv4"
    type: "Convolution"
    bottom: "relu3"
    top: "conv4"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu4"
    type: "TanH"
    bottom: "conv4"
    top: "relu4"
  }
  layer {
    name: "conv5"
    type: "Convolution"
    bottom: "relu4"
    top: "conv5"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
      group: 2
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu5"
    type: "TanH"
    bottom: "conv5"
    top: "relu5"
  }
  layer {
    name: "pool5"
    type: "Pooling"
    bottom: "relu5"
    top: "pool5"
    pooling_param {
      pool: MAX
      kernel_size: 3
      stride: 2
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "pool5"
    top: "fc6"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu6"
    type: "TanH"
    bottom: "fc6"
    top: "relu6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "relu6"
    top: "drop6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "drop6"
    top: "fc7"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 2048
      weight_filler {
        type: "gaussian"
        std: 0.005
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "relu7"
    type: "TanH"
    bottom: "fc7"
    top: "relu7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "relu7"
    top: "drop7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc8"
    type: "InnerProduct"
    bottom: "drop7"
    top: "fc8"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 1000
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "accuracy"
    type: "Accuracy"
    bottom: "fc8"
    bottom: "label"
    top: "accuracy"
    include {
      phase: TEST
    }
  }
  layer {
    name: "loss"
    type: "SoftmaxWithLoss"
    bottom: "fc8"
    bottom: "label"
    top: "loss"
  }
}
test_initialization: true
iter_size: 1
type: "SGD"
I0402 07:43:41.484074 28891 solver.cpp:86] Creating training net specified in net_param.
I0402 07:43:41.484199 28891 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0402 07:43:41.484230 28891 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0402 07:43:41.484455 28891 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.04
    mirror: true
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 124
    force_color: true
  }
  data_param {
    source: "/home/old-ufo/storage/datasets/imagenet/imgs128/lmdb/ilsvrc12_128_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "TanH"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "TanH"
  bottom: "conv2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "TanH"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "TanH"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "TanH"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "TanH"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "TanH"
  bottom: "fc7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0402 07:43:41.484596 28891 layer_factory.hpp:76] Creating layer data
I0402 07:43:41.485249 28891 net.cpp:106] Creating Layer data
I0402 07:43:41.485262 28891 net.cpp:411] data -> data
I0402 07:43:41.485298 28891 net.cpp:411] data -> label
I0402 07:43:41.486245 28896 db_lmdb.cpp:38] Opened lmdb /home/old-ufo/storage/datasets/imagenet/imgs128/lmdb/ilsvrc12_128_train_lmdb
I0402 07:43:41.534951 28891 data_layer.cpp:41] output data size: 64,3,128,128
I0402 07:43:41.558370 28891 net.cpp:150] Setting up data
I0402 07:43:41.558404 28891 net.cpp:157] Top shape: 64 3 128 128 (3145728)
I0402 07:43:41.558413 28891 net.cpp:157] Top shape: 64 (64)
I0402 07:43:41.558419 28891 net.cpp:165] Memory required for data: 12583168
I0402 07:43:41.558450 28891 layer_factory.hpp:76] Creating layer conv1
I0402 07:43:41.558477 28891 net.cpp:106] Creating Layer conv1
I0402 07:43:41.558485 28891 net.cpp:454] conv1 <- data
I0402 07:43:41.558501 28891 net.cpp:411] conv1 -> conv1
I0402 07:43:41.770608 28891 net.cpp:150] Setting up conv1
I0402 07:43:41.770642 28891 net.cpp:157] Top shape: 64 96 30 30 (5529600)
I0402 07:43:41.770648 28891 net.cpp:165] Memory required for data: 34701568
I0402 07:43:41.770766 28891 layer_factory.hpp:76] Creating layer relu1
I0402 07:43:41.770787 28891 net.cpp:106] Creating Layer relu1
I0402 07:43:41.770795 28891 net.cpp:454] relu1 <- conv1
I0402 07:43:41.770809 28891 net.cpp:411] relu1 -> relu1
I0402 07:43:41.771595 28891 net.cpp:150] Setting up relu1
I0402 07:43:41.771613 28891 net.cpp:157] Top shape: 64 96 30 30 (5529600)
I0402 07:43:41.771620 28891 net.cpp:165] Memory required for data: 56819968
I0402 07:43:41.771625 28891 layer_factory.hpp:76] Creating layer pool1
I0402 07:43:41.771634 28891 net.cpp:106] Creating Layer pool1
I0402 07:43:41.771638 28891 net.cpp:454] pool1 <- relu1
I0402 07:43:41.771646 28891 net.cpp:411] pool1 -> pool1
I0402 07:43:41.773376 28891 net.cpp:150] Setting up pool1
I0402 07:43:41.773391 28891 net.cpp:157] Top shape: 64 96 15 15 (1382400)
I0402 07:43:41.773394 28891 net.cpp:165] Memory required for data: 62349568
I0402 07:43:41.773399 28891 layer_factory.hpp:76] Creating layer conv2
I0402 07:43:41.773414 28891 net.cpp:106] Creating Layer conv2
I0402 07:43:41.773419 28891 net.cpp:454] conv2 <- pool1
I0402 07:43:41.773429 28891 net.cpp:411] conv2 -> conv2
I0402 07:43:41.789857 28891 net.cpp:150] Setting up conv2
I0402 07:43:41.789891 28891 net.cpp:157] Top shape: 64 256 15 15 (3686400)
I0402 07:43:41.789896 28891 net.cpp:165] Memory required for data: 77095168
I0402 07:43:41.789994 28891 layer_factory.hpp:76] Creating layer relu2
I0402 07:43:41.790009 28891 net.cpp:106] Creating Layer relu2
I0402 07:43:41.790014 28891 net.cpp:454] relu2 <- conv2
I0402 07:43:41.790025 28891 net.cpp:411] relu2 -> relu2
I0402 07:43:41.790906 28891 net.cpp:150] Setting up relu2
I0402 07:43:41.790922 28891 net.cpp:157] Top shape: 64 256 15 15 (3686400)
I0402 07:43:41.790961 28891 net.cpp:165] Memory required for data: 91840768
I0402 07:43:41.790969 28891 layer_factory.hpp:76] Creating layer pool2
I0402 07:43:41.790979 28891 net.cpp:106] Creating Layer pool2
I0402 07:43:41.790984 28891 net.cpp:454] pool2 <- relu2
I0402 07:43:41.790992 28891 net.cpp:411] pool2 -> pool2
I0402 07:43:41.792053 28891 net.cpp:150] Setting up pool2
I0402 07:43:41.792068 28891 net.cpp:157] Top shape: 64 256 7 7 (802816)
I0402 07:43:41.792109 28891 net.cpp:165] Memory required for data: 95052032
I0402 07:43:41.792117 28891 layer_factory.hpp:76] Creating layer conv3
I0402 07:43:41.792134 28891 net.cpp:106] Creating Layer conv3
I0402 07:43:41.792174 28891 net.cpp:454] conv3 <- pool2
I0402 07:43:41.792189 28891 net.cpp:411] conv3 -> conv3
I0402 07:43:41.823518 28891 net.cpp:150] Setting up conv3
I0402 07:43:41.823550 28891 net.cpp:157] Top shape: 64 384 7 7 (1204224)
I0402 07:43:41.823556 28891 net.cpp:165] Memory required for data: 99868928
I0402 07:43:41.823649 28891 layer_factory.hpp:76] Creating layer relu3
I0402 07:43:41.823670 28891 net.cpp:106] Creating Layer relu3
I0402 07:43:41.823676 28891 net.cpp:454] relu3 <- conv3
I0402 07:43:41.823685 28891 net.cpp:411] relu3 -> relu3
I0402 07:43:41.825237 28891 net.cpp:150] Setting up relu3
I0402 07:43:41.825254 28891 net.cpp:157] Top shape: 64 384 7 7 (1204224)
I0402 07:43:41.825295 28891 net.cpp:165] Memory required for data: 104685824
I0402 07:43:41.825305 28891 layer_factory.hpp:76] Creating layer conv4
I0402 07:43:41.825320 28891 net.cpp:106] Creating Layer conv4
I0402 07:43:41.825325 28891 net.cpp:454] conv4 <- relu3
I0402 07:43:41.825371 28891 net.cpp:411] conv4 -> conv4
I0402 07:43:41.855453 28891 net.cpp:150] Setting up conv4
I0402 07:43:41.855485 28891 net.cpp:157] Top shape: 64 384 7 7 (1204224)
I0402 07:43:41.855491 28891 net.cpp:165] Memory required for data: 109502720
I0402 07:43:41.855545 28891 layer_factory.hpp:76] Creating layer relu4
I0402 07:43:41.855558 28891 net.cpp:106] Creating Layer relu4
I0402 07:43:41.855564 28891 net.cpp:454] relu4 <- conv4
I0402 07:43:41.855573 28891 net.cpp:411] relu4 -> relu4
I0402 07:43:41.856405 28891 net.cpp:150] Setting up relu4
I0402 07:43:41.856420 28891 net.cpp:157] Top shape: 64 384 7 7 (1204224)
I0402 07:43:41.856426 28891 net.cpp:165] Memory required for data: 114319616
I0402 07:43:41.856431 28891 layer_factory.hpp:76] Creating layer conv5
I0402 07:43:41.856446 28891 net.cpp:106] Creating Layer conv5
I0402 07:43:41.856451 28891 net.cpp:454] conv5 <- relu4
I0402 07:43:41.856462 28891 net.cpp:411] conv5 -> conv5
I0402 07:43:41.878177 28891 net.cpp:150] Setting up conv5
I0402 07:43:41.878213 28891 net.cpp:157] Top shape: 64 256 7 7 (802816)
I0402 07:43:41.878221 28891 net.cpp:165] Memory required for data: 117530880
I0402 07:43:41.878242 28891 layer_factory.hpp:76] Creating layer relu5
I0402 07:43:41.878254 28891 net.cpp:106] Creating Layer relu5
I0402 07:43:41.878262 28891 net.cpp:454] relu5 <- conv5
I0402 07:43:41.878273 28891 net.cpp:411] relu5 -> relu5
I0402 07:43:41.879107 28891 net.cpp:150] Setting up relu5
I0402 07:43:41.879124 28891 net.cpp:157] Top shape: 64 256 7 7 (802816)
I0402 07:43:41.879129 28891 net.cpp:165] Memory required for data: 120742144
I0402 07:43:41.879134 28891 layer_factory.hpp:76] Creating layer pool5
I0402 07:43:41.879144 28891 net.cpp:106] Creating Layer pool5
I0402 07:43:41.879149 28891 net.cpp:454] pool5 <- relu5
I0402 07:43:41.879156 28891 net.cpp:411] pool5 -> pool5
I0402 07:43:41.880039 28891 net.cpp:150] Setting up pool5
I0402 07:43:41.880053 28891 net.cpp:157] Top shape: 64 256 3 3 (147456)
I0402 07:43:41.880059 28891 net.cpp:165] Memory required for data: 121331968
I0402 07:43:41.880064 28891 layer_factory.hpp:76] Creating layer fc6
I0402 07:43:41.880081 28891 net.cpp:106] Creating Layer fc6
I0402 07:43:41.880086 28891 net.cpp:454] fc6 <- pool5
I0402 07:43:41.880095 28891 net.cpp:411] fc6 -> fc6
I0402 07:43:42.049576 28891 net.cpp:150] Setting up fc6
I0402 07:43:42.049607 28891 net.cpp:157] Top shape: 64 2048 (131072)
I0402 07:43:42.049612 28891 net.cpp:165] Memory required for data: 121856256
I0402 07:43:42.049628 28891 layer_factory.hpp:76] Creating layer relu6
I0402 07:43:42.049640 28891 net.cpp:106] Creating Layer relu6
I0402 07:43:42.049648 28891 net.cpp:454] relu6 <- fc6
I0402 07:43:42.049660 28891 net.cpp:411] relu6 -> relu6
I0402 07:43:42.050649 28891 net.cpp:150] Setting up relu6
I0402 07:43:42.050667 28891 net.cpp:157] Top shape: 64 2048 (131072)
I0402 07:43:42.050671 28891 net.cpp:165] Memory required for data: 122380544
I0402 07:43:42.050678 28891 layer_factory.hpp:76] Creating layer drop6
I0402 07:43:42.050688 28891 net.cpp:106] Creating Layer drop6
I0402 07:43:42.050694 28891 net.cpp:454] drop6 <- relu6
I0402 07:43:42.050704 28891 net.cpp:411] drop6 -> drop6
I0402 07:43:42.050765 28891 net.cpp:150] Setting up drop6
I0402 07:43:42.050773 28891 net.cpp:157] Top shape: 64 2048 (131072)
I0402 07:43:42.050777 28891 net.cpp:165] Memory required for data: 122904832
I0402 07:43:42.050781 28891 layer_factory.hpp:76] Creating layer fc7
I0402 07:43:42.050793 28891 net.cpp:106] Creating Layer fc7
I0402 07:43:42.050797 28891 net.cpp:454] fc7 <- drop6
I0402 07:43:42.050806 28891 net.cpp:411] fc7 -> fc7
I0402 07:43:42.196590 28891 net.cpp:150] Setting up fc7
I0402 07:43:42.196621 28891 net.cpp:157] Top shape: 64 2048 (131072)
I0402 07:43:42.196627 28891 net.cpp:165] Memory required for data: 123429120
I0402 07:43:42.196636 28891 layer_factory.hpp:76] Creating layer relu7
I0402 07:43:42.196645 28891 net.cpp:106] Creating Layer relu7
I0402 07:43:42.196648 28891 net.cpp:454] relu7 <- fc7
I0402 07:43:42.196656 28891 net.cpp:411] relu7 -> relu7
I0402 07:43:42.197389 28891 net.cpp:150] Setting up relu7
I0402 07:43:42.197402 28891 net.cpp:157] Top shape: 64 2048 (131072)
I0402 07:43:42.197407 28891 net.cpp:165] Memory required for data: 123953408
I0402 07:43:42.197412 28891 layer_factory.hpp:76] Creating layer drop7
I0402 07:43:42.197438 28891 net.cpp:106] Creating Layer drop7
I0402 07:43:42.197443 28891 net.cpp:454] drop7 <- relu7
I0402 07:43:42.197450 28891 net.cpp:411] drop7 -> drop7
I0402 07:43:42.197506 28891 net.cpp:150] Setting up drop7
I0402 07:43:42.197517 28891 net.cpp:157] Top shape: 64 2048 (131072)
I0402 07:43:42.197522 28891 net.cpp:165] Memory required for data: 124477696
I0402 07:43:42.197526 28891 layer_factory.hpp:76] Creating layer fc8
I0402 07:43:42.197537 28891 net.cpp:106] Creating Layer fc8
I0402 07:43:42.197541 28891 net.cpp:454] fc8 <- drop7
I0402 07:43:42.197553 28891 net.cpp:411] fc8 -> fc8
I0402 07:43:42.250569 28891 net.cpp:150] Setting up fc8
I0402 07:43:42.250597 28891 net.cpp:157] Top shape: 64 1000 (64000)
I0402 07:43:42.250600 28891 net.cpp:165] Memory required for data: 124733696
I0402 07:43:42.250612 28891 layer_factory.hpp:76] Creating layer loss
I0402 07:43:42.250622 28891 net.cpp:106] Creating Layer loss
I0402 07:43:42.250628 28891 net.cpp:454] loss <- fc8
I0402 07:43:42.250635 28891 net.cpp:454] loss <- label
I0402 07:43:42.250645 28891 net.cpp:411] loss -> loss
I0402 07:43:42.250659 28891 layer_factory.hpp:76] Creating layer loss
I0402 07:43:42.251619 28891 net.cpp:150] Setting up loss
I0402 07:43:42.251631 28891 net.cpp:157] Top shape: (1)
I0402 07:43:42.251636 28891 net.cpp:160]     with loss weight 1
I0402 07:43:42.251657 28891 net.cpp:165] Memory required for data: 124733700
I0402 07:43:42.251663 28891 net.cpp:226] loss needs backward computation.
I0402 07:43:42.251668 28891 net.cpp:226] fc8 needs backward computation.
I0402 07:43:42.251672 28891 net.cpp:226] drop7 needs backward computation.
I0402 07:43:42.251677 28891 net.cpp:226] relu7 needs backward computation.
I0402 07:43:42.251682 28891 net.cpp:226] fc7 needs backward computation.
I0402 07:43:42.251685 28891 net.cpp:226] drop6 needs backward computation.
I0402 07:43:42.251689 28891 net.cpp:226] relu6 needs backward computation.
I0402 07:43:42.251694 28891 net.cpp:226] fc6 needs backward computation.
I0402 07:43:42.251698 28891 net.cpp:226] pool5 needs backward computation.
I0402 07:43:42.251703 28891 net.cpp:226] relu5 needs backward computation.
I0402 07:43:42.251706 28891 net.cpp:226] conv5 needs backward computation.
I0402 07:43:42.251713 28891 net.cpp:226] relu4 needs backward computation.
I0402 07:43:42.251718 28891 net.cpp:226] conv4 needs backward computation.
I0402 07:43:42.251723 28891 net.cpp:226] relu3 needs backward computation.
I0402 07:43:42.251726 28891 net.cpp:226] conv3 needs backward computation.
I0402 07:43:42.251730 28891 net.cpp:226] pool2 needs backward computation.
I0402 07:43:42.251734 28891 net.cpp:226] relu2 needs backward computation.
I0402 07:43:42.251739 28891 net.cpp:226] conv2 needs backward computation.
I0402 07:43:42.251744 28891 net.cpp:226] pool1 needs backward computation.
I0402 07:43:42.251747 28891 net.cpp:226] relu1 needs backward computation.
I0402 07:43:42.251752 28891 net.cpp:226] conv1 needs backward computation.
I0402 07:43:42.251756 28891 net.cpp:228] data does not need backward computation.
I0402 07:43:42.251760 28891 net.cpp:270] This network produces output loss
I0402 07:43:42.251781 28891 net.cpp:283] Network initialization done.
I0402 07:43:42.251927 28891 solver.cpp:181] Creating test net (#0) specified by net_param
I0402 07:43:42.251982 28891 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0402 07:43:42.252161 28891 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.04
    mirror: false
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 123
    force_color: true
  }
  data_param {
    source: "/home/old-ufo/storage/datasets/imagenet/imgs128/lmdb/ilsvrc12_128_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "TanH"
  bottom: "conv1"
  top: "relu1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "TanH"
  bottom: "conv2"
  top: "relu2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "TanH"
  bottom: "conv3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "TanH"
  bottom: "conv4"
  top: "relu4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu5"
  type: "TanH"
  bottom: "conv5"
  top: "relu5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "relu5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu6"
  type: "TanH"
  bottom: "fc6"
  top: "relu6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "relu6"
  top: "drop6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "drop6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2048
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu7"
  type: "TanH"
  bottom: "fc7"
  top: "relu7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "relu7"
  top: "drop7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "drop7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0402 07:43:42.252322 28891 layer_factory.hpp:76] Creating layer data
I0402 07:43:42.252436 28891 net.cpp:106] Creating Layer data
I0402 07:43:42.252447 28891 net.cpp:411] data -> data
I0402 07:43:42.252460 28891 net.cpp:411] data -> label
I0402 07:43:42.253329 28898 db_lmdb.cpp:38] Opened lmdb /home/old-ufo/storage/datasets/imagenet/imgs128/lmdb/ilsvrc12_128_val_lmdb
I0402 07:43:42.253962 28891 data_layer.cpp:41] output data size: 50,3,128,128
I0402 07:43:42.269453 28891 net.cpp:150] Setting up data
I0402 07:43:42.269482 28891 net.cpp:157] Top shape: 50 3 128 128 (2457600)
I0402 07:43:42.269490 28891 net.cpp:157] Top shape: 50 (50)
I0402 07:43:42.269495 28891 net.cpp:165] Memory required for data: 9830600
I0402 07:43:42.269502 28891 layer_factory.hpp:76] Creating layer label_data_1_split
I0402 07:43:42.269515 28891 net.cpp:106] Creating Layer label_data_1_split
I0402 07:43:42.269521 28891 net.cpp:454] label_data_1_split <- label
I0402 07:43:42.269531 28891 net.cpp:411] label_data_1_split -> label_data_1_split_0
I0402 07:43:42.269542 28891 net.cpp:411] label_data_1_split -> label_data_1_split_1
I0402 07:43:42.269609 28891 net.cpp:150] Setting up label_data_1_split
I0402 07:43:42.269620 28891 net.cpp:157] Top shape: 50 (50)
I0402 07:43:42.269625 28891 net.cpp:157] Top shape: 50 (50)
I0402 07:43:42.269629 28891 net.cpp:165] Memory required for data: 9831000
I0402 07:43:42.269634 28891 layer_factory.hpp:76] Creating layer conv1
I0402 07:43:42.269647 28891 net.cpp:106] Creating Layer conv1
I0402 07:43:42.269652 28891 net.cpp:454] conv1 <- data
I0402 07:43:42.269660 28891 net.cpp:411] conv1 -> conv1
I0402 07:43:42.275159 28891 net.cpp:150] Setting up conv1
I0402 07:43:42.275188 28891 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I0402 07:43:42.275194 28891 net.cpp:165] Memory required for data: 27111000
I0402 07:43:42.275213 28891 layer_factory.hpp:76] Creating layer relu1
I0402 07:43:42.275229 28891 net.cpp:106] Creating Layer relu1
I0402 07:43:42.275238 28891 net.cpp:454] relu1 <- conv1
I0402 07:43:42.275264 28891 net.cpp:411] relu1 -> relu1
I0402 07:43:42.276283 28891 net.cpp:150] Setting up relu1
I0402 07:43:42.276298 28891 net.cpp:157] Top shape: 50 96 30 30 (4320000)
I0402 07:43:42.276303 28891 net.cpp:165] Memory required for data: 44391000
I0402 07:43:42.276309 28891 layer_factory.hpp:76] Creating layer pool1
I0402 07:43:42.276320 28891 net.cpp:106] Creating Layer pool1
I0402 07:43:42.276325 28891 net.cpp:454] pool1 <- relu1
I0402 07:43:42.276335 28891 net.cpp:411] pool1 -> pool1
I0402 07:43:42.279414 28891 net.cpp:150] Setting up pool1
I0402 07:43:42.279434 28891 net.cpp:157] Top shape: 50 96 15 15 (1080000)
I0402 07:43:42.279439 28891 net.cpp:165] Memory required for data: 48711000
I0402 07:43:42.279445 28891 layer_factory.hpp:76] Creating layer conv2
I0402 07:43:42.279461 28891 net.cpp:106] Creating Layer conv2
I0402 07:43:42.279469 28891 net.cpp:454] conv2 <- pool1
I0402 07:43:42.279480 28891 net.cpp:411] conv2 -> conv2
I0402 07:43:42.300674 28891 net.cpp:150] Setting up conv2
I0402 07:43:42.300704 28891 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I0402 07:43:42.300709 28891 net.cpp:165] Memory required for data: 60231000
I0402 07:43:42.300726 28891 layer_factory.hpp:76] Creating layer relu2
I0402 07:43:42.300740 28891 net.cpp:106] Creating Layer relu2
I0402 07:43:42.300746 28891 net.cpp:454] relu2 <- conv2
I0402 07:43:42.300756 28891 net.cpp:411] relu2 -> relu2
I0402 07:43:42.301833 28891 net.cpp:150] Setting up relu2
I0402 07:43:42.301848 28891 net.cpp:157] Top shape: 50 256 15 15 (2880000)
I0402 07:43:42.301853 28891 net.cpp:165] Memory required for data: 71751000
I0402 07:43:42.301857 28891 layer_factory.hpp:76] Creating layer pool2
I0402 07:43:42.301867 28891 net.cpp:106] Creating Layer pool2
I0402 07:43:42.301872 28891 net.cpp:454] pool2 <- relu2
I0402 07:43:42.301879 28891 net.cpp:411] pool2 -> pool2
I0402 07:43:42.303004 28891 net.cpp:150] Setting up pool2
I0402 07:43:42.303017 28891 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0402 07:43:42.303037 28891 net.cpp:165] Memory required for data: 74259800
I0402 07:43:42.303043 28891 layer_factory.hpp:76] Creating layer conv3
I0402 07:43:42.303059 28891 net.cpp:106] Creating Layer conv3
I0402 07:43:42.303066 28891 net.cpp:454] conv3 <- pool2
I0402 07:43:42.303074 28891 net.cpp:411] conv3 -> conv3
I0402 07:43:42.341205 28891 net.cpp:150] Setting up conv3
I0402 07:43:42.341238 28891 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0402 07:43:42.341243 28891 net.cpp:165] Memory required for data: 78023000
I0402 07:43:42.341261 28891 layer_factory.hpp:76] Creating layer relu3
I0402 07:43:42.341274 28891 net.cpp:106] Creating Layer relu3
I0402 07:43:42.341281 28891 net.cpp:454] relu3 <- conv3
I0402 07:43:42.341294 28891 net.cpp:411] relu3 -> relu3
I0402 07:43:42.342236 28891 net.cpp:150] Setting up relu3
I0402 07:43:42.342255 28891 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0402 07:43:42.342260 28891 net.cpp:165] Memory required for data: 81786200
I0402 07:43:42.342267 28891 layer_factory.hpp:76] Creating layer conv4
I0402 07:43:42.342280 28891 net.cpp:106] Creating Layer conv4
I0402 07:43:42.342286 28891 net.cpp:454] conv4 <- relu3
I0402 07:43:42.342298 28891 net.cpp:411] conv4 -> conv4
I0402 07:43:42.374407 28891 net.cpp:150] Setting up conv4
I0402 07:43:42.374475 28891 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0402 07:43:42.374483 28891 net.cpp:165] Memory required for data: 85549400
I0402 07:43:42.374498 28891 layer_factory.hpp:76] Creating layer relu4
I0402 07:43:42.374511 28891 net.cpp:106] Creating Layer relu4
I0402 07:43:42.374518 28891 net.cpp:454] relu4 <- conv4
I0402 07:43:42.374528 28891 net.cpp:411] relu4 -> relu4
I0402 07:43:42.375481 28891 net.cpp:150] Setting up relu4
I0402 07:43:42.375497 28891 net.cpp:157] Top shape: 50 384 7 7 (940800)
I0402 07:43:42.375502 28891 net.cpp:165] Memory required for data: 89312600
I0402 07:43:42.375509 28891 layer_factory.hpp:76] Creating layer conv5
I0402 07:43:42.375524 28891 net.cpp:106] Creating Layer conv5
I0402 07:43:42.375531 28891 net.cpp:454] conv5 <- relu4
I0402 07:43:42.375541 28891 net.cpp:411] conv5 -> conv5
I0402 07:43:42.398069 28891 net.cpp:150] Setting up conv5
I0402 07:43:42.398102 28891 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0402 07:43:42.398108 28891 net.cpp:165] Memory required for data: 91821400
I0402 07:43:42.398128 28891 layer_factory.hpp:76] Creating layer relu5
I0402 07:43:42.398143 28891 net.cpp:106] Creating Layer relu5
I0402 07:43:42.398149 28891 net.cpp:454] relu5 <- conv5
I0402 07:43:42.398161 28891 net.cpp:411] relu5 -> relu5
I0402 07:43:42.399600 28891 net.cpp:150] Setting up relu5
I0402 07:43:42.399615 28891 net.cpp:157] Top shape: 50 256 7 7 (627200)
I0402 07:43:42.399619 28891 net.cpp:165] Memory required for data: 94330200
I0402 07:43:42.399624 28891 layer_factory.hpp:76] Creating layer pool5
I0402 07:43:42.399636 28891 net.cpp:106] Creating Layer pool5
I0402 07:43:42.399641 28891 net.cpp:454] pool5 <- relu5
I0402 07:43:42.399649 28891 net.cpp:411] pool5 -> pool5
I0402 07:43:42.400881 28891 net.cpp:150] Setting up pool5
I0402 07:43:42.400894 28891 net.cpp:157] Top shape: 50 256 3 3 (115200)
I0402 07:43:42.400898 28891 net.cpp:165] Memory required for data: 94791000
I0402 07:43:42.400903 28891 layer_factory.hpp:76] Creating layer fc6
I0402 07:43:42.400918 28891 net.cpp:106] Creating Layer fc6
I0402 07:43:42.400923 28891 net.cpp:454] fc6 <- pool5
I0402 07:43:42.400933 28891 net.cpp:411] fc6 -> fc6
I0402 07:43:42.522402 28891 net.cpp:150] Setting up fc6
I0402 07:43:42.522431 28891 net.cpp:157] Top shape: 50 2048 (102400)
I0402 07:43:42.522439 28891 net.cpp:165] Memory required for data: 95200600
I0402 07:43:42.522454 28891 layer_factory.hpp:76] Creating layer relu6
I0402 07:43:42.522474 28891 net.cpp:106] Creating Layer relu6
I0402 07:43:42.522481 28891 net.cpp:454] relu6 <- fc6
I0402 07:43:42.522496 28891 net.cpp:411] relu6 -> relu6
I0402 07:43:42.523517 28891 net.cpp:150] Setting up relu6
I0402 07:43:42.523532 28891 net.cpp:157] Top shape: 50 2048 (102400)
I0402 07:43:42.523537 28891 net.cpp:165] Memory required for data: 95610200
I0402 07:43:42.526446 28891 layer_factory.hpp:76] Creating layer drop6
I0402 07:43:42.526464 28891 net.cpp:106] Creating Layer drop6
I0402 07:43:42.526470 28891 net.cpp:454] drop6 <- relu6
I0402 07:43:42.526482 28891 net.cpp:411] drop6 -> drop6
I0402 07:43:42.526582 28891 net.cpp:150] Setting up drop6
I0402 07:43:42.526595 28891 net.cpp:157] Top shape: 50 2048 (102400)
I0402 07:43:42.526600 28891 net.cpp:165] Memory required for data: 96019800
I0402 07:43:42.526605 28891 layer_factory.hpp:76] Creating layer fc7
I0402 07:43:42.526618 28891 net.cpp:106] Creating Layer fc7
I0402 07:43:42.526623 28891 net.cpp:454] fc7 <- drop6
I0402 07:43:42.526631 28891 net.cpp:411] fc7 -> fc7
I0402 07:43:42.635071 28891 net.cpp:150] Setting up fc7
I0402 07:43:42.635097 28891 net.cpp:157] Top shape: 50 2048 (102400)
I0402 07:43:42.635102 28891 net.cpp:165] Memory required for data: 96429400
I0402 07:43:42.635116 28891 layer_factory.hpp:76] Creating layer relu7
I0402 07:43:42.635133 28891 net.cpp:106] Creating Layer relu7
I0402 07:43:42.635143 28891 net.cpp:454] relu7 <- fc7
I0402 07:43:42.635154 28891 net.cpp:411] relu7 -> relu7
I0402 07:43:42.636086 28891 net.cpp:150] Setting up relu7
I0402 07:43:42.636101 28891 net.cpp:157] Top shape: 50 2048 (102400)
I0402 07:43:42.636106 28891 net.cpp:165] Memory required for data: 96839000
I0402 07:43:42.636111 28891 layer_factory.hpp:76] Creating layer drop7
I0402 07:43:42.636121 28891 net.cpp:106] Creating Layer drop7
I0402 07:43:42.636127 28891 net.cpp:454] drop7 <- relu7
I0402 07:43:42.636135 28891 net.cpp:411] drop7 -> drop7
I0402 07:43:42.636195 28891 net.cpp:150] Setting up drop7
I0402 07:43:42.636204 28891 net.cpp:157] Top shape: 50 2048 (102400)
I0402 07:43:42.636207 28891 net.cpp:165] Memory required for data: 97248600
I0402 07:43:42.636211 28891 layer_factory.hpp:76] Creating layer fc8
I0402 07:43:42.636224 28891 net.cpp:106] Creating Layer fc8
I0402 07:43:42.636230 28891 net.cpp:454] fc8 <- drop7
I0402 07:43:42.636240 28891 net.cpp:411] fc8 -> fc8
I0402 07:43:42.689105 28891 net.cpp:150] Setting up fc8
I0402 07:43:42.689134 28891 net.cpp:157] Top shape: 50 1000 (50000)
I0402 07:43:42.689141 28891 net.cpp:165] Memory required for data: 97448600
I0402 07:43:42.689154 28891 layer_factory.hpp:76] Creating layer fc8_fc8_0_split
I0402 07:43:42.689167 28891 net.cpp:106] Creating Layer fc8_fc8_0_split
I0402 07:43:42.689177 28891 net.cpp:454] fc8_fc8_0_split <- fc8
I0402 07:43:42.689189 28891 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0402 07:43:42.689203 28891 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0402 07:43:42.689291 28891 net.cpp:150] Setting up fc8_fc8_0_split
I0402 07:43:42.689299 28891 net.cpp:157] Top shape: 50 1000 (50000)
I0402 07:43:42.689304 28891 net.cpp:157] Top shape: 50 1000 (50000)
I0402 07:43:42.689309 28891 net.cpp:165] Memory required for data: 97848600
I0402 07:43:42.689316 28891 layer_factory.hpp:76] Creating layer accuracy
I0402 07:43:42.689332 28891 net.cpp:106] Creating Layer accuracy
I0402 07:43:42.689337 28891 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I0402 07:43:42.689354 28891 net.cpp:454] accuracy <- label_data_1_split_0
I0402 07:43:42.689365 28891 net.cpp:411] accuracy -> accuracy
I0402 07:43:42.689379 28891 net.cpp:150] Setting up accuracy
I0402 07:43:42.689388 28891 net.cpp:157] Top shape: (1)
I0402 07:43:42.689393 28891 net.cpp:165] Memory required for data: 97848604
I0402 07:43:42.689399 28891 layer_factory.hpp:76] Creating layer loss
I0402 07:43:42.689406 28891 net.cpp:106] Creating Layer loss
I0402 07:43:42.689412 28891 net.cpp:454] loss <- fc8_fc8_0_split_1
I0402 07:43:42.689419 28891 net.cpp:454] loss <- label_data_1_split_1
I0402 07:43:42.689426 28891 net.cpp:411] loss -> loss
I0402 07:43:42.689440 28891 layer_factory.hpp:76] Creating layer loss
I0402 07:43:42.690490 28891 net.cpp:150] Setting up loss
I0402 07:43:42.690505 28891 net.cpp:157] Top shape: (1)
I0402 07:43:42.690508 28891 net.cpp:160]     with loss weight 1
I0402 07:43:42.690521 28891 net.cpp:165] Memory required for data: 97848608
I0402 07:43:42.690527 28891 net.cpp:226] loss needs backward computation.
I0402 07:43:42.690549 28891 net.cpp:228] accuracy does not need backward computation.
I0402 07:43:42.690556 28891 net.cpp:226] fc8_fc8_0_split needs backward computation.
I0402 07:43:42.690559 28891 net.cpp:226] fc8 needs backward computation.
I0402 07:43:42.690564 28891 net.cpp:226] drop7 needs backward computation.
I0402 07:43:42.690570 28891 net.cpp:226] relu7 needs backward computation.
I0402 07:43:42.690575 28891 net.cpp:226] fc7 needs backward computation.
I0402 07:43:42.690579 28891 net.cpp:226] drop6 needs backward computation.
I0402 07:43:42.690584 28891 net.cpp:226] relu6 needs backward computation.
I0402 07:43:42.690588 28891 net.cpp:226] fc6 needs backward computation.
I0402 07:43:42.690593 28891 net.cpp:226] pool5 needs backward computation.
I0402 07:43:42.690598 28891 net.cpp:226] relu5 needs backward computation.
I0402 07:43:42.690603 28891 net.cpp:226] conv5 needs backward computation.
I0402 07:43:42.690608 28891 net.cpp:226] relu4 needs backward computation.
I0402 07:43:42.690613 28891 net.cpp:226] conv4 needs backward computation.
I0402 07:43:42.690616 28891 net.cpp:226] relu3 needs backward computation.
I0402 07:43:42.690621 28891 net.cpp:226] conv3 needs backward computation.
I0402 07:43:42.690625 28891 net.cpp:226] pool2 needs backward computation.
I0402 07:43:42.690630 28891 net.cpp:226] relu2 needs backward computation.
I0402 07:43:42.690635 28891 net.cpp:226] conv2 needs backward computation.
I0402 07:43:42.690640 28891 net.cpp:226] pool1 needs backward computation.
I0402 07:43:42.690644 28891 net.cpp:226] relu1 needs backward computation.
I0402 07:43:42.690649 28891 net.cpp:226] conv1 needs backward computation.
I0402 07:43:42.690654 28891 net.cpp:228] label_data_1_split does not need backward computation.
I0402 07:43:42.690659 28891 net.cpp:228] data does not need backward computation.
I0402 07:43:42.690664 28891 net.cpp:270] This network produces output accuracy
I0402 07:43:42.690668 28891 net.cpp:270] This network produces output loss
I0402 07:43:42.690693 28891 net.cpp:283] Network initialization done.
I0402 07:43:42.690810 28891 solver.cpp:60] Solver scaffolding done.
I0402 07:43:42.691416 28891 caffe.cpp:128] Finetuning from ./caffenet128_lsuv_bs64_lr_004.prototxt.caffemodel
I0402 07:43:42.887500 28891 caffe.cpp:212] Starting Optimization
I0402 07:43:42.887523 28891 solver.cpp:288] Solving CaffeNet
I0402 07:43:42.887528 28891 solver.cpp:289] Learning Rate Policy: step
I0402 07:43:42.888658 28891 solver.cpp:341] Iteration 0, Testing net (#0)
I0402 07:43:43.099892 28891 blocking_queue.cpp:50] Data layer prefetch queue empty
I0402 07:44:16.557276 28891 solver.cpp:409]     Test net output #0: accuracy = 0.00086
I0402 07:44:16.557346 28891 solver.cpp:409]     Test net output #1: loss = 7.08458 (* 1 = 7.08458 loss)
I0402 07:44:16.590070 28891 solver.cpp:237] Iteration 0, loss = 7.18433
I0402 07:44:16.590159 28891 solver.cpp:253]     Train net output #0: loss = 7.18433 (* 1 = 7.18433 loss)
I0402 07:44:16.590195 28891 sgd_solver.cpp:106] Iteration 0, lr = 0.04
I0402 07:44:17.102946 28891 blocking_queue.cpp:50] Data layer prefetch queue empty
I0402 07:44:17.478034 28891 solver.cpp:237] Iteration 20, loss = 7.8859
I0402 07:44:17.478075 28891 solver.cpp:253]     Train net output #0: loss = 7.8859 (* 1 = 7.8859 loss)
I0402 07:44:17.478085 28891 sgd_solver.cpp:106] Iteration 20, lr = 0.04
I0402 07:44:18.429299 28891 solver.cpp:237] Iteration 40, loss = 8.6089
I0402 07:44:18.429337 28891 solver.cpp:253]     Train net output #0: loss = 8.6089 (* 1 = 8.6089 loss)
I0402 07:44:18.429345 28891 sgd_solver.cpp:106] Iteration 40, lr = 0.04
I0402 07:44:19.380623 28891 solver.cpp:237] Iteration 60, loss = 9.07562
I0402 07:44:19.380722 28891 solver.cpp:253]     Train net output #0: loss = 9.07562 (* 1 = 9.07562 loss)
I0402 07:44:19.380753 28891 sgd_solver.cpp:106] Iteration 60, lr = 0.04
I0402 07:44:20.323770 28891 solver.cpp:237] Iteration 80, loss = 9.53838
I0402 07:44:20.323864 28891 solver.cpp:253]     Train net output #0: loss = 9.53838 (* 1 = 9.53838 loss)
I0402 07:44:20.323894 28891 sgd_solver.cpp:106] Iteration 80, lr = 0.04
I0402 07:44:21.279635 28891 solver.cpp:237] Iteration 100, loss = 9.28444
I0402 07:44:21.279726 28891 solver.cpp:253]     Train net output #0: loss = 9.28444 (* 1 = 9.28444 loss)
I0402 07:44:21.279749 28891 sgd_solver.cpp:106] Iteration 100, lr = 0.04
I0402 07:44:22.234582 28891 solver.cpp:237] Iteration 120, loss = 8.98317
I0402 07:44:22.234673 28891 solver.cpp:253]     Train net output #0: loss = 8.98317 (* 1 = 8.98317 loss)
I0402 07:44:22.234696 28891 sgd_solver.cpp:106] Iteration 120, lr = 0.04
I0402 07:44:23.178378 28891 solver.cpp:237] Iteration 140, loss = 9.97474
I0402 07:44:23.178481 28891 solver.cpp:253]     Train net output #0: loss = 9.97474 (* 1 = 9.97474 loss)
I0402 07:44:23.178519 28891 sgd_solver.cpp:106] Iteration 140, lr = 0.04
I0402 07:44:24.131913 28891 solver.cpp:237] Iteration 160, loss = 9.74935
I0402 07:44:24.131999 28891 solver.cpp:253]     Train net output #0: loss = 9.74935 (* 1 = 9.74935 loss)
I0402 07:44:24.132021 28891 sgd_solver.cpp:106] Iteration 160, lr = 0.04
I0402 07:44:25.091462 28891 solver.cpp:237] Iteration 180, loss = 10.035
I0402 07:44:25.091548 28891 solver.cpp:253]     Train net output #0: loss = 10.035 (* 1 = 10.035 loss)
I0402 07:44:25.091572 28891 sgd_solver.cpp:106] Iteration 180, lr = 0.04
I0402 07:44:26.035631 28891 solver.cpp:237] Iteration 200, loss = 10.706
I0402 07:44:26.035748 28891 solver.cpp:253]     Train net output #0: loss = 10.706 (* 1 = 10.706 loss)
I0402 07:44:26.035773 28891 sgd_solver.cpp:106] Iteration 200, lr = 0.04
I0402 07:44:26.989758 28891 solver.cpp:237] Iteration 220, loss = 9.71378
I0402 07:44:26.989843 28891 solver.cpp:253]     Train net output #0: loss = 9.71378 (* 1 = 9.71378 loss)
I0402 07:44:26.989866 28891 sgd_solver.cpp:106] Iteration 220, lr = 0.04
I0402 07:44:27.932667 28891 solver.cpp:237] Iteration 240, loss = 11.3165
I0402 07:44:27.932763 28891 solver.cpp:253]     Train net output #0: loss = 11.3165 (* 1 = 11.3165 loss)
I0402 07:44:27.932776 28891 sgd_solver.cpp:106] Iteration 240, lr = 0.04
I0402 07:44:28.886863 28891 solver.cpp:237] Iteration 260, loss = 9.74699
I0402 07:44:28.886952 28891 solver.cpp:253]     Train net output #0: loss = 9.74699 (* 1 = 9.74699 loss)
I0402 07:44:28.886975 28891 sgd_solver.cpp:106] Iteration 260, lr = 0.04
I0402 07:44:29.840445 28891 solver.cpp:237] Iteration 280, loss = 10.848
I0402 07:44:29.840541 28891 solver.cpp:253]     Train net output #0: loss = 10.848 (* 1 = 10.848 loss)
I0402 07:44:29.840564 28891 sgd_solver.cpp:106] Iteration 280, lr = 0.04
I0402 07:44:30.932956 28891 solver.cpp:237] Iteration 300, loss = 11.715
I0402 07:44:30.933049 28891 solver.cpp:253]     Train net output #0: loss = 11.715 (* 1 = 11.715 loss)
I0402 07:44:30.933104 28891 sgd_solver.cpp:106] Iteration 300, lr = 0.04
I0402 07:44:32.221592 28891 solver.cpp:237] Iteration 320, loss = 11.3803
I0402 07:44:32.221688 28891 solver.cpp:253]     Train net output #0: loss = 11.3803 (* 1 = 11.3803 loss)
I0402 07:44:32.221711 28891 sgd_solver.cpp:106] Iteration 320, lr = 0.04
I0402 07:44:33.513310 28891 solver.cpp:237] Iteration 340, loss = 11.3881
I0402 07:44:33.513407 28891 solver.cpp:253]     Train net output #0: loss = 11.3881 (* 1 = 11.3881 loss)
I0402 07:44:33.513434 28891 sgd_solver.cpp:106] Iteration 340, lr = 0.04
I0402 07:44:34.815096 28891 solver.cpp:237] Iteration 360, loss = 11.4421
I0402 07:44:34.815131 28891 solver.cpp:253]     Train net output #0: loss = 11.4421 (* 1 = 11.4421 loss)
I0402 07:44:34.815143 28891 sgd_solver.cpp:106] Iteration 360, lr = 0.04
I0402 07:44:36.105285 28891 solver.cpp:237] Iteration 380, loss = 12.1831
I0402 07:44:36.105384 28891 solver.cpp:253]     Train net output #0: loss = 12.1831 (* 1 = 12.1831 loss)
I0402 07:44:36.105408 28891 sgd_solver.cpp:106] Iteration 380, lr = 0.04
I0402 07:44:37.389309 28891 solver.cpp:237] Iteration 400, loss = 12.2301
I0402 07:44:37.389430 28891 solver.cpp:253]     Train net output #0: loss = 12.2301 (* 1 = 12.2301 loss)
I0402 07:44:37.389454 28891 sgd_solver.cpp:106] Iteration 400, lr = 0.04
I0402 07:44:38.677789 28891 solver.cpp:237] Iteration 420, loss = 12.167
I0402 07:44:38.677884 28891 solver.cpp:253]     Train net output #0: loss = 12.167 (* 1 = 12.167 loss)
I0402 07:44:38.677911 28891 sgd_solver.cpp:106] Iteration 420, lr = 0.04
I0402 07:44:39.956972 28891 solver.cpp:237] Iteration 440, loss = 10.441
I0402 07:44:39.957062 28891 solver.cpp:253]     Train net output #0: loss = 10.441 (* 1 = 10.441 loss)
I0402 07:44:39.957087 28891 sgd_solver.cpp:106] Iteration 440, lr = 0.04
I0402 07:44:41.239035 28891 solver.cpp:237] Iteration 460, loss = 11.256
I0402 07:44:41.239074 28891 solver.cpp:253]     Train net output #0: loss = 11.256 (* 1 = 11.256 loss)
I0402 07:44:41.239084 28891 sgd_solver.cpp:106] Iteration 460, lr = 0.04
I0402 07:44:42.526944 28891 solver.cpp:237] Iteration 480, loss = 11.5119
I0402 07:44:42.527040 28891 solver.cpp:253]     Train net output #0: loss = 11.5119 (* 1 = 11.5119 loss)
I0402 07:44:42.527073 28891 sgd_solver.cpp:106] Iteration 480, lr = 0.04
I0402 07:44:43.834096 28891 solver.cpp:237] Iteration 500, loss = 12.6471
I0402 07:44:43.834136 28891 solver.cpp:253]     Train net output #0: loss = 12.6471 (* 1 = 12.6471 loss)
I0402 07:44:43.834146 28891 sgd_solver.cpp:106] Iteration 500, lr = 0.04
I0402 07:44:45.127045 28891 solver.cpp:237] Iteration 520, loss = 12.275
I0402 07:44:45.127084 28891 solver.cpp:253]     Train net output #0: loss = 12.275 (* 1 = 12.275 loss)
I0402 07:44:45.127094 28891 sgd_solver.cpp:106] Iteration 520, lr = 0.04
I0402 07:44:46.408753 28891 solver.cpp:237] Iteration 540, loss = 11.8241
I0402 07:44:46.408846 28891 solver.cpp:253]     Train net output #0: loss = 11.8241 (* 1 = 11.8241 loss)
I0402 07:44:46.408879 28891 sgd_solver.cpp:106] Iteration 540, lr = 0.04
I0402 07:44:47.698861 28891 solver.cpp:237] Iteration 560, loss = 10.5248
I0402 07:44:47.698963 28891 solver.cpp:253]     Train net output #0: loss = 10.5248 (* 1 = 10.5248 loss)
I0402 07:44:47.698976 28891 sgd_solver.cpp:106] Iteration 560, lr = 0.04
I0402 07:44:48.975056 28891 solver.cpp:237] Iteration 580, loss = 11.6691
I0402 07:44:48.975152 28891 solver.cpp:253]     Train net output #0: loss = 11.6691 (* 1 = 11.6691 loss)
I0402 07:44:48.975188 28891 sgd_solver.cpp:106] Iteration 580, lr = 0.04
I0402 07:44:50.266737 28891 solver.cpp:237] Iteration 600, loss = 12.4001
I0402 07:44:50.266829 28891 solver.cpp:253]     Train net output #0: loss = 12.4001 (* 1 = 12.4001 loss)
I0402 07:44:50.266861 28891 sgd_solver.cpp:106] Iteration 600, lr = 0.04
I0402 07:44:51.558058 28891 solver.cpp:237] Iteration 620, loss = 11.2511
I0402 07:44:51.558148 28891 solver.cpp:253]     Train net output #0: loss = 11.2511 (* 1 = 11.2511 loss)
I0402 07:44:51.558182 28891 sgd_solver.cpp:106] Iteration 620, lr = 0.04
I0402 07:44:52.863309 28891 solver.cpp:237] Iteration 640, loss = 11.559
I0402 07:44:52.863395 28891 solver.cpp:253]     Train net output #0: loss = 11.559 (* 1 = 11.559 loss)
I0402 07:44:52.863417 28891 sgd_solver.cpp:106] Iteration 640, lr = 0.04
I0402 07:44:54.148263 28891 solver.cpp:237] Iteration 660, loss = 12.0991
I0402 07:44:54.148351 28891 solver.cpp:253]     Train net output #0: loss = 12.0991 (* 1 = 12.0991 loss)
I0402 07:44:54.148375 28891 sgd_solver.cpp:106] Iteration 660, lr = 0.04
I0402 07:44:55.444720 28891 solver.cpp:237] Iteration 680, loss = 11.9108
I0402 07:44:55.444808 28891 solver.cpp:253]     Train net output #0: loss = 11.9108 (* 1 = 11.9108 loss)
I0402 07:44:55.444831 28891 sgd_solver.cpp:106] Iteration 680, lr = 0.04
I0402 07:44:56.745741 28891 solver.cpp:237] Iteration 700, loss = 11.094
I0402 07:44:56.745836 28891 solver.cpp:253]     Train net output #0: loss = 11.094 (* 1 = 11.094 loss)
I0402 07:44:56.745868 28891 sgd_solver.cpp:106] Iteration 700, lr = 0.04
I0402 07:44:58.029386 28891 solver.cpp:237] Iteration 720, loss = 10.9928
I0402 07:44:58.029474 28891 solver.cpp:253]     Train net output #0: loss = 10.9928 (* 1 = 10.9928 loss)
I0402 07:44:58.029520 28891 sgd_solver.cpp:106] Iteration 720, lr = 0.04
I0402 07:44:59.330014 28891 solver.cpp:237] Iteration 740, loss = 11.6309
I0402 07:44:59.330121 28891 solver.cpp:253]     Train net output #0: loss = 11.6309 (* 1 = 11.6309 loss)
I0402 07:44:59.330158 28891 sgd_solver.cpp:106] Iteration 740, lr = 0.04
I0402 07:45:00.624333 28891 solver.cpp:237] Iteration 760, loss = 13.3636
I0402 07:45:00.624421 28891 solver.cpp:253]     Train net output #0: loss = 13.3636 (* 1 = 13.3636 loss)
I0402 07:45:00.624444 28891 sgd_solver.cpp:106] Iteration 760, lr = 0.04
I0402 07:45:01.916509 28891 solver.cpp:237] Iteration 780, loss = 13.9942
I0402 07:45:01.916544 28891 solver.cpp:253]     Train net output #0: loss = 13.9942 (* 1 = 13.9942 loss)
I0402 07:45:01.916553 28891 sgd_solver.cpp:106] Iteration 780, lr = 0.04
I0402 07:45:03.203392 28891 solver.cpp:237] Iteration 800, loss = 12.2917
I0402 07:45:03.203492 28891 solver.cpp:253]     Train net output #0: loss = 12.2917 (* 1 = 12.2917 loss)
I0402 07:45:03.203524 28891 sgd_solver.cpp:106] Iteration 800, lr = 0.04
I0402 07:45:04.500473 28891 solver.cpp:237] Iteration 820, loss = 11.6009
I0402 07:45:04.500562 28891 solver.cpp:253]     Train net output #0: loss = 11.6009 (* 1 = 11.6009 loss)
I0402 07:45:04.500572 28891 sgd_solver.cpp:106] Iteration 820, lr = 0.04
I0402 07:45:05.793123 28891 solver.cpp:237] Iteration 840, loss = 11.4827
I0402 07:45:05.793216 28891 solver.cpp:253]     Train net output #0: loss = 11.4827 (* 1 = 11.4827 loss)
I0402 07:45:05.793241 28891 sgd_solver.cpp:106] Iteration 840, lr = 0.04
I0402 07:45:07.091801 28891 solver.cpp:237] Iteration 860, loss = 12.9861
I0402 07:45:07.091889 28891 solver.cpp:253]     Train net output #0: loss = 12.9861 (* 1 = 12.9861 loss)
I0402 07:45:07.091912 28891 sgd_solver.cpp:106] Iteration 860, lr = 0.04
I0402 07:45:08.368911 28891 solver.cpp:237] Iteration 880, loss = 13.7044
I0402 07:45:08.369009 28891 solver.cpp:253]     Train net output #0: loss = 13.7044 (* 1 = 13.7044 loss)
I0402 07:45:08.369071 28891 sgd_solver.cpp:106] Iteration 880, lr = 0.04
I0402 07:45:09.655941 28891 solver.cpp:237] Iteration 900, loss = 12.5252
I0402 07:45:09.656026 28891 solver.cpp:253]     Train net output #0: loss = 12.5252 (* 1 = 12.5252 loss)
I0402 07:45:09.656050 28891 sgd_solver.cpp:106] Iteration 900, lr = 0.04
I0402 07:45:10.952203 28891 solver.cpp:237] Iteration 920, loss = 12.0437
I0402 07:45:10.952289 28891 solver.cpp:253]     Train net output #0: loss = 12.0437 (* 1 = 12.0437 loss)
I0402 07:45:10.952311 28891 sgd_solver.cpp:106] Iteration 920, lr = 0.04
I0402 07:45:12.240433 28891 solver.cpp:237] Iteration 940, loss = 13.1246
I0402 07:45:12.240469 28891 solver.cpp:253]     Train net output #0: loss = 13.1246 (* 1 = 13.1246 loss)
I0402 07:45:12.240479 28891 sgd_solver.cpp:106] Iteration 940, lr = 0.04
I0402 07:45:13.539783 28891 solver.cpp:237] Iteration 960, loss = 12.685
I0402 07:45:13.539882 28891 solver.cpp:253]     Train net output #0: loss = 12.685 (* 1 = 12.685 loss)
I0402 07:45:13.539916 28891 sgd_solver.cpp:106] Iteration 960, lr = 0.04
I0402 07:45:14.836242 28891 solver.cpp:237] Iteration 980, loss = 14.5904
I0402 07:45:14.836318 28891 solver.cpp:253]     Train net output #0: loss = 14.5904 (* 1 = 14.5904 loss)
I0402 07:45:14.836328 28891 sgd_solver.cpp:106] Iteration 980, lr = 0.04
I0402 07:45:15.820065 28891 solver.cpp:341] Iteration 1000, Testing net (#0)
I0402 07:45:40.182909 28891 blocking_queue.cpp:50] Data layer prefetch queue empty
I0402 07:45:48.229744 28891 solver.cpp:409]     Test net output #0: accuracy = 0.0015
I0402 07:45:48.229977 28891 solver.cpp:409]     Test net output #1: loss = 10.6006 (* 1 = 10.6006 loss)
I0402 07:45:48.255380 28891 solver.cpp:237] Iteration 1000, loss = 11.9998
I0402 07:45:48.255630 28891 solver.cpp:253]     Train net output #0: loss = 11.9998 (* 1 = 11.9998 loss)
I0402 07:45:48.255719 28891 sgd_solver.cpp:106] Iteration 1000, lr = 0.04
I0402 07:45:49.539083 28891 solver.cpp:237] Iteration 1020, loss = 11.9434
I0402 07:45:49.539166 28891 solver.cpp:253]     Train net output #0: loss = 11.9434 (* 1 = 11.9434 loss)
I0402 07:45:49.539201 28891 sgd_solver.cpp:106] Iteration 1020, lr = 0.04
I0402 07:45:50.818457 28891 solver.cpp:237] Iteration 1040, loss = 12.7079
I0402 07:45:50.818541 28891 solver.cpp:253]     Train net output #0: loss = 12.7079 (* 1 = 12.7079 loss)
I0402 07:45:50.818568 28891 sgd_solver.cpp:106] Iteration 1040, lr = 0.04
I0402 07:45:52.107283 28891 solver.cpp:237] Iteration 1060, loss = 12.6245
I0402 07:45:52.107380 28891 solver.cpp:253]     Train net output #0: loss = 12.6245 (* 1 = 12.6245 loss)
I0402 07:45:52.107412 28891 sgd_solver.cpp:106] Iteration 1060, lr = 0.04
I0402 07:45:53.380975 28891 solver.cpp:237] Iteration 1080, loss = 12.5354
I0402 07:45:53.381057 28891 solver.cpp:253]     Train net output #0: loss = 12.5354 (* 1 = 12.5354 loss)
I0402 07:45:53.381083 28891 sgd_solver.cpp:106] Iteration 1080, lr = 0.04
I0402 07:45:54.676625 28891 solver.cpp:237] Iteration 1100, loss = 13.1555
I0402 07:45:54.676717 28891 solver.cpp:253]     Train net output #0: loss = 13.1555 (* 1 = 13.1555 loss)
I0402 07:45:54.676743 28891 sgd_solver.cpp:106] Iteration 1100, lr = 0.04
I0402 07:45:55.968046 28891 solver.cpp:237] Iteration 1120, loss = 13.1082
I0402 07:45:55.968135 28891 solver.cpp:253]     Train net output #0: loss = 13.1082 (* 1 = 13.1082 loss)
I0402 07:45:55.968158 28891 sgd_solver.cpp:106] Iteration 1120, lr = 0.04
I0402 07:45:57.254488 28891 solver.cpp:237] Iteration 1140, loss = 12.0962
I0402 07:45:57.254573 28891 solver.cpp:253]     Train net output #0: loss = 12.0962 (* 1 = 12.0962 loss)
I0402 07:45:57.254596 28891 sgd_solver.cpp:106] Iteration 1140, lr = 0.04
I0402 07:45:58.542110 28891 solver.cpp:237] Iteration 1160, loss = 13.3029
I0402 07:45:58.542192 28891 solver.cpp:253]     Train net output #0: loss = 13.3029 (* 1 = 13.3029 loss)
I0402 07:45:58.542215 28891 sgd_solver.cpp:106] Iteration 1160, lr = 0.04
I0402 07:45:59.839242 28891 solver.cpp:237] Iteration 1180, loss = 11.9282
I0402 07:45:59.839328 28891 solver.cpp:253]     Train net output #0: loss = 11.9282 (* 1 = 11.9282 loss)
I0402 07:45:59.839350 28891 sgd_solver.cpp:106] Iteration 1180, lr = 0.04
I0402 07:46:01.133821 28891 solver.cpp:237] Iteration 1200, loss = 12.4179
I0402 07:46:01.133906 28891 solver.cpp:253]     Train net output #0: loss = 12.4179 (* 1 = 12.4179 loss)
I0402 07:46:01.133929 28891 sgd_solver.cpp:106] Iteration 1200, lr = 0.04
*** Aborted at 1459572361 (unix time) try "date -d @1459572361" if you are using GNU date ***
PC: @     0x7fff90910a52 (unknown)
*** SIGTERM (@0x3e800006dc4) received by PID 28891 (TID 0x7f756b8deac0) from PID 28100; stack trace: ***
    @     0x7f7569acad40 (unknown)
    @     0x7fff90910a52 (unknown)
    @     0x7f7569b9c92d (unknown)
    @     0x7f75433761de (unknown)
    @     0x7f7542d2b7ab (unknown)
    @     0x7f7542d08e33 (unknown)
    @     0x7f7542d00b71 (unknown)
    @     0x7f7542d018c6 (unknown)
    @     0x7f7542c6f3e2 (unknown)
    @     0x7f7542c6f53a (unknown)
    @     0x7f7542c53005 (unknown)
    @     0x7f756a9fd482 (unknown)
    @     0x7f756a9dfdb1 (unknown)
    @     0x7f756aa039b8 (unknown)
    @     0x7f756af72c8e caffe::caffe_gpu_memcpy()
    @     0x7f756af3603e caffe::SyncedMemory::to_gpu()
    @     0x7f756af354e9 caffe::SyncedMemory::gpu_data()
    @     0x7f756ae10052 caffe::Blob<>::gpu_data()
    @     0x7f756af927ac caffe::BasePrefetchingDataLayer<>::Forward_gpu()
    @     0x7f756af4ed51 caffe::Net<>::ForwardFromTo()
    @     0x7f756af4f0c7 caffe::Net<>::ForwardPrefilled()
    @     0x7f756af67a91 caffe::Solver<>::Step()
    @     0x7f756af68665 caffe::Solver<>::Solve()
    @           0x4081f7 train()
    @           0x405b11 main
    @     0x7f7569ab5ec5 (unknown)
    @           0x406221 (unknown)
    @                0x0 (unknown)
