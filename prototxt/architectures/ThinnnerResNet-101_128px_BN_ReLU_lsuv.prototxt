test_initialization: true
test_iter: 200
test_interval: 1000
base_lr: 0.01
lr_policy: "step"
gamma: 0.1
stepsize: 100000
display: 20
max_iter: 320000
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
iter_size: 1
solver_mode: GPU
solver_type: SGD
snapshot_prefix: "/media/old-ufo/MyBook/caffe-train-snashots/imagenet128/ThinResNet128_lsuv"

net_param { 
name: "ThinnerResNet-101"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
 transform_param {
    force_color: true
    scale: 0.04
    mirror: true
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 124
    resize_param {
      prob: 1.0
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: CUBIC
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/old-ufo/datasets/imagenet/ilsvrc12_train_shuffle_lmdb"
    backend: LMDB
    batch_size: 256
  }
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    force_color: true
    scale: 0.04
    mirror: false
    crop_size: 128
    mean_value: 104
    mean_value: 117
    mean_value: 123
    resize_param {
      prob: 1.0
      resize_mode: FIT_SMALL_SIZE
      height: 144
      width: 144
      pad_mode: MIRRORED
      pad_value: 104
      pad_value: 117
      pad_value: 124
      interp_mode: CUBIC
      center_object: false
      max_angle: 0
    }
  }
  data_param {
    source: "/home/old-ufo/datasets/imagenet/ilsvrc12_val_lmdb"    
    backend: LMDB
    batch_size: 250
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv0/nl"
  type: "ReLU"
  bottom: "conv0"
  top: "conv0"
}

layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "stage0/block0/conv_reduce"
  type: "Convolution"
  bottom: "pool0"
  top: "stage0/block0/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 36
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage0/block0/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage0/block0/conv_reduce"
  top: "stage0/block0/conv_reduce"
}
layer {
  name: "stage0/block0/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage0/block0/conv_reduce"
  top: "stage0/block0/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block0/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage0/block0/conv_reduce"
  top: "stage0/block0/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block0/conv"
  type: "Convolution"
  bottom: "stage0/block0/conv_reduce/BN"
  top: "stage0/block0/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 36
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage0/block0/conv/nl"
  type: "ReLU"
  bottom: "stage0/block0/conv"
  top: "stage0/block0/conv"
}
layer {
  name: "stage0/block0/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage0/block0/conv"
  top: "stage0/block0/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block0/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage0/block0/conv"
  top: "stage0/block0/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block0/conv_restore"
  type: "Convolution"
  bottom: "stage0/block0/conv/BN"
  top: "stage0/block0/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 120
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage0/block0/shortcut"
  type: "Convolution"
  bottom: "pool0"
  top: "stage0/block0/shortcut"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 120
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage0/block0/sum"
  type: "Eltwise"
  bottom: "stage0/block0/conv_restore"
  bottom: "stage0/block0/shortcut"
  top: "stage0/block0/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage0/block0/sum/nl"
  type: "ReLU"
  bottom: "stage0/block0/sum"
  top: "stage0/block0/sum"
}
layer {
  name: "stage0/block0/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage0/block0/sum"
  top: "stage0/block0/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block0/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage0/block0/sum"
  top: "stage0/block0/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block1/conv_reduce"
  type: "Convolution"
  bottom: "stage0/block0/sum/BN"
  top: "stage0/block1/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 36
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage0/block1/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage0/block1/conv_reduce"
  top: "stage0/block1/conv_reduce"
}
layer {
  name: "stage0/block1/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage0/block1/conv_reduce"
  top: "stage0/block1/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block1/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage0/block1/conv_reduce"
  top: "stage0/block1/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block1/conv"
  type: "Convolution"
  bottom: "stage0/block1/conv_reduce/BN"
  top: "stage0/block1/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 36
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage0/block1/conv/nl"
  type: "ReLU"
  bottom: "stage0/block1/conv"
  top: "stage0/block1/conv"
}
layer {
  name: "stage0/block1/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage0/block1/conv"
  top: "stage0/block1/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block1/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage0/block1/conv"
  top: "stage0/block1/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block1/conv_restore"
  type: "Convolution"
  bottom: "stage0/block1/conv/BN"
  top: "stage0/block1/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 120
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage0/block1/sum"
  type: "Eltwise"
  bottom: "stage0/block1/conv_restore"
  bottom: "stage0/block0/sum/BN"
  top: "stage0/block1/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage0/block1/sum/nl"
  type: "ReLU"
  bottom: "stage0/block1/sum"
  top: "stage0/block1/sum"
}
layer {
  name: "stage0/block1/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage0/block1/sum"
  top: "stage0/block1/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block1/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage0/block1/sum"
  top: "stage0/block1/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block2/conv_reduce"
  type: "Convolution"
  bottom: "stage0/block1/sum/BN"
  top: "stage0/block2/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 36
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage0/block2/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage0/block2/conv_reduce"
  top: "stage0/block2/conv_reduce"
}
layer {
  name: "stage0/block2/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage0/block2/conv_reduce"
  top: "stage0/block2/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block2/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage0/block2/conv_reduce"
  top: "stage0/block2/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block2/conv"
  type: "Convolution"
  bottom: "stage0/block2/conv_reduce/BN"
  top: "stage0/block2/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 36
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage0/block2/conv/nl"
  type: "ReLU"
  bottom: "stage0/block2/conv"
  top: "stage0/block2/conv"
}
layer {
  name: "stage0/block2/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage0/block2/conv"
  top: "stage0/block2/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block2/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage0/block2/conv"
  top: "stage0/block2/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block2/conv_restore"
  type: "Convolution"
  bottom: "stage0/block2/conv/BN"
  top: "stage0/block2/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 120
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage0/block2/sum"
  type: "Eltwise"
  bottom: "stage0/block2/conv_restore"
  bottom: "stage0/block1/sum/BN"
  top: "stage0/block2/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage0/block2/sum/nl"
  type: "ReLU"
  bottom: "stage0/block2/sum"
  top: "stage0/block2/sum"
}
layer {
  name: "stage0/block2/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage0/block2/sum"
  top: "stage0/block2/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage0/block2/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage0/block2/sum"
  top: "stage0/block2/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block0/conv_reduce"
  type: "Convolution"
  bottom: "stage0/block2/sum/BN"
  top: "stage1/block0/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage1/block0/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage1/block0/conv_reduce"
  top: "stage1/block0/conv_reduce"
}
layer {
  name: "stage1/block0/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage1/block0/conv_reduce"
  top: "stage1/block0/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block0/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage1/block0/conv_reduce"
  top: "stage1/block0/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block0/conv"
  type: "Convolution"
  bottom: "stage1/block0/conv_reduce/BN"
  top: "stage1/block0/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    kernel_size: 3
    stride: 2
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage1/block0/conv/nl"
  type: "ReLU"
  bottom: "stage1/block0/conv"
  top: "stage1/block0/conv"
}
layer {
  name: "stage1/block0/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage1/block0/conv"
  top: "stage1/block0/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block0/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage1/block0/conv"
  top: "stage1/block0/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block0/conv_restore"
  type: "Convolution"
  bottom: "stage1/block0/conv/BN"
  top: "stage1/block0/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 240
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage1/block0/shortcut"
  type: "Convolution"
  bottom: "stage0/block2/sum/BN"
  top: "stage1/block0/shortcut"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 240
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage1/block0/sum"
  type: "Eltwise"
  bottom: "stage1/block0/conv_restore"
  bottom: "stage1/block0/shortcut"
  top: "stage1/block0/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage1/block0/sum/nl"
  type: "ReLU"
  bottom: "stage1/block0/sum"
  top: "stage1/block0/sum"
}
layer {
  name: "stage1/block0/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage1/block0/sum"
  top: "stage1/block0/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block0/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage1/block0/sum"
  top: "stage1/block0/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block1/conv_reduce"
  type: "Convolution"
  bottom: "stage1/block0/sum/BN"
  top: "stage1/block1/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage1/block1/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage1/block1/conv_reduce"
  top: "stage1/block1/conv_reduce"
}
layer {
  name: "stage1/block1/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage1/block1/conv_reduce"
  top: "stage1/block1/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block1/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage1/block1/conv_reduce"
  top: "stage1/block1/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block1/conv"
  type: "Convolution"
  bottom: "stage1/block1/conv_reduce/BN"
  top: "stage1/block1/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage1/block1/conv/nl"
  type: "ReLU"
  bottom: "stage1/block1/conv"
  top: "stage1/block1/conv"
}
layer {
  name: "stage1/block1/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage1/block1/conv"
  top: "stage1/block1/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block1/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage1/block1/conv"
  top: "stage1/block1/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block1/conv_restore"
  type: "Convolution"
  bottom: "stage1/block1/conv/BN"
  top: "stage1/block1/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 240
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage1/block1/sum"
  type: "Eltwise"
  bottom: "stage1/block1/conv_restore"
  bottom: "stage1/block0/sum/BN"
  top: "stage1/block1/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage1/block1/sum/nl"
  type: "ReLU"
  bottom: "stage1/block1/sum"
  top: "stage1/block1/sum"
}
layer {
  name: "stage1/block1/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage1/block1/sum"
  top: "stage1/block1/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block1/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage1/block1/sum"
  top: "stage1/block1/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block2/conv_reduce"
  type: "Convolution"
  bottom: "stage1/block1/sum/BN"
  top: "stage1/block2/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage1/block2/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage1/block2/conv_reduce"
  top: "stage1/block2/conv_reduce"
}
layer {
  name: "stage1/block2/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage1/block2/conv_reduce"
  top: "stage1/block2/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block2/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage1/block2/conv_reduce"
  top: "stage1/block2/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block2/conv"
  type: "Convolution"
  bottom: "stage1/block2/conv_reduce/BN"
  top: "stage1/block2/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage1/block2/conv/nl"
  type: "ReLU"
  bottom: "stage1/block2/conv"
  top: "stage1/block2/conv"
}
layer {
  name: "stage1/block2/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage1/block2/conv"
  top: "stage1/block2/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block2/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage1/block2/conv"
  top: "stage1/block2/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block2/conv_restore"
  type: "Convolution"
  bottom: "stage1/block2/conv/BN"
  top: "stage1/block2/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 240
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage1/block2/sum"
  type: "Eltwise"
  bottom: "stage1/block2/conv_restore"
  bottom: "stage1/block1/sum/BN"
  top: "stage1/block2/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage1/block2/sum/nl"
  type: "ReLU"
  bottom: "stage1/block2/sum"
  top: "stage1/block2/sum"
}
layer {
  name: "stage1/block2/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage1/block2/sum"
  top: "stage1/block2/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block2/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage1/block2/sum"
  top: "stage1/block2/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block3/conv_reduce"
  type: "Convolution"
  bottom: "stage1/block2/sum/BN"
  top: "stage1/block3/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage1/block3/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage1/block3/conv_reduce"
  top: "stage1/block3/conv_reduce"
}
layer {
  name: "stage1/block3/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage1/block3/conv_reduce"
  top: "stage1/block3/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block3/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage1/block3/conv_reduce"
  top: "stage1/block3/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block3/conv"
  type: "Convolution"
  bottom: "stage1/block3/conv_reduce/BN"
  top: "stage1/block3/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 72
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage1/block3/conv/nl"
  type: "ReLU"
  bottom: "stage1/block3/conv"
  top: "stage1/block3/conv"
}
layer {
  name: "stage1/block3/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage1/block3/conv"
  top: "stage1/block3/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block3/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage1/block3/conv"
  top: "stage1/block3/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block3/conv_restore"
  type: "Convolution"
  bottom: "stage1/block3/conv/BN"
  top: "stage1/block3/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 240
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage1/block3/sum"
  type: "Eltwise"
  bottom: "stage1/block3/conv_restore"
  bottom: "stage1/block2/sum/BN"
  top: "stage1/block3/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage1/block3/sum/nl"
  type: "ReLU"
  bottom: "stage1/block3/sum"
  top: "stage1/block3/sum"
}
layer {
  name: "stage1/block3/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage1/block3/sum"
  top: "stage1/block3/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage1/block3/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage1/block3/sum"
  top: "stage1/block3/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block0/conv_reduce"
  type: "Convolution"
  bottom: "stage1/block3/sum/BN"
  top: "stage2/block0/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block0/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block0/conv_reduce"
  top: "stage2/block0/conv_reduce"
}
layer {
  name: "stage2/block0/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block0/conv_reduce"
  top: "stage2/block0/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block0/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block0/conv_reduce"
  top: "stage2/block0/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block0/conv"
  type: "Convolution"
  bottom: "stage2/block0/conv_reduce/BN"
  top: "stage2/block0/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 2
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block0/conv/nl"
  type: "ReLU"
  bottom: "stage2/block0/conv"
  top: "stage2/block0/conv"
}
layer {
  name: "stage2/block0/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block0/conv"
  top: "stage2/block0/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block0/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block0/conv"
  top: "stage2/block0/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block0/conv_restore"
  type: "Convolution"
  bottom: "stage2/block0/conv/BN"
  top: "stage2/block0/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block0/shortcut"
  type: "Convolution"
  bottom: "stage1/block3/sum/BN"
  top: "stage2/block0/shortcut"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block0/sum"
  type: "Eltwise"
  bottom: "stage2/block0/conv_restore"
  bottom: "stage2/block0/shortcut"
  top: "stage2/block0/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block0/sum/nl"
  type: "ReLU"
  bottom: "stage2/block0/sum"
  top: "stage2/block0/sum"
}
layer {
  name: "stage2/block0/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block0/sum"
  top: "stage2/block0/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block0/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block0/sum"
  top: "stage2/block0/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block1/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block0/sum/BN"
  top: "stage2/block1/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block1/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block1/conv_reduce"
  top: "stage2/block1/conv_reduce"
}
layer {
  name: "stage2/block1/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block1/conv_reduce"
  top: "stage2/block1/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block1/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block1/conv_reduce"
  top: "stage2/block1/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block1/conv"
  type: "Convolution"
  bottom: "stage2/block1/conv_reduce/BN"
  top: "stage2/block1/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block1/conv/nl"
  type: "ReLU"
  bottom: "stage2/block1/conv"
  top: "stage2/block1/conv"
}
layer {
  name: "stage2/block1/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block1/conv"
  top: "stage2/block1/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block1/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block1/conv"
  top: "stage2/block1/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block1/conv_restore"
  type: "Convolution"
  bottom: "stage2/block1/conv/BN"
  top: "stage2/block1/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block1/sum"
  type: "Eltwise"
  bottom: "stage2/block1/conv_restore"
  bottom: "stage2/block0/sum/BN"
  top: "stage2/block1/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block1/sum/nl"
  type: "ReLU"
  bottom: "stage2/block1/sum"
  top: "stage2/block1/sum"
}
layer {
  name: "stage2/block1/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block1/sum"
  top: "stage2/block1/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block1/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block1/sum"
  top: "stage2/block1/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block2/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block1/sum/BN"
  top: "stage2/block2/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block2/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block2/conv_reduce"
  top: "stage2/block2/conv_reduce"
}
layer {
  name: "stage2/block2/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block2/conv_reduce"
  top: "stage2/block2/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block2/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block2/conv_reduce"
  top: "stage2/block2/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block2/conv"
  type: "Convolution"
  bottom: "stage2/block2/conv_reduce/BN"
  top: "stage2/block2/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block2/conv/nl"
  type: "ReLU"
  bottom: "stage2/block2/conv"
  top: "stage2/block2/conv"
}
layer {
  name: "stage2/block2/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block2/conv"
  top: "stage2/block2/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block2/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block2/conv"
  top: "stage2/block2/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block2/conv_restore"
  type: "Convolution"
  bottom: "stage2/block2/conv/BN"
  top: "stage2/block2/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block2/sum"
  type: "Eltwise"
  bottom: "stage2/block2/conv_restore"
  bottom: "stage2/block1/sum/BN"
  top: "stage2/block2/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block2/sum/nl"
  type: "ReLU"
  bottom: "stage2/block2/sum"
  top: "stage2/block2/sum"
}
layer {
  name: "stage2/block2/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block2/sum"
  top: "stage2/block2/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block2/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block2/sum"
  top: "stage2/block2/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block3/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block2/sum/BN"
  top: "stage2/block3/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block3/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block3/conv_reduce"
  top: "stage2/block3/conv_reduce"
}
layer {
  name: "stage2/block3/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block3/conv_reduce"
  top: "stage2/block3/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block3/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block3/conv_reduce"
  top: "stage2/block3/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block3/conv"
  type: "Convolution"
  bottom: "stage2/block3/conv_reduce/BN"
  top: "stage2/block3/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block3/conv/nl"
  type: "ReLU"
  bottom: "stage2/block3/conv"
  top: "stage2/block3/conv"
}
layer {
  name: "stage2/block3/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block3/conv"
  top: "stage2/block3/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block3/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block3/conv"
  top: "stage2/block3/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block3/conv_restore"
  type: "Convolution"
  bottom: "stage2/block3/conv/BN"
  top: "stage2/block3/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block3/sum"
  type: "Eltwise"
  bottom: "stage2/block3/conv_restore"
  bottom: "stage2/block2/sum/BN"
  top: "stage2/block3/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block3/sum/nl"
  type: "ReLU"
  bottom: "stage2/block3/sum"
  top: "stage2/block3/sum"
}
layer {
  name: "stage2/block3/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block3/sum"
  top: "stage2/block3/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block3/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block3/sum"
  top: "stage2/block3/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block4/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block3/sum/BN"
  top: "stage2/block4/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block4/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block4/conv_reduce"
  top: "stage2/block4/conv_reduce"
}
layer {
  name: "stage2/block4/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block4/conv_reduce"
  top: "stage2/block4/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block4/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block4/conv_reduce"
  top: "stage2/block4/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block4/conv"
  type: "Convolution"
  bottom: "stage2/block4/conv_reduce/BN"
  top: "stage2/block4/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block4/conv/nl"
  type: "ReLU"
  bottom: "stage2/block4/conv"
  top: "stage2/block4/conv"
}
layer {
  name: "stage2/block4/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block4/conv"
  top: "stage2/block4/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block4/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block4/conv"
  top: "stage2/block4/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block4/conv_restore"
  type: "Convolution"
  bottom: "stage2/block4/conv/BN"
  top: "stage2/block4/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block4/sum"
  type: "Eltwise"
  bottom: "stage2/block4/conv_restore"
  bottom: "stage2/block3/sum/BN"
  top: "stage2/block4/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block4/sum/nl"
  type: "ReLU"
  bottom: "stage2/block4/sum"
  top: "stage2/block4/sum"
}
layer {
  name: "stage2/block4/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block4/sum"
  top: "stage2/block4/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block4/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block4/sum"
  top: "stage2/block4/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block5/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block4/sum/BN"
  top: "stage2/block5/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block5/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block5/conv_reduce"
  top: "stage2/block5/conv_reduce"
}
layer {
  name: "stage2/block5/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block5/conv_reduce"
  top: "stage2/block5/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block5/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block5/conv_reduce"
  top: "stage2/block5/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block5/conv"
  type: "Convolution"
  bottom: "stage2/block5/conv_reduce/BN"
  top: "stage2/block5/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block5/conv/nl"
  type: "ReLU"
  bottom: "stage2/block5/conv"
  top: "stage2/block5/conv"
}
layer {
  name: "stage2/block5/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block5/conv"
  top: "stage2/block5/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block5/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block5/conv"
  top: "stage2/block5/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block5/conv_restore"
  type: "Convolution"
  bottom: "stage2/block5/conv/BN"
  top: "stage2/block5/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block5/sum"
  type: "Eltwise"
  bottom: "stage2/block5/conv_restore"
  bottom: "stage2/block4/sum/BN"
  top: "stage2/block5/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block5/sum/nl"
  type: "ReLU"
  bottom: "stage2/block5/sum"
  top: "stage2/block5/sum"
}
layer {
  name: "stage2/block5/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block5/sum"
  top: "stage2/block5/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block5/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block5/sum"
  top: "stage2/block5/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block6/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block5/sum/BN"
  top: "stage2/block6/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block6/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block6/conv_reduce"
  top: "stage2/block6/conv_reduce"
}
layer {
  name: "stage2/block6/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block6/conv_reduce"
  top: "stage2/block6/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block6/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block6/conv_reduce"
  top: "stage2/block6/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block6/conv"
  type: "Convolution"
  bottom: "stage2/block6/conv_reduce/BN"
  top: "stage2/block6/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block6/conv/nl"
  type: "ReLU"
  bottom: "stage2/block6/conv"
  top: "stage2/block6/conv"
}
layer {
  name: "stage2/block6/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block6/conv"
  top: "stage2/block6/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block6/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block6/conv"
  top: "stage2/block6/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block6/conv_restore"
  type: "Convolution"
  bottom: "stage2/block6/conv/BN"
  top: "stage2/block6/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block6/sum"
  type: "Eltwise"
  bottom: "stage2/block6/conv_restore"
  bottom: "stage2/block5/sum/BN"
  top: "stage2/block6/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block6/sum/nl"
  type: "ReLU"
  bottom: "stage2/block6/sum"
  top: "stage2/block6/sum"
}
layer {
  name: "stage2/block6/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block6/sum"
  top: "stage2/block6/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block6/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block6/sum"
  top: "stage2/block6/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block7/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block6/sum/BN"
  top: "stage2/block7/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block7/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block7/conv_reduce"
  top: "stage2/block7/conv_reduce"
}
layer {
  name: "stage2/block7/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block7/conv_reduce"
  top: "stage2/block7/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block7/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block7/conv_reduce"
  top: "stage2/block7/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block7/conv"
  type: "Convolution"
  bottom: "stage2/block7/conv_reduce/BN"
  top: "stage2/block7/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block7/conv/nl"
  type: "ReLU"
  bottom: "stage2/block7/conv"
  top: "stage2/block7/conv"
}
layer {
  name: "stage2/block7/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block7/conv"
  top: "stage2/block7/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block7/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block7/conv"
  top: "stage2/block7/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block7/conv_restore"
  type: "Convolution"
  bottom: "stage2/block7/conv/BN"
  top: "stage2/block7/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block7/sum"
  type: "Eltwise"
  bottom: "stage2/block7/conv_restore"
  bottom: "stage2/block6/sum/BN"
  top: "stage2/block7/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block7/sum/nl"
  type: "ReLU"
  bottom: "stage2/block7/sum"
  top: "stage2/block7/sum"
}
layer {
  name: "stage2/block7/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block7/sum"
  top: "stage2/block7/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block7/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block7/sum"
  top: "stage2/block7/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block8/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block7/sum/BN"
  top: "stage2/block8/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block8/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block8/conv_reduce"
  top: "stage2/block8/conv_reduce"
}
layer {
  name: "stage2/block8/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block8/conv_reduce"
  top: "stage2/block8/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block8/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block8/conv_reduce"
  top: "stage2/block8/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block8/conv"
  type: "Convolution"
  bottom: "stage2/block8/conv_reduce/BN"
  top: "stage2/block8/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block8/conv/nl"
  type: "ReLU"
  bottom: "stage2/block8/conv"
  top: "stage2/block8/conv"
}
layer {
  name: "stage2/block8/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block8/conv"
  top: "stage2/block8/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block8/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block8/conv"
  top: "stage2/block8/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block8/conv_restore"
  type: "Convolution"
  bottom: "stage2/block8/conv/BN"
  top: "stage2/block8/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block8/sum"
  type: "Eltwise"
  bottom: "stage2/block8/conv_restore"
  bottom: "stage2/block7/sum/BN"
  top: "stage2/block8/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block8/sum/nl"
  type: "ReLU"
  bottom: "stage2/block8/sum"
  top: "stage2/block8/sum"
}
layer {
  name: "stage2/block8/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block8/sum"
  top: "stage2/block8/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block8/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block8/sum"
  top: "stage2/block8/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block9/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block8/sum/BN"
  top: "stage2/block9/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block9/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block9/conv_reduce"
  top: "stage2/block9/conv_reduce"
}
layer {
  name: "stage2/block9/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block9/conv_reduce"
  top: "stage2/block9/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block9/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block9/conv_reduce"
  top: "stage2/block9/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block9/conv"
  type: "Convolution"
  bottom: "stage2/block9/conv_reduce/BN"
  top: "stage2/block9/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block9/conv/nl"
  type: "ReLU"
  bottom: "stage2/block9/conv"
  top: "stage2/block9/conv"
}
layer {
  name: "stage2/block9/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block9/conv"
  top: "stage2/block9/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block9/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block9/conv"
  top: "stage2/block9/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block9/conv_restore"
  type: "Convolution"
  bottom: "stage2/block9/conv/BN"
  top: "stage2/block9/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block9/sum"
  type: "Eltwise"
  bottom: "stage2/block9/conv_restore"
  bottom: "stage2/block8/sum/BN"
  top: "stage2/block9/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block9/sum/nl"
  type: "ReLU"
  bottom: "stage2/block9/sum"
  top: "stage2/block9/sum"
}
layer {
  name: "stage2/block9/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block9/sum"
  top: "stage2/block9/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block9/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block9/sum"
  top: "stage2/block9/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block10/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block9/sum/BN"
  top: "stage2/block10/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block10/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block10/conv_reduce"
  top: "stage2/block10/conv_reduce"
}
layer {
  name: "stage2/block10/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block10/conv_reduce"
  top: "stage2/block10/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block10/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block10/conv_reduce"
  top: "stage2/block10/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block10/conv"
  type: "Convolution"
  bottom: "stage2/block10/conv_reduce/BN"
  top: "stage2/block10/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block10/conv/nl"
  type: "ReLU"
  bottom: "stage2/block10/conv"
  top: "stage2/block10/conv"
}
layer {
  name: "stage2/block10/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block10/conv"
  top: "stage2/block10/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block10/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block10/conv"
  top: "stage2/block10/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block10/conv_restore"
  type: "Convolution"
  bottom: "stage2/block10/conv/BN"
  top: "stage2/block10/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block10/sum"
  type: "Eltwise"
  bottom: "stage2/block10/conv_restore"
  bottom: "stage2/block9/sum/BN"
  top: "stage2/block10/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block10/sum/nl"
  type: "ReLU"
  bottom: "stage2/block10/sum"
  top: "stage2/block10/sum"
}
layer {
  name: "stage2/block10/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block10/sum"
  top: "stage2/block10/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block10/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block10/sum"
  top: "stage2/block10/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block11/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block10/sum/BN"
  top: "stage2/block11/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block11/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block11/conv_reduce"
  top: "stage2/block11/conv_reduce"
}
layer {
  name: "stage2/block11/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block11/conv_reduce"
  top: "stage2/block11/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block11/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block11/conv_reduce"
  top: "stage2/block11/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block11/conv"
  type: "Convolution"
  bottom: "stage2/block11/conv_reduce/BN"
  top: "stage2/block11/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block11/conv/nl"
  type: "ReLU"
  bottom: "stage2/block11/conv"
  top: "stage2/block11/conv"
}
layer {
  name: "stage2/block11/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block11/conv"
  top: "stage2/block11/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block11/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block11/conv"
  top: "stage2/block11/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block11/conv_restore"
  type: "Convolution"
  bottom: "stage2/block11/conv/BN"
  top: "stage2/block11/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block11/sum"
  type: "Eltwise"
  bottom: "stage2/block11/conv_restore"
  bottom: "stage2/block10/sum/BN"
  top: "stage2/block11/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block11/sum/nl"
  type: "ReLU"
  bottom: "stage2/block11/sum"
  top: "stage2/block11/sum"
}
layer {
  name: "stage2/block11/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block11/sum"
  top: "stage2/block11/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block11/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block11/sum"
  top: "stage2/block11/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block12/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block11/sum/BN"
  top: "stage2/block12/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block12/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block12/conv_reduce"
  top: "stage2/block12/conv_reduce"
}
layer {
  name: "stage2/block12/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block12/conv_reduce"
  top: "stage2/block12/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block12/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block12/conv_reduce"
  top: "stage2/block12/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block12/conv"
  type: "Convolution"
  bottom: "stage2/block12/conv_reduce/BN"
  top: "stage2/block12/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block12/conv/nl"
  type: "ReLU"
  bottom: "stage2/block12/conv"
  top: "stage2/block12/conv"
}
layer {
  name: "stage2/block12/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block12/conv"
  top: "stage2/block12/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block12/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block12/conv"
  top: "stage2/block12/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block12/conv_restore"
  type: "Convolution"
  bottom: "stage2/block12/conv/BN"
  top: "stage2/block12/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block12/sum"
  type: "Eltwise"
  bottom: "stage2/block12/conv_restore"
  bottom: "stage2/block11/sum/BN"
  top: "stage2/block12/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block12/sum/nl"
  type: "ReLU"
  bottom: "stage2/block12/sum"
  top: "stage2/block12/sum"
}
layer {
  name: "stage2/block12/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block12/sum"
  top: "stage2/block12/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block12/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block12/sum"
  top: "stage2/block12/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block13/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block12/sum/BN"
  top: "stage2/block13/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block13/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block13/conv_reduce"
  top: "stage2/block13/conv_reduce"
}
layer {
  name: "stage2/block13/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block13/conv_reduce"
  top: "stage2/block13/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block13/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block13/conv_reduce"
  top: "stage2/block13/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block13/conv"
  type: "Convolution"
  bottom: "stage2/block13/conv_reduce/BN"
  top: "stage2/block13/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block13/conv/nl"
  type: "ReLU"
  bottom: "stage2/block13/conv"
  top: "stage2/block13/conv"
}
layer {
  name: "stage2/block13/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block13/conv"
  top: "stage2/block13/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block13/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block13/conv"
  top: "stage2/block13/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block13/conv_restore"
  type: "Convolution"
  bottom: "stage2/block13/conv/BN"
  top: "stage2/block13/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block13/sum"
  type: "Eltwise"
  bottom: "stage2/block13/conv_restore"
  bottom: "stage2/block12/sum/BN"
  top: "stage2/block13/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block13/sum/nl"
  type: "ReLU"
  bottom: "stage2/block13/sum"
  top: "stage2/block13/sum"
}
layer {
  name: "stage2/block13/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block13/sum"
  top: "stage2/block13/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block13/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block13/sum"
  top: "stage2/block13/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block14/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block13/sum/BN"
  top: "stage2/block14/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block14/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block14/conv_reduce"
  top: "stage2/block14/conv_reduce"
}
layer {
  name: "stage2/block14/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block14/conv_reduce"
  top: "stage2/block14/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block14/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block14/conv_reduce"
  top: "stage2/block14/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block14/conv"
  type: "Convolution"
  bottom: "stage2/block14/conv_reduce/BN"
  top: "stage2/block14/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block14/conv/nl"
  type: "ReLU"
  bottom: "stage2/block14/conv"
  top: "stage2/block14/conv"
}
layer {
  name: "stage2/block14/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block14/conv"
  top: "stage2/block14/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block14/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block14/conv"
  top: "stage2/block14/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block14/conv_restore"
  type: "Convolution"
  bottom: "stage2/block14/conv/BN"
  top: "stage2/block14/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block14/sum"
  type: "Eltwise"
  bottom: "stage2/block14/conv_restore"
  bottom: "stage2/block13/sum/BN"
  top: "stage2/block14/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block14/sum/nl"
  type: "ReLU"
  bottom: "stage2/block14/sum"
  top: "stage2/block14/sum"
}
layer {
  name: "stage2/block14/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block14/sum"
  top: "stage2/block14/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block14/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block14/sum"
  top: "stage2/block14/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block15/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block14/sum/BN"
  top: "stage2/block15/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block15/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block15/conv_reduce"
  top: "stage2/block15/conv_reduce"
}
layer {
  name: "stage2/block15/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block15/conv_reduce"
  top: "stage2/block15/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block15/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block15/conv_reduce"
  top: "stage2/block15/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block15/conv"
  type: "Convolution"
  bottom: "stage2/block15/conv_reduce/BN"
  top: "stage2/block15/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block15/conv/nl"
  type: "ReLU"
  bottom: "stage2/block15/conv"
  top: "stage2/block15/conv"
}
layer {
  name: "stage2/block15/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block15/conv"
  top: "stage2/block15/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block15/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block15/conv"
  top: "stage2/block15/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block15/conv_restore"
  type: "Convolution"
  bottom: "stage2/block15/conv/BN"
  top: "stage2/block15/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block15/sum"
  type: "Eltwise"
  bottom: "stage2/block15/conv_restore"
  bottom: "stage2/block14/sum/BN"
  top: "stage2/block15/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block15/sum/nl"
  type: "ReLU"
  bottom: "stage2/block15/sum"
  top: "stage2/block15/sum"
}
layer {
  name: "stage2/block15/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block15/sum"
  top: "stage2/block15/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block15/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block15/sum"
  top: "stage2/block15/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block16/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block15/sum/BN"
  top: "stage2/block16/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block16/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block16/conv_reduce"
  top: "stage2/block16/conv_reduce"
}
layer {
  name: "stage2/block16/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block16/conv_reduce"
  top: "stage2/block16/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block16/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block16/conv_reduce"
  top: "stage2/block16/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block16/conv"
  type: "Convolution"
  bottom: "stage2/block16/conv_reduce/BN"
  top: "stage2/block16/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block16/conv/nl"
  type: "ReLU"
  bottom: "stage2/block16/conv"
  top: "stage2/block16/conv"
}
layer {
  name: "stage2/block16/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block16/conv"
  top: "stage2/block16/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block16/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block16/conv"
  top: "stage2/block16/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block16/conv_restore"
  type: "Convolution"
  bottom: "stage2/block16/conv/BN"
  top: "stage2/block16/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block16/sum"
  type: "Eltwise"
  bottom: "stage2/block16/conv_restore"
  bottom: "stage2/block15/sum/BN"
  top: "stage2/block16/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block16/sum/nl"
  type: "ReLU"
  bottom: "stage2/block16/sum"
  top: "stage2/block16/sum"
}
layer {
  name: "stage2/block16/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block16/sum"
  top: "stage2/block16/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block16/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block16/sum"
  top: "stage2/block16/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block17/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block16/sum/BN"
  top: "stage2/block17/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block17/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block17/conv_reduce"
  top: "stage2/block17/conv_reduce"
}
layer {
  name: "stage2/block17/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block17/conv_reduce"
  top: "stage2/block17/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block17/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block17/conv_reduce"
  top: "stage2/block17/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block17/conv"
  type: "Convolution"
  bottom: "stage2/block17/conv_reduce/BN"
  top: "stage2/block17/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block17/conv/nl"
  type: "ReLU"
  bottom: "stage2/block17/conv"
  top: "stage2/block17/conv"
}
layer {
  name: "stage2/block17/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block17/conv"
  top: "stage2/block17/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block17/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block17/conv"
  top: "stage2/block17/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block17/conv_restore"
  type: "Convolution"
  bottom: "stage2/block17/conv/BN"
  top: "stage2/block17/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block17/sum"
  type: "Eltwise"
  bottom: "stage2/block17/conv_restore"
  bottom: "stage2/block16/sum/BN"
  top: "stage2/block17/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block17/sum/nl"
  type: "ReLU"
  bottom: "stage2/block17/sum"
  top: "stage2/block17/sum"
}
layer {
  name: "stage2/block17/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block17/sum"
  top: "stage2/block17/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block17/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block17/sum"
  top: "stage2/block17/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block18/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block17/sum/BN"
  top: "stage2/block18/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block18/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block18/conv_reduce"
  top: "stage2/block18/conv_reduce"
}
layer {
  name: "stage2/block18/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block18/conv_reduce"
  top: "stage2/block18/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block18/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block18/conv_reduce"
  top: "stage2/block18/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block18/conv"
  type: "Convolution"
  bottom: "stage2/block18/conv_reduce/BN"
  top: "stage2/block18/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block18/conv/nl"
  type: "ReLU"
  bottom: "stage2/block18/conv"
  top: "stage2/block18/conv"
}
layer {
  name: "stage2/block18/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block18/conv"
  top: "stage2/block18/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block18/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block18/conv"
  top: "stage2/block18/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block18/conv_restore"
  type: "Convolution"
  bottom: "stage2/block18/conv/BN"
  top: "stage2/block18/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block18/sum"
  type: "Eltwise"
  bottom: "stage2/block18/conv_restore"
  bottom: "stage2/block17/sum/BN"
  top: "stage2/block18/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block18/sum/nl"
  type: "ReLU"
  bottom: "stage2/block18/sum"
  top: "stage2/block18/sum"
}
layer {
  name: "stage2/block18/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block18/sum"
  top: "stage2/block18/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block18/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block18/sum"
  top: "stage2/block18/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block19/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block18/sum/BN"
  top: "stage2/block19/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block19/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block19/conv_reduce"
  top: "stage2/block19/conv_reduce"
}
layer {
  name: "stage2/block19/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block19/conv_reduce"
  top: "stage2/block19/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block19/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block19/conv_reduce"
  top: "stage2/block19/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block19/conv"
  type: "Convolution"
  bottom: "stage2/block19/conv_reduce/BN"
  top: "stage2/block19/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block19/conv/nl"
  type: "ReLU"
  bottom: "stage2/block19/conv"
  top: "stage2/block19/conv"
}
layer {
  name: "stage2/block19/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block19/conv"
  top: "stage2/block19/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block19/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block19/conv"
  top: "stage2/block19/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block19/conv_restore"
  type: "Convolution"
  bottom: "stage2/block19/conv/BN"
  top: "stage2/block19/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block19/sum"
  type: "Eltwise"
  bottom: "stage2/block19/conv_restore"
  bottom: "stage2/block18/sum/BN"
  top: "stage2/block19/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block19/sum/nl"
  type: "ReLU"
  bottom: "stage2/block19/sum"
  top: "stage2/block19/sum"
}
layer {
  name: "stage2/block19/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block19/sum"
  top: "stage2/block19/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block19/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block19/sum"
  top: "stage2/block19/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block20/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block19/sum/BN"
  top: "stage2/block20/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block20/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block20/conv_reduce"
  top: "stage2/block20/conv_reduce"
}
layer {
  name: "stage2/block20/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block20/conv_reduce"
  top: "stage2/block20/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block20/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block20/conv_reduce"
  top: "stage2/block20/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block20/conv"
  type: "Convolution"
  bottom: "stage2/block20/conv_reduce/BN"
  top: "stage2/block20/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block20/conv/nl"
  type: "ReLU"
  bottom: "stage2/block20/conv"
  top: "stage2/block20/conv"
}
layer {
  name: "stage2/block20/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block20/conv"
  top: "stage2/block20/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block20/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block20/conv"
  top: "stage2/block20/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block20/conv_restore"
  type: "Convolution"
  bottom: "stage2/block20/conv/BN"
  top: "stage2/block20/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block20/sum"
  type: "Eltwise"
  bottom: "stage2/block20/conv_restore"
  bottom: "stage2/block19/sum/BN"
  top: "stage2/block20/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block20/sum/nl"
  type: "ReLU"
  bottom: "stage2/block20/sum"
  top: "stage2/block20/sum"
}
layer {
  name: "stage2/block20/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block20/sum"
  top: "stage2/block20/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block20/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block20/sum"
  top: "stage2/block20/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block21/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block20/sum/BN"
  top: "stage2/block21/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block21/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block21/conv_reduce"
  top: "stage2/block21/conv_reduce"
}
layer {
  name: "stage2/block21/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block21/conv_reduce"
  top: "stage2/block21/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block21/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block21/conv_reduce"
  top: "stage2/block21/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block21/conv"
  type: "Convolution"
  bottom: "stage2/block21/conv_reduce/BN"
  top: "stage2/block21/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block21/conv/nl"
  type: "ReLU"
  bottom: "stage2/block21/conv"
  top: "stage2/block21/conv"
}
layer {
  name: "stage2/block21/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block21/conv"
  top: "stage2/block21/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block21/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block21/conv"
  top: "stage2/block21/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block21/conv_restore"
  type: "Convolution"
  bottom: "stage2/block21/conv/BN"
  top: "stage2/block21/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block21/sum"
  type: "Eltwise"
  bottom: "stage2/block21/conv_restore"
  bottom: "stage2/block20/sum/BN"
  top: "stage2/block21/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block21/sum/nl"
  type: "ReLU"
  bottom: "stage2/block21/sum"
  top: "stage2/block21/sum"
}
layer {
  name: "stage2/block21/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block21/sum"
  top: "stage2/block21/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block21/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block21/sum"
  top: "stage2/block21/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block22/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block21/sum/BN"
  top: "stage2/block22/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block22/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage2/block22/conv_reduce"
  top: "stage2/block22/conv_reduce"
}
layer {
  name: "stage2/block22/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block22/conv_reduce"
  top: "stage2/block22/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block22/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block22/conv_reduce"
  top: "stage2/block22/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block22/conv"
  type: "Convolution"
  bottom: "stage2/block22/conv_reduce/BN"
  top: "stage2/block22/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block22/conv/nl"
  type: "ReLU"
  bottom: "stage2/block22/conv"
  top: "stage2/block22/conv"
}
layer {
  name: "stage2/block22/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block22/conv"
  top: "stage2/block22/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block22/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block22/conv"
  top: "stage2/block22/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block22/conv_restore"
  type: "Convolution"
  bottom: "stage2/block22/conv/BN"
  top: "stage2/block22/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage2/block22/sum"
  type: "Eltwise"
  bottom: "stage2/block22/conv_restore"
  bottom: "stage2/block21/sum/BN"
  top: "stage2/block22/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage2/block22/sum/nl"
  type: "ReLU"
  bottom: "stage2/block22/sum"
  top: "stage2/block22/sum"
}
layer {
  name: "stage2/block22/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage2/block22/sum"
  top: "stage2/block22/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage2/block22/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage2/block22/sum"
  top: "stage2/block22/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block0/conv_reduce"
  type: "Convolution"
  bottom: "stage2/block22/sum/BN"
  top: "stage3/block0/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage3/block0/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage3/block0/conv_reduce"
  top: "stage3/block0/conv_reduce"
}
layer {
  name: "stage3/block0/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage3/block0/conv_reduce"
  top: "stage3/block0/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block0/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage3/block0/conv_reduce"
  top: "stage3/block0/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block0/conv"
  type: "Convolution"
  bottom: "stage3/block0/conv_reduce/BN"
  top: "stage3/block0/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 3
    stride: 2
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage3/block0/conv/nl"
  type: "ReLU"
  bottom: "stage3/block0/conv"
  top: "stage3/block0/conv"
}
layer {
  name: "stage3/block0/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage3/block0/conv"
  top: "stage3/block0/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block0/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage3/block0/conv"
  top: "stage3/block0/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block0/conv_restore"
  type: "Convolution"
  bottom: "stage3/block0/conv/BN"
  top: "stage3/block0/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 768
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage3/block0/shortcut"
  type: "Convolution"
  bottom: "stage2/block22/sum/BN"
  top: "stage3/block0/shortcut"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 768
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage3/block0/sum"
  type: "Eltwise"
  bottom: "stage3/block0/conv_restore"
  bottom: "stage3/block0/shortcut"
  top: "stage3/block0/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage3/block0/sum/nl"
  type: "ReLU"
  bottom: "stage3/block0/sum"
  top: "stage3/block0/sum"
}
layer {
  name: "stage3/block0/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage3/block0/sum"
  top: "stage3/block0/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block0/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage3/block0/sum"
  top: "stage3/block0/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block1/conv_reduce"
  type: "Convolution"
  bottom: "stage3/block0/sum/BN"
  top: "stage3/block1/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage3/block1/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage3/block1/conv_reduce"
  top: "stage3/block1/conv_reduce"
}
layer {
  name: "stage3/block1/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage3/block1/conv_reduce"
  top: "stage3/block1/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block1/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage3/block1/conv_reduce"
  top: "stage3/block1/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block1/conv"
  type: "Convolution"
  bottom: "stage3/block1/conv_reduce/BN"
  top: "stage3/block1/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage3/block1/conv/nl"
  type: "ReLU"
  bottom: "stage3/block1/conv"
  top: "stage3/block1/conv"
}
layer {
  name: "stage3/block1/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage3/block1/conv"
  top: "stage3/block1/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block1/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage3/block1/conv"
  top: "stage3/block1/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block1/conv_restore"
  type: "Convolution"
  bottom: "stage3/block1/conv/BN"
  top: "stage3/block1/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 768
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage3/block1/sum"
  type: "Eltwise"
  bottom: "stage3/block1/conv_restore"
  bottom: "stage3/block0/sum/BN"
  top: "stage3/block1/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage3/block1/sum/nl"
  type: "ReLU"
  bottom: "stage3/block1/sum"
  top: "stage3/block1/sum"
}
layer {
  name: "stage3/block1/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage3/block1/sum"
  top: "stage3/block1/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block1/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage3/block1/sum"
  top: "stage3/block1/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block2/conv_reduce"
  type: "Convolution"
  bottom: "stage3/block1/sum/BN"
  top: "stage3/block2/conv_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage3/block2/conv_reduce/nl"
  type: "ReLU"
  bottom: "stage3/block2/conv_reduce"
  top: "stage3/block2/conv_reduce"
}
layer {
  name: "stage3/block2/conv_reduce/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage3/block2/conv_reduce"
  top: "stage3/block2/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block2/conv_reduce/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage3/block2/conv_reduce"
  top: "stage3/block2/conv_reduce/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block2/conv"
  type: "Convolution"
  bottom: "stage3/block2/conv_reduce/BN"
  top: "stage3/block2/conv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage3/block2/conv/nl"
  type: "ReLU"
  bottom: "stage3/block2/conv"
  top: "stage3/block2/conv"
}
layer {
  name: "stage3/block2/conv/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage3/block2/conv"
  top: "stage3/block2/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block2/conv/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage3/block2/conv"
  top: "stage3/block2/conv/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block2/conv_restore"
  type: "Convolution"
  bottom: "stage3/block2/conv/BN"
  top: "stage3/block2/conv_restore"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 768
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "stage3/block2/sum"
  type: "Eltwise"
  bottom: "stage3/block2/conv_restore"
  bottom: "stage3/block1/sum/BN"
  top: "stage3/block2/sum"
  eltwise_param { operation: SUM }
}
layer {
  name: "stage3/block2/sum/nl"
  type: "ReLU"
  bottom: "stage3/block2/sum"
  top: "stage3/block2/sum"
}
layer {
  name: "stage3/block2/sum/BN"
  type: "BatchNorm" include {phase: TRAIN}
  bottom: "stage3/block2/sum"
  top: "stage3/block2/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "stage3/block2/sum/BN"
  type: "BatchNorm" include {phase: TEST}
  bottom: "stage3/block2/sum"
  top: "stage3/block2/sum/BN"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "global_pool"
  type: "Pooling"
  bottom: "stage3/block2/sum/BN"
  top: "global_pool"
  pooling_param {
    pool: AVE
    pad: 0
    global_pooling: true
  }
}
layer {
  name: "classifier"
  type: "InnerProduct"
  bottom: "global_pool"
  top: "classifier"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "acc"
  type: "Accuracy"
  bottom: "classifier"
  bottom: "label"
  top: "acc"
  include {
    phase: TEST
  }
}
layer {
  name: "loss/loss"
  type: "SoftmaxWithLoss"
  bottom: "classifier"
  bottom: "label"
  top: "loss/loss"
  loss_weight: 1
}

layer {
  name: "loss/top-5"
  type: "Accuracy"
  bottom: "classifier"
  bottom: "label"
  top: "loss/top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
}